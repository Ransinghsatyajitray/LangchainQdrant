{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the packages\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "import os\n",
    "from qdrant_client.http import models\n",
    "#from langchain.document_loaders import PyPDFLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the environment keys\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "qdrant_api_key = os.environ[\"QDRANT_API_KEY\"]\n",
    "qdrant_url = os.environ[\"QDRANT_URL\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client initialization\n",
    "qdrant_client = QdrantClient(\n",
    "    url=qdrant_url,\n",
    "    api_key=qdrant_api_key,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In Qdrant we create cluster and then we create collection which is more like vector db table in it\n",
    "# create collection\n",
    "QDRANT_COLLECTION_NAME = \"my-collection-Semantic-1\"\n",
    "\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=QDRANT_COLLECTION_NAME,\n",
    "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),\n",
    "    #init_from=models.InitFrom(collection=QDRANT_COLLECTION_NAME),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1:\n",
      "Introduction to Transformers\n",
      "Introduction to Transformers: an NLP Perspective\n",
      "Tong Xiao xiaotong@mail.neu.edu.cn\n",
      "NLP Lab., Northeastern University, Shenyang, China\n",
      "NiuTrans Research, Shenyang, China\n",
      "Jingbo Zhu zhujingbo@mail.neu.edu.cn\n",
      "NLP Lab., Northeastern University, Shenyang, China\n",
      "NiuTrans Research, Shenyang, China\n",
      "Abstract\n",
      "Transformers have dominated empirical machine learning models of natural language pro-\n",
      "cessing. In this paper, we introduce basic concepts of Transformers and present key tech-\n",
      "niques that form the recent advances of these models. This includes a description of the\n",
      "standard Transformer architecture, a series of model refinements, and common applica-\n",
      "tions. Given that Transformers and related deep learning techniques might be evolving in\n",
      "ways we have never seen, we cannot dive into all the model details or cover all the tech-\n",
      "nical areas. Instead, we focus on just those concepts that are helpful for gaining a good\n",
      "understanding of Transformers and their variants. We also summarize the key ideas that\n",
      "impact this field, thereby yielding some insights into the strengths and limitations of these\n",
      "models.\n",
      "1arXiv:2311.17633v1  [cs.CL]  29 Nov 2023\n",
      "Page 2:\n",
      "Xiao and Zhu\n",
      "Contents\n",
      "1 Background 4\n",
      "2 The Basic Model 4\n",
      "2.1 The Transformer Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n",
      "2.2 Positional Encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n",
      "2.3 Multi-head Self-attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n",
      "2.4 Layer Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n",
      "2.5 Feed-forward Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . 12\n",
      "2.6 Attention Models on the Decoder Side . . . . . . . . . . . . . . . . . . . . . 13\n",
      "2.7 Training and Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n",
      "3 Syntax-aware Models 17\n",
      "3.1 Syntax-aware Input and Output . . . . . . . . . . . . . . . . . . . . . . . . 18\n",
      "3.2 Syntax-aware Attention Models . . . . . . . . . . . . . . . . . . . . . . . . . 19\n",
      "3.3 Multi-branch Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n",
      "3.4 Multi-scale Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n",
      "3.5 Transformers as Syntax Learners . . . . . . . . . . . . . . . . . . . . . . . . 25\n",
      "4 Improved Architectures 29\n",
      "4.1 Locally Attentive Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n",
      "4.2 Deep Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n",
      "4.3 Numerical Method-Inspired Models . . . . . . . . . . . . . . . . . . . . . . . 39\n",
      "4.4 Wide Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n",
      "5 Efficient Models 45\n",
      "5.1 Sparse Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n",
      "5.2 Recurrent and Memory Models . . . . . . . . . . . . . . . . . . . . . . . . . 49\n",
      "5.3 Low-dimensional Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n",
      "5.4 Parameter and Activation Sharing . . . . . . . . . . . . . . . . . . . . . . . 60\n",
      "5.5 Alternatives to Self-Attention . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n",
      "5.6 Conditional Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n",
      "5.7 Model Transfer and Pruning . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\n",
      "5.8 Sequence Compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\n",
      "5.9 High Performance Computing Methods . . . . . . . . . . . . . . . . . . . . 77\n",
      "6 Applications 80\n",
      "6.1 Language Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n",
      "6.2 Text Encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n",
      "6.3 Speech Translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n",
      "6.4 Vision Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n",
      "6.5 Multimodal Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n",
      "7 Summary 89\n",
      "2\n",
      "Page 3:\n",
      "Introduction to Transformers\n",
      "Appendices 96\n",
      "A Sinusoidal Positional Encoding 96\n",
      "3\n",
      "Page 4:\n",
      "Xiao and Zhu\n",
      "1. Background\n",
      "Transformers are a type of neural network (Vaswani et al., 2017). They were originally\n",
      "known for their strong performance in machine translation, and are now a de facto standard\n",
      "for building large-scale self-supervised learning systems (Brown et al., 2020; Devlin et al.,\n",
      "2019). The past few years have seen the rise of Transformers not only in natural language\n",
      "processing (NLP) but also in several other fields, such as computer vision and multi-modal\n",
      "processing. As Transformers continue to mature, these models are playing an increasingly\n",
      "important role in the research and application of artificial intelligence (AI).\n",
      "Looking back at the history of neural networks, Transformers have not been around for\n",
      "a long time. While Transformers are “newcomers” in NLP, they were developed on top of\n",
      "several ideas, the origins of which can be traced back to earlier work, such as word embedding\n",
      "(Bengio et al., 2003; Mikolov et al., 2013) and attention mechanisms (Bahdanau et al., 2014;\n",
      "Luong et al., 2015). As a result, Transformers can benefit from the advancements of different\n",
      "sub-fields of deep learning, and provide an elegant way to combine these neural models.\n",
      "On the other hand, Transformers are unique, and differ from previous models in several\n",
      "ways. First, they do not depend on recurrent or convolutional neural networks for modeling\n",
      "sequences of words, but use only attention mechanisms and feed-forward neural networks.\n",
      "Second, the use of self-attention in Transformers makes it easier to deal with global contexts\n",
      "and dependencies among words. Third, Transformers are very flexible architectures and can\n",
      "be easily modified to accommodate different tasks.\n",
      "The widespread use of Transformers motivates the development of cutting-edge tech-\n",
      "niques in deep learning. For example, there are significant refinements in self-attention\n",
      "mechanisms, which have been incorporated into many state-of-the-art NLP systems. The\n",
      "resulting techniques, together with the progress in self-supervised learning, have led us to\n",
      "a new era of AI: we are beginning to obtain models of universal language understanding,\n",
      "generation and reasoning. This has been evidenced by recent Transformer-based large lan-\n",
      "guage models (LLMs) which demonstrate amazing performance across a broad variety of\n",
      "tasks (Bubeck et al., 2023).\n",
      "This paper provides an introduction to Transformers while reflecting the recent devel-\n",
      "opments in applying these models to different problems. However, Transformers are so\n",
      "successful that there have been numerous related studies and we cannot give a full descrip-\n",
      "tion of them. Therefore, we focus this work on the core ideas of Transformers, and present\n",
      "a basic description of the common techniques. We also discuss some recent advances in\n",
      "Transformers, such as model improvements for efficiency and accuracy considerations. Be-\n",
      "cause the field is very active and new techniques are coming out every day, it is impossible\n",
      "to survey all the latest literature and we are not attempting to do so. Instead, we focus on\n",
      "just those concepts and algorithms most relevant to Transformers, aimed at the people who\n",
      "wish to get a general understanding of these models.\n",
      "2. The Basic Model\n",
      "Here we consider the model presented in Vaswani et al. (2017)’s work. We start by consid-\n",
      "ering the Transformer architecture and discuss the details of the sub-models subsequently.\n",
      "4\n",
      "Page 5:\n",
      "Introduction to Transformers\n",
      "2.1 The Transformer Architecture\n",
      "Figure 1 shows the standard Transformer model which follows the general encoder-decoder\n",
      "framework. A Transformer encoder comprises a number of stacked encoding layers (or\n",
      "encoding blocks ). Each encoding layer has two different sub-layers (or sub-blocks), called\n",
      "the self-attention sub-layer and the feed-forward neural network (FFN) sub-layer. Suppose\n",
      "we have a source-side sequence x=x1...xmand a target-side sequence y=y1...yn. The\n",
      "input of an encoding layer is a sequence of mvectors h1...hm, each having dmodel dimensions\n",
      "(orddimensions for simplicity). We follow the notation adopted in the previous chapters,\n",
      "using H∈Rm×dto denote these input vectors1. The self-attention sub-layer first performs\n",
      "a self-attention operation Att self(·) onHto generate an output C:\n",
      "C= Att self(H) (1)\n",
      "Here Cis of the same size as H, and can thus be viewed as a new representation of the\n",
      "inputs. Then, a residual connection and a layer normalization unit are added to the output\n",
      "so that the resulting model is easier to optimize.\n",
      "The original Transformer model employs the post-norm structure where a residual\n",
      "connection is created before layer normalization is performed, like this\n",
      "Hself= LNorm( C+H) (2)\n",
      "where the addition of Hdenotes the residual connection (He et al., 2016a), and LNorm( ·)\n",
      "denotes the layer normalization function (Ba et al., 2016). Substituting Eq. (1) into Eq.\n",
      "(2), we obtain the form of the self-attention sub-layer\n",
      "Layerself(H) = Hself\n",
      "= LNorm(Att self(H) +H) (3)\n",
      "The definitions of LNorm( ·) and Att self(·) will be given later in this section.\n",
      "The FFN sub-layer takes Hselfand outputs a new representation Hffn∈Rm×d. It has\n",
      "the same form as the self-attention sub-layer, with the attention function replaced by the\n",
      "FFN function, given by\n",
      "Layerffn(Hself) = Hffn\n",
      "= LNorm(FFN( Hself) +Hself) (4)\n",
      "Here FFN( ·) could be any feed-forward neural networks with non-linear activation func-\n",
      "tions. The most common structure of FFN( ·) is a two-layer network involving two linear\n",
      "transformations and a ReLU activation function between them.\n",
      "For deep models, we can stack the above neural networks. Let Hlbe the output of layer\n",
      "l. Then, we can express Hlas a function of Hl−1. We write this as a composition of two\n",
      "sub-layers\n",
      "Hl= Layerffn(Hl\n",
      "self) (5)\n",
      "Hl\n",
      "self= Layerself(Hl−1) (6)\n",
      "1. Provided hj∈Rdis a row vector, we have H=\n",
      "h1\n",
      "...\n",
      "hm\n",
      ".\n",
      "5\n",
      "Page 6:\n",
      "Xiao and Zhu\n",
      "Self-Attention\n",
      "Layerself(·)Add & LayerNormFeed-Forward Network\n",
      "Layerffn(·)Add & LayerNorm\n",
      "Word Position\n",
      "x1. . . x mSelf-Attention\n",
      "Layerself(·)Add & LayerNormEncoder-Decoder Attention\n",
      "Layercross(·)Add & LayerNormFeed-Forward Network\n",
      "Layerffn(·)Add & LayerNormSoftmax( SLWo)\n",
      "Word Position\n",
      "y0y1. . . y n−1L×Encoder×LDecoder...\n",
      "Pr(·|y0...yn−1, x1...xm)Pr(·|y0, x1...xm)\n",
      "...\n",
      "Figure 1: The Transformer architecture (Vaswani et al., 2017). There are Lstacked layers\n",
      "on each of the encoder and decoder sides. An encoding layer comprises a self-\n",
      "attention sub-layer and an FFN sub-layer. Both of these sub-layers share the\n",
      "same structure which involves a core function (either Layerself(·) or Layerffn(·)),\n",
      "followed by a residual connection and a layer normalization unit. Each decoding\n",
      "layer has a similar architecture with the encoding layers, but with an additional\n",
      "encoder-decoder attention sub-layer sandwiched between the self-attention and\n",
      "FFN sub-layers. As with most sequence-to-sequence models, Transformer takes\n",
      "x1...xmandy0...yi−1for predicting yi. The representation of an input word com-\n",
      "prises a sum of a word embedding and a positional embedding. The distributions\n",
      "{Pr(·|y0...yi−1, x1...xm)}are generated in sequence by a Softmax layer, which op-\n",
      "erates on a linear transformation of the output from the last decoding layer.\n",
      "6\n",
      "Page 7:\n",
      "Introduction to Transformers\n",
      "If there are Lencoding layers, then HLwill be the output of the encoder. In this case, HL\n",
      "can be viewed as a representation of the input sequence that is learned by the Transformer\n",
      "encoder. H0denotes the input of the encoder. In recurrent and convolutional models, H0\n",
      "can simply be word embeddings of the input sequence. Transformer takes a different way of\n",
      "representing the input words , and encodes the positional information explicitly. In Section\n",
      "2.2 we will discuss the embedding model used in Transformers.\n",
      "The Transformer decoder has a similar structure as the Transformer encoder. It com-\n",
      "prises Lstacked decoding layers (ordecoding blocks ). Let Slbe the output of the l-th\n",
      "decoding layer. We can formulate a decoding layer by using the following equations\n",
      "Sl= Layerffn(Sl\n",
      "cross) (7)\n",
      "Sl\n",
      "cross = Layercross(HL,Sl−1\n",
      "self) (8)\n",
      "Sl\n",
      "self= Layerself(Sl−1) (9)\n",
      "Here there are three decoder sub-layers. The self-attention and FFN sub-layers are the\n",
      "same as those used in the encoder. Layercross(·) denotes a cross attention sub-layer (or\n",
      "encoder-decoder sub-layer) which models the transformation from the source-side to the\n",
      "target-side. In Section 2.6 we will see that Layercross(·) can be implemented using the same\n",
      "function as Layerself(·).\n",
      "The Transformer decoder outputs a distribution over a vocabulary Vyat each target-side\n",
      "position. This is achieved by using a softmax layer that normalizes a linear transformation\n",
      "ofSLto distributions of target-side words. To do this, we map SLto an n× |Vy|matrix O\n",
      "by\n",
      "O=SL·Wo (10)\n",
      "where Wo∈Rd×|Vy|is the parameter matrix of the linear transformation.\n",
      "Then, the output of the Transformer decoder is given in the form\n",
      "\n",
      "Pr(·|y0,x)\n",
      "...\n",
      "Pr(·|y0...yn−1,x)\n",
      "= Softmax( O)\n",
      "=\n",
      "Softmax( o1)\n",
      "...\n",
      "Softmax( on)\n",
      " (11)\n",
      "where oidenotes the i-th row vector of O, and y0denotes the start symbol ⟨SOS⟩. Under\n",
      "this model, the probability of xgiven ycan be defined as usual,\n",
      "log Pr( y|x) =nX\n",
      "i=1log Pr( yi|y0...yi−1,x) (12)\n",
      "This equation resembles the general form of language modeling: we predict the word\n",
      "at time igiven all of the words up to time i−1. Therefore, the input of the Transformer\n",
      "decoder is shifted one word left, that is, the input is y0...yn−1and the output is y1...yn.\n",
      "7\n",
      "Page 8:\n",
      "Xiao and Zhu\n",
      "The Transformer architecture discussed above has several variants which have been\n",
      "successfully used in different fields of NLP. For example, we can use a Transformer encoder\n",
      "to represent texts (call it the encoder-only architecture ), can use a Transformer decoder\n",
      "to generate texts (call it the decoder-only architecture ), and can use a standard encoder-\n",
      "decoder Transformer model to transform an input sequence to an output sequence. In\n",
      "the rest of this chapter, most of the discussion is independent of the particular choice of\n",
      "application, and will be mostly focused on the encoder-decoder architecture. In Section 6,\n",
      "we will see applications of the encoder-only and decoder-only architectures.\n",
      "2.2 Positional Encoding\n",
      "In their original form, both FFNs and attention models used in Transformer ignore an\n",
      "important property of sequence modeling, which is that the order of the words plays a\n",
      "crucial role in expressing the meaning of a sequence. This means that the encoder and\n",
      "decoder are insensitive to the positional information of the input words. A simple approach\n",
      "to overcoming this problem is to add positional encoding to the representation of each word\n",
      "of the sequence. More formally, a word xjcan be represented as a d-dimensional vector\n",
      "xpj=xj+ PE( j) (13)\n",
      "Here xj∈Rdis the embedding of the word which can be obtained by using the word\n",
      "embedding models. PE( j)∈Rdis the representation of the position j. Vanilla Transformer\n",
      "employs the sinusoidal positional encoding models which we write in the form\n",
      "PE(i,2k) = sin( i·1\n",
      "100002k/d) (14)\n",
      "PE(i,2k+ 1) = cos( i·1\n",
      "100002k/d) (15)\n",
      "where PE( i, k) denotes the k-th entry of PE( i). The idea of positional encoding is to\n",
      "distinguish different positions using continuous systems. Here we use the sine and cosine\n",
      "functions with different frequencies. The interested reader can refer to Appendix A to see\n",
      "that such a method can be interpreted as a carrying system. Because the encoding is based\n",
      "on individual positions, it is also called absolute positional encoding . In Section 4.1 we\n",
      "will see an improvement to this method.\n",
      "Once we have the above embedding result, xp1...xpmis taken as the input to the\n",
      "Transformer encoder, that is,\n",
      "H0=\n",
      "xp1...\n",
      "xpm\n",
      " (16)\n",
      "Similarly, we can also define the input on the decoder side.\n",
      "2.3 Multi-head Self-attention\n",
      "The use of self-attention is perhaps one of the most significant advances in sequence-to-\n",
      "sequence models. It attempts to learn and make use of direct interactions between each\n",
      "8\n",
      "Page 9:\n",
      "Introduction to Transformers\n",
      "(a) RNN (b) CNN ( r= 3) (c) Self-attention\n",
      "Figure 2: Information flows in recurrent, convolutional and self-attention models, shown as\n",
      "arrow lines between positions.\n",
      "pair of inputs. From a representation learning perspective, self-attention models assume\n",
      "that the learned representation at position i(denoted by ci) is a weighted sum of the inputs\n",
      "over the sequence. The output ciis thus given by\n",
      "ci=mX\n",
      "j=1αi,jhj (17)\n",
      "where αi,jindicates how strong the input hiis correlated with the input hj. We thus\n",
      "can view cias a representation of the global context at position i.αi,jcan be defined in\n",
      "different ways if one considers different attention models. Here we use the scaled dot-product\n",
      "attention function to compute αi,j, as follows\n",
      "αi,j= Softmax( hihT\n",
      "j/β)\n",
      "=exp(hihT\n",
      "j/β)Pm\n",
      "k=1exp(hihT\n",
      "k/β)(18)\n",
      "where βis a scaling factor and is set to√\n",
      "d.\n",
      "Compared with conventional recurrent and convolutional models, an advantage of self-\n",
      "attention models is that they shorten the computational “distance” between two inputs.\n",
      "Figure 2 illustrates the information flow in these models. We see that, given the input at\n",
      "position i, self-attention models can directly access any other input. By contrast, recurrent\n",
      "and convolutional models might need two or more jumps to see the whole sequence.\n",
      "We can have a more general view of self-attention by using the QKV attention model.\n",
      "Suppose we have a sequence of κqueries Q=\n",
      "q1\n",
      "...\n",
      "qκ\n",
      ", and a sequence of ψkey-value pairs\n",
      "(K=\n",
      "k1\n",
      "...\n",
      "kψ\n",
      ",V=\n",
      "v1\n",
      "...\n",
      "vψ\n",
      "). The output of the model is a sequence of vectors, each corre-\n",
      "sponding to a query. The form of the QKV attention is given by\n",
      "Attqkv(Q,K,V) = Softmax(QKT\n",
      "√\n",
      "d)V (19)\n",
      "9\n",
      "Page 10:\n",
      "Xiao and Zhu\n",
      "We can write the output of the QKV attention model as a sequence of row vectors\n",
      "C=\n",
      "c1\n",
      "...\n",
      "cκ\n",
      "\n",
      "= Att qkv(Q,K,V) (20)\n",
      "To apply this equation to self-attention, we simply have\n",
      "Hq=HWq(21)\n",
      "Hk=HWk(22)\n",
      "Hv=HWv(23)\n",
      "where Wq,Wk,Wv∈Rd×drepresents linear transformations of H.\n",
      "By considering Eq. (1), we then obtain\n",
      "C= Att self(H)\n",
      "= Att qkv(Hq,Hk,Hv)\n",
      "= Softmax(Hq[Hk]T\n",
      "√\n",
      "d)Hv(24)\n",
      "Here Softmax(Hq[Hk]T\n",
      "√\n",
      "d) is an m×mmatrix in which each row represents a distribution over\n",
      "{h1, ...,hm}, that is\n",
      "rowi=\u0002\n",
      "αi,1... α i,m\u0003\n",
      "(25)\n",
      "We can improve the above self-attention model by using a technique called multi-head\n",
      "attention . This method can be motivated from the perspective of learning from multiple\n",
      "lower-dimensional feature sub-spaces, which projects a feature vector onto multiple sub-\n",
      "spaces and learns feature mappings on individual sub-spaces. Specifically, we project the\n",
      "whole of the input space into τsub-spaces (call them heads ), for example, we transform\n",
      "H∈Rm×dintoτmatrices of size m×d\n",
      "τ, denoted by {Hhead\n",
      "1, ...,Hhead\n",
      "τ}. The attention\n",
      "model is then run τtimes, each time on a head. Finally, the outputs of these model runs\n",
      "are concatenated, and transformed by a linear projection. This procedure can be expressed\n",
      "by\n",
      "C= Merge( Chead\n",
      "1, ...,Chead\n",
      "τ)Wc (26)\n",
      "(27)\n",
      "For each head h,\n",
      "Chead\n",
      "h = Softmax(Hq\n",
      "h[Hk\n",
      "h]T\n",
      "√\n",
      "d)Hv\n",
      "h (28)\n",
      "Hq\n",
      "h=HWq\n",
      "h(29)\n",
      "Hk\n",
      "h=HWk\n",
      "h (30)\n",
      "Hv\n",
      "h=HWv\n",
      "h (31)\n",
      "10\n",
      "Page 11:\n",
      "Introduction to Transformers\n",
      "Here Merge( ·) is the concatenation function, and Att QKV(·) is the attention function de-\n",
      "scribed in Eq. (20). Wq\n",
      "h,Wk\n",
      "h,Wv\n",
      "h∈Rd×d\n",
      "τare the parameters of the projections from a\n",
      "d-dimensional space to ad\n",
      "τ-dimensional space for the queries, keys, and values. Thus, Hq\n",
      "h,\n",
      "Hk\n",
      "h,Hv\n",
      "h, and Chead\n",
      "hare all m×d\n",
      "τmatrices. Merge( Chead\n",
      "1, ...,Chead\n",
      "τ) produces an m×d\n",
      "matrix. It is then transformed by a linear mapping Wc∈Rd×d, leading to the final result\n",
      "C∈Rd×d.\n",
      "While the notation here seems somewhat tedious, it is convenient to implement multi-\n",
      "head models using various deep learning toolkits. A common method in Transformer-based\n",
      "systems is to store inputs from all the heads in data structures called tensors, so that we\n",
      "can make use of parallel computing resources to have efficient systems.\n",
      "2.4 Layer Normalization\n",
      "Layer normalization provides a simple and effective means to make the training of neural\n",
      "networks more stable by standardizing the activations of the hidden layers in a layer-wise\n",
      "manner. As introduced in Ba et al. (2016)’s work, given a layer’s output h∈Rd, the layer\n",
      "normalization method computes a standardized output LNorm( h)∈Rdby\n",
      "LNorm( h) = g⊙h−µ\n",
      "σ+ϵ+b (32)\n",
      "Here µ∈Rdandσ∈Rdare the mean and standard derivation of the activations. Let hk\n",
      "be the k-th dimension of h.µandσare given by\n",
      "µ=1\n",
      "d·dX\n",
      "k=1hk (33)\n",
      "σ=vuut1\n",
      "d·dX\n",
      "k=1(hk−µ)2 (34)\n",
      "Hereg∈Rdandb∈Rdare the rescaling and bias terms. They can be treated as parameters\n",
      "of layer normalization, whose values are to be learned together with other parameters of the\n",
      "Transformer model. The addition of ϵtoσis used for the purpose of numerical stability.\n",
      "In general, ϵis chosen to be a small number.\n",
      "We illustrate the layer normalization method for the hidden states of an encoder in the\n",
      "following example (assume that m= 4,d= 3,g=1,b=0, and ϵ= 0.1).\n",
      "h1\n",
      "h2\n",
      "h3\n",
      "h4\n",
      "1 1 2\n",
      "0.9 0.9 0\n",
      "0.7 0.8 0\n",
      "3 1 7\n",
      "µ= 1.3, σ= 0.5\n",
      "µ= 0.6, σ= 0.4\n",
      "µ= 0.5, σ= 0.4\n",
      "µ= 3.7, σ= 2.5=⇒\n",
      "1−1.3\n",
      "0.5+0.11−1.3\n",
      "0.5+0.12−1.3\n",
      "0.5+0.10.9−0.6\n",
      "0.4+0.10.9−0.6\n",
      "0.4+0.10−0.6\n",
      "0.4+0.10.7−0.5\n",
      "0.4+0.10.8−0.5\n",
      "0.4+0.10−0.5\n",
      "0.4+0.13−3.7\n",
      "2.5+0.11−3.7\n",
      "2.5+0.17−3.7\n",
      "2.5+0.1\n",
      "\n",
      "As discussed in Section 2.1, the layer normalization unit in each sub-layer is used to\n",
      "standardize the output of a residual block. Here we describe a more general formulation for\n",
      "this structure. Suppose that F(·) is a neural network we want to run. Then, the post-norm\n",
      "structure of F(·) is given by\n",
      "Hout= LNorm( F(Hin) +Hin) (35)\n",
      "11\n",
      "Page 12:\n",
      "Xiao and Zhu\n",
      "Core Function\n",
      "F(·)Add & LayerNorm\n",
      "......\n",
      "Hin F(·) LNorm( ·)Hout\n",
      "Hin LNorm( ·) F(·) Hout(a) Post-norm\n",
      "(b) Pre-norm\n",
      "Figure 3: The post-norm and pre-norm structures. F(·) = core function, LNorm( ·) = layer\n",
      "normalization, and ⊕= residual connection.\n",
      "where HinandHoutput are the input and output of this model. Clearly, Eq. (4) is an\n",
      "instance of this equation.\n",
      "An alternative approach to introducing layer normalization and residual connections\n",
      "into modeling is to execute the LNorm( ·) function right after the F(·) function, and to\n",
      "establish an identity mapping from the input to the output of the entire sub-layer. This\n",
      "structure, known as the pre-norm structure, can be expressed in the form\n",
      "Hout= LNorm( F(Hin)) +Hin (36)\n",
      "Both post-norm and pre-norm Transformer models are widely used in NLP systems.\n",
      "See Figure 3 for a comparison of these two structures. In general, residual connections are\n",
      "considered an effective means to make the training of multi-layer neural networks easier. In\n",
      "this sense, pre-norm Transformer seems promising because it follows the convention that a\n",
      "residual connection is created to bypass the whole network and that the identity mapping\n",
      "from the input to the output leads to easier optimization of deep models. However, by\n",
      "considering the expressive power of a model, there may be modeling advantages in using\n",
      "post-norm Transformer because it does not so much rely on residual connections and enforces\n",
      "more sophisticated modeling for representation learning. In Section 4.2, we will see a\n",
      "discussion on this issue.\n",
      "2.5 Feed-forward Neural Networks\n",
      "The use of FFNs in Transformer is inspired in part by the fact that complex outputs can be\n",
      "formed by transforming the inputs through nonlinearities. While the self-attention model\n",
      "itself has some nonlinearity (in Softmax( ·)), a more common way to do this is to consider\n",
      "additional layers with non-linear activation functions and linear transformations. Given\n",
      "an input Hin∈Rm×dand an output Hout∈Rm×d, the Hout= FFN( Hin) function in\n",
      "Transformer has the following form\n",
      "Hout=HhiddenWf+bf (37)\n",
      "Hhidden = ReLU( HinWh+bh) (38)\n",
      "12\n",
      "Page 13:\n",
      "Introduction to Transformers\n",
      "Sub-model # of Parameters Time Complexity ×\n",
      "EncoderMulti-head Self-attention 4d2O(m2·d) L\n",
      "Feed-forward Network 2d·dffn+d+dffn O(m·d·dffn) L\n",
      "Layer Normalization 2d O(d) 2L\n",
      "DecoderMulti-head Self-attention 4d2O(n2·d) L\n",
      "Multi-head Cross-attention 4d2O(m·n·d) L\n",
      "Feed-forward Network 2d·dffn+d+dffn O(n·d·dffn) L\n",
      "Layer Normalization 2d O(d) 3L\n",
      "Table 1: Numbers of parameters and time complexities of different Transformer modules\n",
      "under different setups. m= source-sequence length, n= target-sequence length,\n",
      "d= default number of dimensions of a hidden layer, dffn= number of dimensions\n",
      "of the FFN hidden layer, τ= number of heads in the attention models, and L=\n",
      "number of encoding or decoding layers. The column ×means the number of times\n",
      "a sub-model is applied on the encoder or decoder side. The time complexities are\n",
      "estimated by counting the number of multiplication of floating-point numbers.\n",
      "where Hhidden ∈Rm×dffnis the hidden states, and Wh∈Rd×dffn,bh∈Rdffn,Wf∈Rdffn×d\n",
      "andbf∈Rdare the parameters. This is a two-layer FFN in which the first layer (or\n",
      "hidden layer) introduces a nonlinearity through ReLU( ·)2and the second layer involves\n",
      "only a linear transformation. It is common practice in Transformer to use a larger size of\n",
      "the hidden layer. For example, a common choice is dffn= 4d, that is, the size of each hidden\n",
      "representation is 4 times as large as the input.\n",
      "Note that using a wide FFN sub-layer has been proven to be of great practical value in\n",
      "many state-of-the-art systems. However, a consequence of this is that the model is occupied\n",
      "by the parameters of the FFN. Table 1 shows parameter numbers and time complexities\n",
      "for different modules of a standard Transformer system. We see that FFNs dominate the\n",
      "model size when dffnis large, though they are not the most time consuming components.\n",
      "In the case of very big Transform models, we therefore wish to address this problem for\n",
      "building efficient systems.\n",
      "2.6 Attention Models on the Decoder Side\n",
      "A decoder layer involves two attention sub-layers, the first of which is a self-attention sub-\n",
      "layer, and the second is a cross-attention sub-layer. These sub-layers are based on either\n",
      "the post-norm or the pre-norm structure, but differ by designs of the attention functions.\n",
      "Consider, for example, the post-norm structure, described in Eq. (35). We can define the\n",
      "cross-attention and self-attention sub-layers for a decoding layer to be\n",
      "Scross = Layercross(Henc,Sself)\n",
      "= LNorm(Att cross(Henc,Sself) +Sself) (39)\n",
      "Sself= Layerself(S)\n",
      "= LNorm(Att self(S) +S) (40)\n",
      "2. ReLU( x) = max {0, x}.\n",
      "13\n",
      "Page 14:\n",
      "Xiao and Zhu\n",
      "where S∈Rn×dis the input of the self-attention sub-layer, Scross∈Rn×dandSself∈Rn×d\n",
      "are the outputs of the sub-layers, and Henc∈Rm×dis the output of the encoder3.\n",
      "As with conventional attention models, cross-attention is primarily used to model the\n",
      "correspondence between the source-side and target-side sequences. The Att cross(·) function\n",
      "is based on the QKV attention model which generates the result of querying a collection of\n",
      "key-value pairs. More specifically, we define the queries, keys and values as linear mappings\n",
      "ofSselfandHenc, as follows\n",
      "Sq\n",
      "self=SselfWq\n",
      "cross (41)\n",
      "Hk\n",
      "enc=HencWk\n",
      "enc (42)\n",
      "Hv\n",
      "enc=HencWv\n",
      "enc (43)\n",
      "where Wq\n",
      "cross,Wk\n",
      "enc,Wv\n",
      "enc∈Rd×dare the parameters of the mappings. In other words, the\n",
      "queries are defined based on Sself, and the keys and values are defined based on Henc.\n",
      "Attcross(·) is then defined as\n",
      "Attcross(Henc,Sself) = Att qkv(Sq\n",
      "self,Hk\n",
      "enc,Hv\n",
      "enc)\n",
      "= Softmax(Sq\n",
      "self[Hk\n",
      "enc]T\n",
      "√\n",
      "d)Hv\n",
      "enc (44)\n",
      "The Att self(·) function has a similar form as Att cross(·), with linear mappings of Staken\n",
      "as the queries, keys, and values, like this\n",
      "Attself(S) = Att qkv(Sq,Sk,Sv)\n",
      "= Softmax(Sq[Sk]T\n",
      "√\n",
      "d+M)Sv(45)\n",
      "where Sq=SWq\n",
      "dec,Sk=SWk\n",
      "dec, and Sv=SWv\n",
      "decare linear mappings of Swith parame-\n",
      "tersWq\n",
      "dec,Wk\n",
      "dec,Wv\n",
      "dec∈Rd×d.\n",
      "This form is similar to that of Eq. (20). A difference compared to self-attention on\n",
      "the encoder side, however, is that the model here needs to follow the rule of left-to-right\n",
      "generation (see Figure 2). That is, given a target-side word at the position i, we can see only\n",
      "the target-side words in the left context y1...yi−1. To do this, we add a masking variable M\n",
      "to the unnormalized weight matrixSq[Sk]T\n",
      "√\n",
      "d+M. Both MandSq[Sk]T\n",
      "√\n",
      "d+Mare of size n×n,\n",
      "and so a lower value of an entry of Mmeans a larger bias towards lower alignment scores\n",
      "for the corresponding entry ofSq[Sk]T\n",
      "√\n",
      "d+M. In order to avoid access to the right context\n",
      "given i,Mis defined to be\n",
      "M(i, k) =(\n",
      "0 i≤k\n",
      "−∞ i > k(46)\n",
      "3. For an encoder having Lencoder layers, Henc=HL.\n",
      "14\n",
      "Page 15:\n",
      "Introduction to Transformers\n",
      "i−3i−2i−1 i i+ 1\n",
      "(a) Encoder-side Self-attentioni−3i−2i−1 i i+ 1\n",
      "(b) Decoder-side Self-attention\n",
      "Table 2: Self-attention on the encoder and decoder sides. Each line connects an input\n",
      "and an output of the self-attention model, indicating a dependency of an output\n",
      "state on an input state. For encoder self-attention, the output at any position is\n",
      "computed by having access to the entire sequence. By contrast, for decoder self-\n",
      "attention, the output at position iis computed by seeing only inputs at positions\n",
      "up to i.\n",
      "where M(i, k) indicates a bias term for the alignment score between positions iandk.\n",
      "Below we show an example of how the masking variable is applied (assume n= 4).\n",
      "Softmax(Sq[Sk]T\n",
      "√\n",
      "d+M)\n",
      "= Softmax(\n",
      "2 0 .1 1 1\n",
      "0 0 .9 0.9 0.9\n",
      "0.2 0.8 0.7 2\n",
      "0.3 1 0 .3 3\n",
      "+\n",
      "0−∞ −∞ −∞\n",
      "0 0 −∞ −∞\n",
      "0 0 0 −∞\n",
      "0 0 0 0\n",
      ")\n",
      "= Softmax(\n",
      "2−∞ −∞ −∞\n",
      "0 0 .9−∞ −∞\n",
      "0.2 0.8 0 .7−∞\n",
      "0.3 1 0 .3 3\n",
      ")\n",
      "=\n",
      "1 0 0 0\n",
      "0.3 0.7 0 0\n",
      "0.2 0.4 0.4 0\n",
      "0.05 0 .1 0.05 0 .8\n",
      "(47)\n",
      "As noted in Section 2.3, it is easy to improve these models by using the multi-head\n",
      "attention mechanism. Also, since decoders are typically the most time-consuming part of\n",
      "practical systems, the bulk of the computational effort in running these systems is very\n",
      "much concerned with the efficiency of the attention modules on the decoder side.\n",
      "2.7 Training and Inference\n",
      "Transformers can be trained and used in a regular way. For example, we can train a\n",
      "Transformer model by performing gradient descent to minimize some loss function on the\n",
      "training data, and test the trained model by performing beam search on the unseen data.\n",
      "15\n",
      "Page 16:\n",
      "Xiao and Zhu\n",
      "Below we present some of the techniques that are typically used in the training and inference\n",
      "of Transformer models.\n",
      "•Learning Rate Scheduling . As standard neural networks, Transformers can be\n",
      "directly trained using back-propagation. The training process is generally iterated\n",
      "many times to make the models fit the training data well. In each training step,\n",
      "we update the weights of the neural networks by moving them a small step in the\n",
      "direction of negative gradients of errors. There are many ways to design the update\n",
      "rule of training. A popular choice is to use the Adam optimization method (Kingma\n",
      "and Ba, 2014). To adjust the learning rate during training, Vaswani et al. (2017)\n",
      "present a learning rate scheduling strategy which increases the learning rate linearly\n",
      "for a number of steps and then decay it gradually. They design a learning rate of the\n",
      "form\n",
      "lr=lr0·mi\n",
      "n−0.5\n",
      "step, nstep·(nwarmup )−1.5\t\n",
      "(48)\n",
      "where lr0denotes the initial learning rate, and nstepdenotes the number of training\n",
      "steps we have executed, and nwarmup denotes the number of warmup steps. In the\n",
      "firstnwarmup steps, the learning rate lrgrows larger as training proceeds. It reaches\n",
      "the highest value at the point of nstep=nwarmup , and then decreases as an inverse\n",
      "square root function (i.e., lr0·n−0.5\n",
      "step).\n",
      "•Batching and Padding . To make a trade-off between global optimization and\n",
      "training convergency, it is common to update the weights each time on a relatively\n",
      "small collection of samples, called a minibatch of samples. Therefore, we can consider\n",
      "a batch version of forward and backward computation processes in which the whole\n",
      "minibatch is used together to obtain the gradient information. One advantage of\n",
      "batching is that it allows the system to make use of efficient tensor operations to deal\n",
      "with multiple sequences in a single run. This requires that all the input sequences\n",
      "in a minibatch are stored in a single memory block, so that they can be read in\n",
      "and processed together. To illustrate this idea, consider a minimatch containing four\n",
      "samples whose source-sides are\n",
      "A B C D E F\n",
      "M N\n",
      "R S T\n",
      "W X Y Z\n",
      "We can store these sequences in a 4 ×6 continuous block where each “row” represents\n",
      "a sequence, like this\n",
      "A B C D E F\n",
      "M N □ □ □ □\n",
      "R S T □ □ □\n",
      "W X Y Z □ □\n",
      "Here padding words □are inserted between sequences, so that these sequences are\n",
      "aligned in the memory. Typically, we do not want padding to affect the operation of\n",
      "16\n",
      "Page 17:\n",
      "Introduction to Transformers\n",
      "the system, and so we can simply define □as a zero vector (call it zero padding ).\n",
      "On the other hand, in some cases we are interested in using padding to describe\n",
      "something that is not covered by the input sequences. For example, we can replace\n",
      "padding words with the words in the left (or right) context of a sequence, though\n",
      "this may require modifications to the system to ensure that the newly added context\n",
      "words do not cause additional content to appear in the output.\n",
      "•Search and Caching . At test time, we need to search the space of candidate hy-\n",
      "potheses (or candidate target-side sequences) to identify the hypothesis (or target-side\n",
      "sequence) with the highest score.\n",
      "ˆy= arg max\n",
      "yscore( x,y) (49)\n",
      "where score( x,y) is the model score of the target-side sequence ygiven the source-side\n",
      "sequence x. While there are many search algorithms to achieve this, most of them\n",
      "share a similar structure: the search program operates by extending candidate target-\n",
      "side sequences in a pool at a time. In this way, the resulting algorithm can be viewed\n",
      "as a left-to-right generation procedure. Note that all of the designs of score( x,y),\n",
      "no matter how complex, are based on computing Pr( y|x). Because the attention\n",
      "models used in Transformer require computing the dot-product of each pair of the\n",
      "input vectors of a layer, the time complexity of the search algorithm is a quadratic\n",
      "function of the length of y. It is therefore not efficient to repeatedly compute the\n",
      "outputs of the attention models for positions that have been dealt with. This problem\n",
      "can be addressed by caching the states of each layer for words we have seen. Figure\n",
      "4 illustrates the use of the caching mechanism in a search step. All the states for\n",
      "positions < iare maintained and easily accessed in a cache. At position i, all we need\n",
      "is to compute the states for the newly added word, and then to update the cache.\n",
      "3. Syntax-aware Models\n",
      "Although Transformer is simply a deep learning model that does not make use of any\n",
      "linguistic structure or assumption, it may be necessary to incorporate our prior knowledge\n",
      "into such systems. This is in part because NLP researchers have long believed that a higher\n",
      "level of abstraction of data is needed to develop ideal NLP systems, and there have been\n",
      "many systems that use structure as priors. However, structure is a wide-ranging topic\n",
      "and there are several types of structure one may refer to See (2018)’s work. For example,\n",
      "the inductive biases used in our model design can be thought of as some structural prior,\n",
      "while NLP models can also learn the underlying structure of problems by themselves. In\n",
      "this sub-section we will discuss some of these issues. We will focus on the methods of\n",
      "introducing linguistic structure into Transformer models. As Transformer can be applied\n",
      "to many NLP tasks, which differ much in their input and output formats, we will primarily\n",
      "discuss modifications to Transformer encoders (call them syntax-aware Transformer\n",
      "encoders ). Our discussion, however, is general, and the methods can be easily extended\n",
      "to Transformer decoders.\n",
      "17\n",
      "Page 18:\n",
      "Xiao and Zhu\n",
      "...\n",
      "y0...y1\n",
      "Cache\n",
      "...\n",
      "y1...y2\n",
      "(b) decoding step 2...\n",
      "y0...y1\n",
      "(a) decoding step 1...\n",
      "y0...y1\n",
      "...\n",
      "y1...y2\n",
      "Cache\n",
      "...\n",
      "y2...y3\n",
      "(c) decoding step 3\n",
      "Figure 4: Illustration of the caching mechanism in Transformer decoders. Rectangles indi-\n",
      "cate the states of decoding layers or sub-layers. At step i, all the states at previous\n",
      "steps are stored in a cache (see dotted boxes), and we only need to compute the\n",
      "states for this step (see blue rectangles and arrows). Then, we add the newly\n",
      "generated states to the cache, and move on to step i+ 1.\n",
      "3.1 Syntax-aware Input and Output\n",
      "One of the simplest methods of incorporating structure into NLP systems is to modify the\n",
      "input sequence, leaving the system unchanged. As a simple example, consider a sentence\n",
      "where each word xjis assigned a set of κsyntactic labels {tag1\n",
      "j, ...,tagκ\n",
      "j}(e.g., POS labels\n",
      "and dependency labels). We can write these symbols together to define a new “word”\n",
      "xj/tag1\n",
      "j/.../tagκ\n",
      "j\n",
      "Then, the embedding of this word is given by\n",
      "xpj=e(xj/tag1\n",
      "j/.../tagκ\n",
      "j) + PE( j) (50)\n",
      "where e(xj/tag1\n",
      "j/.../tagκ\n",
      "j)∈Rdis the embedding of xj/tag1\n",
      "j/.../tagκ\n",
      "j. Since xj/tag1\n",
      "j/.../tagκ\n",
      "j\n",
      "is a complex symbol, we decompose the learning problem of e(xj/tag1\n",
      "j/.../tagκ\n",
      "j) into easier\n",
      "problems. For example, we can develop κembedding models, each producing an embedding\n",
      "given a tag. Then, we write e(xj/tag1\n",
      "j/.../tagκ\n",
      "j) as a sum of the word embedding and tag\n",
      "embeddings\n",
      "e(xj/tag1\n",
      "j/.../tagκ\n",
      "j) = xj+e(tag1\n",
      "j) +...+e(tagκ\n",
      "j) (51)\n",
      "where {e(tag1\n",
      "j), ..., e (tagκ\n",
      "j)}are the embeddings of the tags. Alternatively, we can combine\n",
      "these embeddings via a neural network in the form\n",
      "e(xj/tag1\n",
      "j/.../tagκ\n",
      "j) = FFN embed (xj, e(tag1\n",
      "j), ..., e (tagκ\n",
      "j)) (52)\n",
      "where FFN embed (·) is a feed-forward neural network that has one layer or two.\n",
      "18\n",
      "Page 19:\n",
      "Introduction to Transformers\n",
      "We can do the same thing for sentences on the decoder side as well, and treat yi/tag1\n",
      "i/.../tagκ\n",
      "i\n",
      "as a syntax-augmented word. However, this may lead to a much larger target-side vocabu-\n",
      "lary and poses a computational challenge for training and inference.\n",
      "Another form that is commonly used to represent a sentence is syntax tree. In lin-\n",
      "guistics, the syntax of a sentence can be interpreted in many different ways, resulting\n",
      "in various grammars and the corresponding tree (or graph)-based representations. While\n",
      "these representations differ in their syntactic forms, a general approach to use them in se-\n",
      "quence modeling is tree linearization . Consider the following sentence annotated with a\n",
      "constituency-based parse tree\n",
      "S\n",
      ".\n",
      "!VP\n",
      "ADJP\n",
      "JJ\n",
      "interestingVBZ\n",
      "’sNP\n",
      "PRP\n",
      "It\n",
      "We can write this tree structure as a sequence of words, syntactic labels and brackets via a\n",
      "tree traversal algorithm, as follows\n",
      "(S (NP (PRP It ) PRP )NP (VP (VBZ ’s ) VBZ (ADJP (JJ\n",
      "interesting ) JJ )ADJP )VP (. ! ) . )S\n",
      "This sequence of syntactic tokens can be used as an input to the system, that is, each\n",
      "token is represented by word and positional embeddings, and then the sum of these embed-\n",
      "dings is treated as a regular input of the encoder. An example of the use of linearized trees\n",
      "is tree-to-string machine translation in which a syntax tree in one language is translated\n",
      "into a string in another language (Currey and Heafield, 2018; Li et al., 2017). Linearized\n",
      "trees can also be used for tree generation. For example, we can frame parsing tasks as\n",
      "sequence-to-sequence problems to map an input text to a sequential representation of its\n",
      "corresponding syntax tree (Choe and Charniak, 2016; Vinyals et al., 2015). See Figure 5 for\n",
      "illustrations of these models. It should be noted that the methods described here are not\n",
      "specific to Transformer but could be applied to many models, such as RNN-based models.\n",
      "3.2 Syntax-aware Attention Models\n",
      "For Transformer models, it also makes sense to make use of syntax trees to guide the process\n",
      "of learning sequence representations. In the previous section we saw how representations\n",
      "of a sequence can be computed by relating different positions within that sequence. This\n",
      "allows us to impose some structure on these relations which are represented by distributions\n",
      "of attention weights over all the positions. To do this we use the encoder self-attention with\n",
      "19\n",
      "Page 20:\n",
      "Xiao and Zhu\n",
      "Encoder Decoder\n",
      "(ADJP (JJ Great )JJ (. ! ).)ADJP ⟨SOS⟩很好很好！\n",
      "(a) Tree-to-String Machine Translation\n",
      "Encoder Decoder\n",
      "Great ! ⟨SOS⟩(ADJP (JJ Great )JJ (. ! ).(ADJP (JJ Great )JJ (. ! ).)ADJP\n",
      "(b) Constituency Parsing\n",
      "Figure 5: Illustration of tree linearization on either the encoder or decoder side. For tree-\n",
      "to-string machine translation, the encoder takes sequential representation of an\n",
      "input parse tree, and the decoder outputs the corresponding translation. For\n",
      "parsing, the encoder takes a sentence, and the decoder outputs the corresponding\n",
      "syntax tree.\n",
      "an additive mask\n",
      "AttSynself(H) = Softmax(Hq[Hk]T\n",
      "√\n",
      "d+M)Hv(53)\n",
      "or alternatively with a multiplicative mask\n",
      "AttSynself(H) = Softmax(Hq[Hk]T\n",
      "√\n",
      "d⊙M)Hv(54)\n",
      "where M∈Rm×mis a matrix of masking variables in which a larger value of M(i, j) indicates\n",
      "a stronger syntactic correlation between positions iandj. In the following description we\n",
      "choose Eq. (54) as the basic form.\n",
      "One common way to design Mis to project syntactic relations of the input tree structure\n",
      "into constraints over the sequence. Here we consider constituency parse trees and depen-\n",
      "dency parse trees for illustration. Generally, two types of masking methods are employed.\n",
      "20\n",
      "Page 21:\n",
      "Introduction to Transformers\n",
      "•0-1 Masking . This method assigns M(i, j) a value of 1 if the words at positions i\n",
      "andjare considered syntactically correlated and a value of 0 otherwise (Bai et al.,\n",
      "2021; Zhang et al., 2020). To model the relation between two words in a syntax tree,\n",
      "we can consider the distance between their corresponding nodes. One of the simplest\n",
      "forms is given by\n",
      "M(i, j) =(\n",
      "1ω(i, j)≤ωmax\n",
      "0 otherwise(55)\n",
      "where ω(i, j) is the length of the shortest path between the nodes of the words at\n",
      "positions iandj. For example, given a dependency parse tree, ω(i, j) is the number\n",
      "of dependency edges in the path between the two words. For a constituency parse\n",
      "tree, all the words are leaf nodes, and so ω(i, j) gives a tree distance between the\n",
      "two leaves in the same branch of the tree. ωmaxis a parameter used to control the\n",
      "maximum distance between two nodes that can be considered syntactically correlated.\n",
      "For example, assuming that there is a dependency parse tree and ωmax= 1, Eq. (55)\n",
      "enforces a constraint that the attention score between positions iandjis computed\n",
      "only if they have a parent-dependent relation4.\n",
      "•Soft Masking . Instead of treating Mas a hard constraint, we can use it as a soft\n",
      "constraint that scales the attention weight between positions iandjin terms of the\n",
      "degree to which the corresponding words are correlated. An idea is to reduce the\n",
      "attention weight as ω(i, j) becomes larger. A very simple method to do this is to\n",
      "transform ω(i, j) in some way that M(i, j) holds a negative correlation relationship\n",
      "with ω(i, j) and its value falls into the interval [0 ,1]\n",
      "M(i, j) = DNorm( ω(i, j)) (56)\n",
      "There are several alternative designs for DNorm( ·). For example, one can compute a\n",
      "standardized score of −ω(i, j) by subtracting its mean and dividing by its standard\n",
      "deviation (Chen et al., 2018a), or can normalize 1 /ω(i, j) over all possible jin the\n",
      "sequence (Xu et al., 2021b). In cases where parsers can output a score between\n",
      "positions iandj, it is also possible to use this score to compute M(i, j). For example,\n",
      "a dependency parser can produce the probability of the word at position ibeing the\n",
      "parent of the word at position j(Strubell et al., 2018). We can then write M(i, j) as\n",
      "M(i, j) = Pr parent (i|j) (57)\n",
      "or alternatively\n",
      "M(i, j) = max {Prparent (i|j),Prparent (j|i)} (58)\n",
      "where Pr parent (i|j) and Pr parent (j|i) are the probabilities given by the parser. See\n",
      "Figure 6 for an example of inducing a soft masking variable from a dependency parse\n",
      "tree.\n",
      "4. For multiplicative masks, M(i, j) = 0 does not mean that the attention weight between jandiis zero\n",
      "because the Softmax function does not give a zero output for a dimension whose corresponding input\n",
      "is of a zero value. A method to “mask” an entry of Softmax(HHT\n",
      "√\n",
      "d) is to use an additive mask and set\n",
      "M(i, j) =−∞ifω(i, j)> ω max.\n",
      "21\n",
      "Page 22:\n",
      "Xiao and Zhu\n",
      "The concert was wonderful !\n",
      "(a) Dependency Parse Tree (b) Mask M(darker color means larger value)The\n",
      "concert\n",
      "was\n",
      "wonderful\n",
      "!Theconcertwaswonderful!\n",
      "Figure 6: Priors induced from a dependency parse tree. The row iof the matrix Mrep-\n",
      "resents a distribution that describes how much weight we can give to M(i, j) in\n",
      "terms of the syntactic distance between iandj.\n",
      "3.3 Multi-branch Models\n",
      "Introducing syntax into NLP systems is not easy. This is partially because automatic\n",
      "parse trees may have errors, and partially because the use of syntax may lead to strong\n",
      "assumption of the underlying structure of a sentence. Rather than combining syntactic and\n",
      "word information into one “big” model, it may be more flexible and effective to build one\n",
      "model to encode syntax and a different one to encode word sequences. One way to achieve\n",
      "this is through the use of multiple neural networks (called branches orpaths ), each dealing\n",
      "with one type of input. The outputs of these branches are then combined to produce an\n",
      "output (Fan et al., 2020; Lin et al., 2022b; Xie et al., 2017). Various methods have therefore\n",
      "been used to combine different types of input for neural models like Transformer.\n",
      "One commonly-used approach is to build two separate encoders, in which one model is\n",
      "trained to encode the syntactic input (denoted by t), and the other is trained to encode\n",
      "the usual input (denoted by x). Figure 7 (a) illustrates this multi-encoder architecture.\n",
      "The syntactic encoder Encode syn(t) is based on models presented in Sections 3.1 and 3.2,\n",
      "and the text encoder Encode text(x) is a standard Transformer encoder. The representations\n",
      "generated by these encoders are then fed into the combination model as input, and combined\n",
      "into a hybrid representation, given by\n",
      "Hhybrid = Combine( Hsyn,Htext)\n",
      "= Combine(Encode syn(t),Encode text(x)) (59)\n",
      "There are several designs for Combine( ·), depending on what kind of problems we apply\n",
      "the encoders to. For example, if we want to develop a text classifier, Combine( ·) can\n",
      "be a simple pooling network. For more complicated tasks, such as machine translation,\n",
      "Combine( ·) can be a Transformer encoder as well, and we can fuse information from different\n",
      "sources by performing self-attention on [ Hsyn,Htext].\n",
      "22\n",
      "Page 23:\n",
      "Introduction to TransformersEncode syn(·)\n",
      "Encode text(·)\n",
      "t xCombine( ·)...\n",
      "(a) Multi-encoderx...Branch 1\n",
      "Branch 2t\n",
      "(b) Multi-branch as a Sub-modelx...\n",
      "Head 1t\n",
      "(c) Multi-head Attention\n",
      "Figure 7: Multi-branch architectures. There are two inputs: a sentence (denoted by x) and\n",
      "the syntax tree of the sentence (denoted by t). In the multi-encoder architecture\n",
      "(see sub-figure (a)), two encoders are constructed to encode xandt, respectively.\n",
      "A combination model then takes the outputs of the encoders and produces a\n",
      "combined representation of xandt. The idea of multi-branch networks can\n",
      "be used for designing sub-models of the encoder. A simple example is that we\n",
      "create multiple paths in parallel for some layers of the encoder (see sub-figure\n",
      "(b)). Another example is multi-head attention (see sub-figure (c)) where we use\n",
      "different heads to learn different representations.\n",
      "While we restrict attention to syntactic models in this section, the general multi-encoder\n",
      "architecture can be used in many problems where inputs from additional sources are re-\n",
      "quired. For example, one can use one encoder to represent a sentence, and use another\n",
      "encoder to represent the previous sentence in the same document. We thus have a context-\n",
      "aware model by combining the two encoders (Li et al., 2020a; Voita et al., 2018). Fur-\n",
      "thermore, the architectures of the encoders do not need to be restricted to Transformer,\n",
      "and we can choose different models for different branches. For example, as a widely-used\n",
      "2-branch encoding architecture, we can use a CNN-based encoder to model local context,\n",
      "and a Transformer encoder to model global context (Wu et al., 2020).\n",
      "Sub-models of a Transformer model can also be multi-branch neural networks. See\n",
      "Figure 7 (b) for an example involving two self-attention branches. One is the standard\n",
      "self-attention network Att self(H). The other is the syntax-aware self-attention network\n",
      "AttSynself(H). The output of the self-attention model is a linear combination of the outputs\n",
      "of these two branches (Xu et al., 2021b), given by\n",
      "Hself=α·Attself(H) + (1 −α)·AttSynself(H) (60)\n",
      "23\n",
      "Page 24:\n",
      "Xiao and Zhu\n",
      "where αis a coefficient of combination. Hselfcan be used as usual by taking a layer\n",
      "normalization function and adding a residual connection, and so the overall architecture is\n",
      "the same as standard Transformer models.\n",
      "Multi-head attention networks can also be viewed as forms of multi-branch models.\n",
      "Therefore, we can provide guidance from syntax to only some of the heads while keeping\n",
      "the rest unchanged (Strubell et al., 2018). This approach is illustrated in Figure 7 (c)\n",
      "where only one head of the self-attention sub-layer makes use of syntax trees for computing\n",
      "attention weights.\n",
      "3.4 Multi-scale Models\n",
      "In linguistics, syntax studies how sentences are built up by smaller constituents. Different\n",
      "levels of these constituents are in general organized in a hierarchical structure, called syn-\n",
      "tactic hierarchy . It is therefore possible to use multiple levels of syntactic constituents to\n",
      "explain the same sentence, for example, words explain how the sentence is constructed from\n",
      "small meaningful units, and phrases explain how the sentence is constructed from larger\n",
      "linguistic units.\n",
      "Multi-scale Transformers leverage varying abstraction levels of data to represent a sen-\n",
      "tence using diverse feature scales. A common approach is to write a sentence in multiple\n",
      "different forms and then to combine them using a multi-branch network (Hao et al., 2019).\n",
      "For example, consider a sentence\n",
      "The oldest beer-making facility was discovered in China.\n",
      "We can tokenize it into a sequence of words, denoted by\n",
      "xwords = The oldest beer-making facility was discovered in China .\n",
      "Alternatively, we can write it as a sequence of phrases by using a parser, denoted by\n",
      "xphrases = [The oldest beer-making facility] NP[was discovered in China] VP[.].\n",
      "The simplest way to build a multi-scale model is to encode xwords andxphrases using two\n",
      "separate Transformer encoders. Then, the outputs of these encoders are combined in some\n",
      "way. This leads to the same form as Eq. (59), and we can view this model as an instance\n",
      "of the general multi-encoder architecture.\n",
      "Both xwords andxphrases can be viewed as sequences of tokens, for example, xwords has\n",
      "nine word-based tokens, and xphrases has three phrase-based tokens5. However, involving\n",
      "all possible phrases will result in a huge vocabulary. We therefore need some method to\n",
      "represent each phrase as an embedding in a cheap way. By treating phrase embedding as\n",
      "a sequence modeling problem, it is straightforward to learn sub-sequence representations\n",
      "simply by considering the sequence models described in the previous chapters and this chap-\n",
      "ter. Now we have a two-stage learning process. In the first stage, we learn the embeddings\n",
      "5.xphrases comprises three tokens The oldest beer-making facility ,was discovered in China , and ..\n",
      "24\n",
      "Page 25:\n",
      "Introduction to Transformers\n",
      "of input units on different scales using separate models. In the second stage, we learn to\n",
      "encode sequences on different scales using a multi-branch model.\n",
      "More generally, we do not need to restrict ourselves to linguistically meaningful units\n",
      "in multi-scale representation learning. For example, we can learn sub-word segmentations\n",
      "from data and represent an input sentence as a sequence of sub-words. This results in a\n",
      "hierarchical representation of the sentence, for example, sub-words →words →phrases.\n",
      "While the learned sub-words may not have linguistic meanings, they provide a new insight\n",
      "into modeling words and phrases, as well as a new scale of features. Also, we do not need\n",
      "to develop multiple encoders for multi-scale modeling. An alternative approach is to take\n",
      "representations on different scales in the multi-head self-attention attention modules, which\n",
      "makes it easier to model the interactions among different scales (Guo et al., 2020; Li et al.,\n",
      "2022b).\n",
      "A problem with the approaches described above, however, is that the representations\n",
      "(or attention weight matrices) learned on different scales are of different sizes. For example,\n",
      "in the above examples, the representation learned from xwords is a 9 ×dmatrix, and the\n",
      "representation learned from xphrases is a 3×dmatrix. A simple solution to this problem is\n",
      "to perform upsampling on the phrase-based representation to expand it to a 9 ×dmatrix.\n",
      "Likewise, we can perform downsampling on the word-based representation to shrink it to a\n",
      "3×dmatrix. Then, the combination model Combine( ·) can be the same as those described\n",
      "in Section 3.3.\n",
      "It is worth noting that multi-scale modeling is widely discussed in several fields. For\n",
      "example, in computer vision, multi-scale modeling is often referred to as a process of learning\n",
      "a series of feature maps on the input image (Fan et al., 2021; Li et al., 2022e). Unlike the\n",
      "multi-branch models presented here, the multi-scale vision Transformer models make use\n",
      "of the hierarchical nature of features in representing images. Systems of this kind are often\n",
      "based on a stack of layers in which each layer learns the features on a larger scale (e.g., a\n",
      "higher channel capacity) from the features produced by the previous layer.\n",
      "3.5 Transformers as Syntax Learners\n",
      "So far we have discussed syntax trees as being constraints or priors on the encoding process\n",
      "so that we can make use of linguistic representations in learning neural networks. It is\n",
      "natural to wonder whether these neural models can learn some knowledge of linguistic\n",
      "structure from data without human design linguistic annotations. This reflects one of\n",
      "the goals of developing NLP systems: linguistic knowledge can be learned from data and\n",
      "encoded in models.\n",
      "In order to explore the linguistic properties learned by NLP systems, a simple method\n",
      "is to examine the syntactic behaviors of the outputs of the systems. For example, we can\n",
      "examine whether the outputs of language generation systems have grammatical errors. An-\n",
      "other example is to ask these systems to accomplish tasks that make sense for linguistics,\n",
      "though they are not trained to do so (Brown et al., 2020). However, examining and ex-\n",
      "plaining how model predictions exhibit syntactic abilities is not sufficient to answer the\n",
      "question. It is also the case that the neural networks have learned some knowledge about\n",
      "language, but it is not used in prediction (Clark et al., 2019). Therefore, we need to see\n",
      "what is modeled and learned inside these neural networks.\n",
      "25\n",
      "Page 26:\n",
      "Xiao and Zhu\n",
      "......\n",
      "Input (Pre-training)Loss (Pre-training)\n",
      "(a) Training the Transformer Model......\n",
      "Input (Probing Training)PredictorLoss (Probing Training)\n",
      "(b) Training the Probing Predictor......\n",
      "Input (Probing)PredictorOutput (Probing)\n",
      "(c) Probing on New Data\n",
      "Figure 8: An overview of probing for Transformer-based models. Given a Transformer\n",
      "model (e.g., a Transformer-based language model), we first optimize the model\n",
      "parameters on some unlabeled data. Then, we develop a predictor which takes\n",
      "the states of a hidden layer of the Transformer model and generates outputs for a\n",
      "probing task (see sub-figure (a)). The predictor can be trained as usual in which\n",
      "only the parameters of the predictor are optimized and the parameters of the\n",
      "Transformer model are fixed (see sub-figure (b)). The Transformer model and\n",
      "the predictor are used together to make predictions on new data for probing (see\n",
      "sub-figure (c)).\n",
      "One approach to examining the latent linguistic structure in Transformer models is to\n",
      "develop probes to see whether and to what extent these models capture notions of lin-\n",
      "guistics, such as dependency relations and parts-of-speech. A general approach to probing\n",
      "is to extract the internal representations of the models and probe them for linguistic phe-\n",
      "nomena. For Transformer, it is usually achieved by examining the attention map and/or\n",
      "output of an attention layer. Then, we construct a probing predictor (orprobing clas-\n",
      "sifier ) that takes these internal representations as input and produces linguistic notions as\n",
      "output (Belinkov, 2022). The probing predictor can be based on either simple heuristics or\n",
      "parameterized models optimized on the probing task. Recent work shows that large-scale\n",
      "Transformer-based language models exhibit good behaviors, called emergent abilities ,\n",
      "in various probing tasks. However, we will not discuss details of these language modeling\n",
      "systems in this chapter, but leave them in the following chapters. Nevertheless, we assume\n",
      "here that we have a Transformer encoder that has been well trained on unlabeled data and\n",
      "can be used for probing. Figure 8 illustrates the process of probing.\n",
      "Many probing methods have been used in recent work on analyzing and understanding\n",
      "what is learned in neural encoders. Here we describe some of the popular ones.\n",
      "•Trees . Given a trained Transformer encoder, it is easy to know how “likely” two\n",
      "words of a sentence have some linguistic relationship by computing the attention\n",
      "26\n",
      "Page 27:\n",
      "Introduction to Transformers\n",
      "weight between them. We can use this quantity to define a metric measuring the\n",
      "syntactic distance between the two words at positions iandj\n",
      "ds(i, j) = 1 −α(i, j) (61)\n",
      "By using this metric it is straightforward to construct the minimum-spanning tree\n",
      "for the sentence, that is, we connect all the words to form a tree structure with the\n",
      "minimum total distance. The tree structure can be seen as a latent tree representation\n",
      "of the sentence that is induced from the neural network. While this dependency-tree-\n",
      "like structure can be used as a source of learned syntactic information in downstream\n",
      "tasks, it says nothing about our knowledge of syntax. An approach to aligning the\n",
      "representations in the encoder with linguistic structure is to learn to produce syntax\n",
      "trees that are consistent with human annotations. To do this, we need to develop a\n",
      "probing predictor that can be trained on tree-annotated data. Suppose that there is\n",
      "a human annotated dependency tree of a given sentence. For each pair of words, we\n",
      "can obtain a distance ω(i, j) by counting the number of edges between them. Then,\n",
      "we can learn a distance metric based on the internal representations of the encoder\n",
      "to approximate ω(i, j). A simple form of such a metric is defined to be the Euclidean\n",
      "distance (Manning et al., 2020). Let A∈Rd×ksbe a parameter matrix. The form of\n",
      "the Euclidean distance is given by\n",
      "ds(i, j) =q\n",
      "||(hi−hj)A||2\n",
      "2 (62)\n",
      "where hiandhjare the representations produced by an encoding layer at positions i\n",
      "andj6. Given a set of tree-annotated sentences S, we can optimize the model by\n",
      "ˆA= arg max\n",
      "AX\n",
      "s∈S1\n",
      "|s|2X\n",
      "i∈s,j∈s\f\fω(i, j)−d2\n",
      "s(i, j)\f\f (63)\n",
      "where |s|is length of the sentence s, and ( i, j) indicates a pair of words in s. The\n",
      "optimized model is then used to parse test sentences via the minimum-spanning tree\n",
      "algorithm, and we can compare the parse trees against the human-annotated trees.\n",
      "To obtain directed trees, which are standard forms of dependency syntax, one can\n",
      "update the above model by considering the relative distance of a word to the root.\n",
      "More details can be found in Manning et al. (2020)’s work. Here the probing predictor\n",
      "functions similarly to a neural parser, trained to predict a syntax tree based on a\n",
      "representation of the input sentence. This idea can be extended to other forms of\n",
      "syntactic structure, such as phrase structure trees (Shi et al., 2016).\n",
      "•Syntactic and Semantic Labels . Many syntactic and semantic parsing tasks can\n",
      "be framed as problems of predicting linguistic labels given a sentence or its segments.\n",
      "A simple example is part-of-speech tagging in which each word of a sentence is labeled\n",
      "with a word class. A probe for part-of-speech tagging can be a classifier that takes a\n",
      "representation hjeach time and outputs the corresponding word class. One general\n",
      "6. In general, hiandhjare the outputs of the last layer of the encoder. Alternatively, they can be weighted\n",
      "sums of the outputs of all the layers.\n",
      "27\n",
      "Page 28:\n",
      "Xiao and Zhu\n",
      "probing approach to these problems is edge probing (Tenney et al., 2019a,b). Given\n",
      "a sentence, a labeled edge is defined as a tuple\n",
      "(span1,span2,label)\n",
      "where span1is a span [ i1, j1], and span2is another span [ i2, j2] (optionally), and label\n",
      "is the corresponding label. Our goal is to learn a probe to predict label given span1\n",
      "and span2. For example, for part-of-speech tagging, span1is a unit span [ j, j] for each\n",
      "position j, span2is an empty span, and label is the part-of-speech tag corresponding\n",
      "to the j-th word of the sentence; for dependency parsing and coreference resolution,\n",
      "span1and span2are two words or entities, and label is the relationship between them;\n",
      "for constituency parsing, span1is a span of words, span2is an empty span, and label\n",
      "is the syntactic category of the tree node yielding span1. In simple cases, the probing\n",
      "model can be a multi-layer feed-forward neural network with a Softmax output layer.\n",
      "As usual, this model is trained on labeled data, and then tested on new data.\n",
      "•Surface Forms of Words and Sentences . Probing tasks can also be designed\n",
      "to examine whether the representations embed the surface information of sentences\n",
      "or words (Adi et al., 2016; Conneau et al., 2018). A simple sentence-level probing\n",
      "task is sentence length prediction . To do this, we first represent the sentence as\n",
      "a single vector h7, and then build a classifier to categorize hinto the corresponding\n",
      "length bin. Similarly, probes can be built to predict whether two words at positions i\n",
      "andjare reordered in the sentence given hiandhj. Also, we can develop probes to\n",
      "address conventional problems in morphology. For example, we reconstruct the word\n",
      "at position jor predict its sense with the representation hj. In addition, probing tasks\n",
      "can be focused on particular linguistic problems, for example, numeracy (Wallace\n",
      "et al., 2019) and function words (Kim et al., 2019).\n",
      "•Cloze . Of course, we can probe neural models for problems beyond syntax and\n",
      "morphology. One perspective on large-scale pre-trained Transformer models is to view\n",
      "them as knowledge bases containing facts about the world. It is therefore tempting\n",
      "to see if we can apply them to test factual knowledge. A simple method is to ask a\n",
      "probe to recover the missing item of a sentence (Petroni et al., 2019). For example, if\n",
      "we have a cloze test\n",
      "Shiji was written by .\n",
      "we wish the probe to give an answer Sima Qian because there is a subject-object-\n",
      "relation fact (Shiji, Sima Qian, written-by). This probe can simply be a masked\n",
      "language model that is widely used in self-supervised learning of Transformer en-\n",
      "coders.\n",
      "In NLP, probing is closely related to pre-training of large language models. In general, we\n",
      "can see probing tasks as applications of these pre-trained language models, though probing\n",
      "is ordinarily used to give a quick test of the models. Ideally we would like to develop a\n",
      "probe that makes best use of the representations to deal with the problems. However, when\n",
      "7.hcan be computed by performing a pooling operation on {h1, ...,hm}\n",
      "28\n",
      "Page 29:\n",
      "Introduction to Transformers\n",
      "a probe is complex and sufficiently well-trained, it might be difficult to say if the problem\n",
      "is solved by using the representations or the probe itself. A common way to emphasize the\n",
      "contribution of probes in problem-solving is to compare them with reasonable baselines or\n",
      "conduct the comparison on control tasks Belinkov (2022); Hewitt and Liang (2019).\n",
      "4. Improved Architectures\n",
      "In this section we present several improvements to the vanilla Transformer model. Unlike\n",
      "the previous section, most of the improvements are from the perspective of machine learning,\n",
      "rather than linguistics.\n",
      "4.1 Locally Attentive Models\n",
      "Methods of self-attention, as discussed Section 2.3, can also be viewed as learning represen-\n",
      "tations of the entire input sequence. The use of this global attention mechanism can lead to\n",
      "a better ability to deal with long-distance dependencies, but this model has a shortcoming:\n",
      "local information is not explicitly captured. Here we consider a few techniques that attempt\n",
      "to model the localness of representations.\n",
      "4.1.1 Priors of Local Modeling\n",
      "One of the simplest ways of introducing local models into Transformers is to add a penalty\n",
      "term to the attention function in order to discourage large attention weights between distant\n",
      "positions. On the encoder-side, this leads to a form that we have already encountered several\n",
      "times in this chapter.\n",
      "AttLocal self(H) = Softmax(Hq[Hk]T\n",
      "√\n",
      "d−γ·G)Hv(64)\n",
      "where γis the weight (or temperature) of the penalty term, and G∈Rm×mis the matrix\n",
      "of penalties. Each entry G(i, j) indicates how much we penalize the model given positions\n",
      "iandj. A simple form of G(i, j) is a distance metric between iandj, for example\n",
      "G(i, j) = |i−j| (65)\n",
      "OrG(i, j) can be defined as a Gaussian penalty function (Yang et al., 2018)\n",
      "G(i, j) =(i−j)2\n",
      "2σ2\n",
      "i(66)\n",
      "where σiis the standard deviation of the Gaussian distribution. For different j, both of the\n",
      "above penalty terms increase, linearly or exponentially, away from the maximum at iwith\n",
      "distance |i−j|.\n",
      "This method can be extended to the cross-attention model, like this\n",
      "AttLocal cross(H,S) = Softmax(Sq[Hk]T\n",
      "√\n",
      "d−γ·G)Hv(67)\n",
      "29\n",
      "Page 30:\n",
      "Xiao and Zhu\n",
      "where Gis an n×mmatrix. Each entry of Gcan be defined as\n",
      "G(i, j) =(µi−j)2\n",
      "2σ2\n",
      "i(68)\n",
      "where µiis the mean of the Gaussian distribution over the source-side positions. Both µi\n",
      "andσican be determined using heuristics. Alternatively, we can develop additional neural\n",
      "networks to model them and learn corresponding parameters together with other parameters\n",
      "of the Transformer model. For example, we can use a feed-forward neural network to predict\n",
      "µigiven si.\n",
      "One alternative to Eq. (64) (or Eq. (67)) treats the penalty term as a separate model\n",
      "and combines it with the original attention model. For example, we can define the self-\n",
      "attention model as\n",
      "AttLocal self(H) =\u0012\n",
      "(1−β)·Softmax(Hq[Hk]T\n",
      "√\n",
      "d) +β·Softmax( −γ·G)\u0013\n",
      "Hv(69)\n",
      "where β∈[0,1] is the coefficient of the linear combination. Note that, to avoid empirical\n",
      "choices of the values of αandβ, we can use gating functions to predict αandβand train\n",
      "these functions as usual.\n",
      "Another alternative is to use a multiplicative mask to incorporate the prior into model-\n",
      "ing, as in Eq. (54). This is given by\n",
      "AttLocal self(H) = Softmax(Hq[Hk]T\n",
      "√\n",
      "d⊙G′)Hv(70)\n",
      "Here G′∈[0,1]m×mis a matrix of scalars. The scalar G′(i, j) gives a value of 1 when\n",
      "i=j, and a smaller value as jmoves away from i.G′(i, j) can be obtained by normalizing\n",
      "−G(i, j) over all jor using alternative functions.\n",
      "4.1.2 Local Attention\n",
      "The term local attention has been used broadly to cover a wide range of problems and\n",
      "to refer to many different models in the NLP literature. The methods discussed above\n",
      "are those that impose soft constraints on attention models. In fact, local attention has its\n",
      "origins in attempts to restrict the scope of attention models for considerations of modeling\n",
      "and computational problems (Luong et al., 2015). Research in this area often looks into\n",
      "introducing hard constraints, so that the resulting models can focus on parts of the input and\n",
      "ignore the rest. For example, we can predict a span of source-side positions for performing\n",
      "the attention function given a target-side position (Sperber et al., 2018; Sukhbaatar et al.,\n",
      "2019; Yang et al., 2018). Also, attention spans can be induced from syntax trees, for\n",
      "example, knowing sub-tree structures of a sentence may help winnow the field that the\n",
      "model concentrates on in learning the representation. Thus, many of the syntax-constrained\n",
      "models are instances of local attention-based models (see Section 3.4) . In addition, the\n",
      "concept of local attention can be extended to develop a rich set of models, such as sparse\n",
      "attention models , although these models are often discussed in the context of efficient\n",
      "machine learning methods. We will see a few examples of them in Section 5.\n",
      "30\n",
      "Page 31:\n",
      "Introduction to Transformers\n",
      "In deep learning, one of the most widely used models for learning features from a re-\n",
      "stricted region of the input is CNNs. It is thus interesting to consider methods of combin-\n",
      "ing CNNs and Transformer models to obtain the benefits of both approaches, for example,\n",
      "CNNs deal with short-term dependencies, and self-attention models deal with long-term\n",
      "dependencies. One approach is to build a two-branch sequence model where one branch is\n",
      "based on CNNs and the other is based on self-attention models (Wu et al., 2020). Another\n",
      "approach is to incorporate CNN layers into Transformer blocks in some way that we can\n",
      "learn both local and global representations through a deep model (Gulati et al., 2020; Wu\n",
      "et al., 2019).\n",
      "4.1.3 Relative Positional Embedding\n",
      "Relative positional embedding, also known as relative positional representation (RPR ),\n",
      "is an improvement to the absolute positional embedding method used in standard Trans-\n",
      "former systems (Huang et al., 2018; Shaw et al., 2018). The idea of RPR is that we model\n",
      "the distance between two positions of a sequence rather than giving each position a fixed\n",
      "representation. As a result, we have a pair-wise representation PE( i, j) for any two positions\n",
      "iandj. One simple way to define PE( i, j) is to consider it as a lookup table for all pairs of\n",
      "iandj. More specifically, let uπbe a d-dimensional representation for a given distance π.\n",
      "The form of PE( i, j) in the vanilla RPR method is given by\n",
      "PE(i, j) = uclip(j−i,krpr) (71)\n",
      "where clip( x, krpr) is a function that clips xin the interval [ −krpr, krpr]\n",
      "clip(x, krpr) = max {−krpr,min{x, krpr}} (72)\n",
      "Thus, we have a model with parameters\n",
      "Urpr=\n",
      "u−krpr...\n",
      "u0\n",
      "...\n",
      "ukrpr\n",
      "(73)\n",
      "While this matrix notation is used in a relatively informal way, we can view Urpras a matrix\n",
      "∈R(2krpr+1)×d, and select a row corresponding to clip( j−i, krpr) when RPR is required for\n",
      "given iandj.\n",
      "Using the above method, we can define three RPR models PEq(i, j), PEk(i, j) and\n",
      "PEv(i, j) for queries, keys, and values, respectively. Then, following the form of Eq. (17),\n",
      "the output of the self-attention model at position ican be written as\n",
      "ci=mX\n",
      "j=1αi,j\u0002\n",
      "hv\n",
      "j+ PEv(i, j)\u0003\n",
      "=mX\n",
      "j=1αi,jhv\n",
      "j+mX\n",
      "j=1αi,jPEv(i, j) (74)\n",
      "31\n",
      "Page 32:\n",
      "Xiao and Zhu\n",
      "Self-attention Sub-layerFFN Sub-layerSelf-attention Sub-layerFFN Sub-layer···\n",
      "block 1block 2\n",
      "x1 x2··· xm\n",
      "+ + +\n",
      "PE(1) PE(2) ··· PE(m)\n",
      "(a) Transformer without RPRSelf-attention Sub-layerFFN Sub-layerSelf-attention Sub-layerFFN Sub-layer···\n",
      "block 1block 2\n",
      "x1 x2··· xm\n",
      "+ + +\n",
      "PE(1) PE(2) ··· PE(m)\n",
      "1 2··· mu−3u−3u−2u−1u0\n",
      "u−3u−2u−1u0u1\n",
      "u−2u−1u0u1u2\n",
      "u−1u0u1u2u3\n",
      "u0u1u2u3u35\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "12345 jiPE(i, j) (krpr= 3)\n",
      "···...···\n",
      "(b) Transformer with RPR\n",
      "Figure 9: Transformer encoders without and with relative positional representation (RPR).\n",
      "In RPR, each pair of positions is represented as a vector PE( i, j) using a model\n",
      "parameterized by Urpr. PE( i, j) is fed into each self-attention sub-layer so that\n",
      "we can make use of the positional information in intermediate steps of learning\n",
      "representations.\n",
      "where hv\n",
      "jis the j-th row vector of Hv. This representation comprises two components:Pm\n",
      "j=1αi,jhv\n",
      "jis the basic representation, andPm\n",
      "j=1αi,jPEv(i, j) is the positional represen-\n",
      "tation.\n",
      "The attention weight αi,jis computed in a regular way, but with additional terms\n",
      "PEq(i, j) and PEk(i, j) added to each query and key.\n",
      "αi,j= Softmax([hq\n",
      "i+ PEq(i, j)][hk\n",
      "j+ PEk(i, j)]T\n",
      "√\n",
      "d) (75)\n",
      "Figure 9 shows the Transformer encoder architectures with and without RPR. When\n",
      "RPR is adopted, PEq(i, j), PEk(i, j), PEv(i, j) are directly fed to each self-attention sub-\n",
      "layer, and so we can make better use of positional information for sequence modeling. Note\n",
      "that, the use of the clipping function (see Eq. (72)) makes the modeling simple because we\n",
      "do not need to distinguish the relative distances for the cases |j−i| ≥krpr. This clipped\n",
      "distance-based model can lead, in turn, to better modeling in local context windows.\n",
      "32\n",
      "Page 33:\n",
      "Introduction to Transformers\n",
      "Eqs. (74) and (75) provide a general approach to position-sensitive sequence modeling.\n",
      "There are many variants of this model. In Shaw et al. (2018)’s early work on RPR, the\n",
      "positional representations for queries are removed, and the model works only with PEk(i, j)\n",
      "and PEv(i, j), like this\n",
      "αi,j= Softmax(hq\n",
      "i[hk\n",
      "j+ PEk(i, j)]T\n",
      "√\n",
      "d) (76)\n",
      "By contrast, there are examples that attempt to improve the RPR model in computing\n",
      "attention weights but ignore PEv(i, j) in learning values (Dai et al., 2019; He et al., 2021).\n",
      "Instead of treating RPR as an additive term to each representation, researchers also explore\n",
      "other ways of introducing RPR into Transformer (Huang et al., 2020; Raffel et al., 2020).\n",
      "We refer the interested readers to these papers for more details.\n",
      "4.2 Deep Models\n",
      "Many state-of-the-art NLP systems are based on deep Transformer models. For example, re-\n",
      "cent large language models generally comprise tens of Transformer layers (or more precisely,\n",
      "hundreds of layers of neurons), demonstrating strong performance on many tasks (Ouyang\n",
      "et al., 2022; Touvron et al., 2023a). By stacking Transformer layers, it is straightforward to\n",
      "obtain a deep model. However, as is often the case, training very deep neural networks is\n",
      "challenging. A difficulty arises from the fact that the error surfaces of deep neural networks\n",
      "are highly non-convex and have many local optima that make the training process likely\n",
      "to get stuck in them. While there are optimization algorithms that can help alleviate this\n",
      "problem, most of the practical efforts explore the use of gradient-based methods for opti-\n",
      "mizing deep neural networks. As a result, training a model with many Transformer layers\n",
      "becomes challenging due to vanishing and exploding gradients during back-propagation.\n",
      "Here we consider several techniques for training deep Transformer models.\n",
      "4.2.1 Re-thinking the Pre-Norm and Post-Norm Architectures\n",
      "As introduced previously, a Transformer sub-layer is a residual network where a shortcut is\n",
      "created to add the input of the network directly to the output of this sub-layer. This allows\n",
      "gradients to flow more directly from the output back to the input, mitigating the vanishing\n",
      "gradient problem. In general, a residual connection in Transformer is used together with\n",
      "a layer normalization unit to form a sub-layer. This leads to two types of architecture,\n",
      "called post-norm and pre-norm. To be specific, recall from Section 2.4 that the post-norm\n",
      "architecture can be expressed as\n",
      "zl= LNorm( Fl(zl−1) +zl−1) (77)\n",
      "where zlandzl−1are the output and input of the sub-layer l, and Fl(·) is the core function\n",
      "of this sub-layer. The pre-norm architecture takes the identity mapping zloutside the layer\n",
      "normalization function, given in the form\n",
      "zl= LNorm( Fl(zl−1)) +zl−1(78)\n",
      "Consider the difference between the information flow in these two architectures:\n",
      "33\n",
      "Page 34:\n",
      "Xiao and Zhu\n",
      "•The post-norm architecture prevents the identity mapping of the input from adding\n",
      "to the output of the sub-layer. This is not a true residual network, because all the\n",
      "information is passed on through a non-linear function (i.e., the layer normalization\n",
      "unit). Thus, the post-norm architecture is not very “efficient” for back-propagation.\n",
      "Wang et al. (2019) show that the gradient of the loss of an Lsub-layer Transformer\n",
      "network with respect to zlis given by\n",
      "∂E\n",
      "∂zl=∂E\n",
      "∂zL·L−1Y\n",
      "k=l∂LNorm( vk)\n",
      "∂vk·L−1Y\n",
      "k=l\u0012\n",
      "1 +∂Fk(zk)\n",
      "∂zk\u0013\n",
      "(79)\n",
      "where zLis the output of the last layer, vkis a short for Fk(zk−1), and Eis the error\n",
      "measured by some loss function.∂LNorm( vk)\n",
      "∂vk and∂Fk(zk)\n",
      "∂zkare the gradients of the layer\n",
      "normalization function and the core function, respectively. Although the equation\n",
      "here appears a bit complex, we see thatQL−1\n",
      "k=l∂LNorm( vk)\n",
      "∂vk is simply a product of L−l\n",
      "factors. This means that the error gradient will be rescaled more times if Lbecomes\n",
      "larger, and there is a higher risk of vanishing and exploding gradients for a deeper\n",
      "model.\n",
      "•The pre-norm architecture describes a standard residual neural network where the\n",
      "input of a whole network is added to its output. We can write the gradient of the\n",
      "error at zlas\n",
      "∂E\n",
      "∂zl=∂E\n",
      "∂zL· \n",
      "1 +L−1Y\n",
      "k=l∂Fk(LNorm( zk))\n",
      "∂zk!\n",
      "=∂E\n",
      "∂zL+∂E\n",
      "∂zL·L−1Y\n",
      "k=l∂Fk(LNorm( zk))\n",
      "∂zk(80)\n",
      "It is easy to see that∂E\n",
      "∂zlreceives direct feedback regarding the errors made by the\n",
      "model, because the first term of the summation on the right-hand side (i.e.,∂E\n",
      "∂zL) is\n",
      "the gradient of the model output which is independent of the network depth.\n",
      "The use of the pre-norm architecture also helps optimization during early gradient de-\n",
      "scent steps. For example, it has been found that pre-norm Transformer models can be\n",
      "trained by using a larger learning rate in the early stage of training instead of gradually\n",
      "increasing the learning rate from a small value (Xiong et al., 2020).\n",
      "While the pre-norm architecture leads to easier optimization of deep Transformer mod-\n",
      "els, we would not simply say that it is a better choice compared to the post-norm architec-\n",
      "ture. In fact, both post-norm and pre-norm Transformer models have been successfully used\n",
      "in many applications. For example, the post-norm architecture is widely used in BERT-\n",
      "like models, while the pre-norm architecture is a more popular choice in recent generative\n",
      "large language models. Broadly, these two architectures provide different ways to design\n",
      "a deep Transformer model, as well as different advantages and disadvantages in doing so.\n",
      "The post-norm architecture forces the representation to be learned through more non-linear\n",
      "functions, but in turn results in a complicated model that is relatively hard to train. By\n",
      "contrast, the pre-norm architecture can make the training of Transformer models easier, but\n",
      "34\n",
      "Page 35:\n",
      "Introduction to Transformers\n",
      "would be less expressive than the post-norm counterpart if the learned models are overly\n",
      "dependent on the shortcut paths.\n",
      "An improvement to these architectures is to control the extent to which we want to\n",
      "“skip” a sub-layer. A simple way to do this is to weight different paths rather than treating\n",
      "them equally. For example, a scalar factor of a residual connection can be introduced to\n",
      "determine how heavily we weight this residual connection relative to the path of the core\n",
      "function (He et al., 2016b; Liu et al., 2020a,b). A more general form of this model is given\n",
      "by\n",
      "zl= LNorm( Fl(zl−1) +β·zl−1) +γ·zl−1(81)\n",
      "where βis the weight of the identity mapping inside the layer normalization function, and\n",
      "γis the weight of the identity mapping outside the layer normalization function. Clearly,\n",
      "both the post-norm and pre-norm architectures can be seen as special cases of this equation.\n",
      "That is, if β= 1 and γ= 0, then it will become Eq. (77); if β= 0 and γ= 1, it will become\n",
      "Eq. (78). This model provides a multi-branch view of building residual blocks. The input\n",
      "to this block can be computed through multiple paths with different modeling complexities.\n",
      "When βandγare small, the representation is forced to be learned through a “deep” model\n",
      "with multiple layers of cascaded non-linear units. In contrast, when βandγare large, the\n",
      "representation is more likely to be learned using a “shallow” model with fewer layers. To\n",
      "determine the optimal choices of βandγ, one can give them fixed values by considering some\n",
      "theoretical properties or system performance on validation sets, or compute these values by\n",
      "using additional functions that can be trained to do so (Srivastava et al., 2015). It should\n",
      "be emphasized that many other types of architecture can be considered in the design of a\n",
      "Transformer sub-layer. It is possible, for instance, to introduce more layer normalization\n",
      "units into a sub-layer (Ding et al., 2021; Wang et al., 2022b), or, on the contrary, to simply\n",
      "remove them from a sub-layer (Bachlechner et al., 2021).\n",
      "4.2.2 Parameter Initialization\n",
      "As with other deep neural networks, there is interest in developing parameter initialization\n",
      "methods for deep Transformer models in order to perform optimization on some region\n",
      "around a better local optimum. However, initialization is a wide-ranging topic for opti-\n",
      "mization of machine learning models, and the discussion of this general topic lies beyond\n",
      "the scope of this section. Here we will discuss some of the parameter initialization methods\n",
      "used in Transformer-based systems rather than the general optimization problems.\n",
      "While the parameters of a neural network can be set in various different ways, most\n",
      "practical systems adopt simple techniques to give appropriate initial values of model param-\n",
      "eters. Consider, for example, the Xavier initialization for a parameter matrix W∈Rdin×dout\n",
      "(Glorot and Bengio, 2010). We define a variable ηby\n",
      "η= gain ·r\n",
      "6\n",
      "din+dout(82)\n",
      "where gain is a hyper-parameter which equals 1 by default. Then, each entry of Wcan be\n",
      "initialized by using a uniform distribution\n",
      "W∼U(−η, η) (83)\n",
      "35\n",
      "Page 36:\n",
      "Xiao and Zhu\n",
      "or, alternatively, using a Gaussian distribution\n",
      "W∼Gaussian\u0000\n",
      "0, η2\u0001\n",
      "(84)\n",
      "This method can be easily adapted to initialize Transformer models having a large\n",
      "number of layers. One common way is to find a more suitable value of gain by taking\n",
      "into account the fact that the initial states of optimization might be different for neural\n",
      "networks of different depths. For example, one can increase the value of gain as the depth\n",
      "of the model grows. Then, gain can be defined as a function of the network depth in the\n",
      "form\n",
      "gain = a·Lb(85)\n",
      "where ais the scalar, and Lbis the network depth raised to the power of b. Typically, aand\n",
      "bcan be positive numbers, which means that it is preferred to have larger initial values for\n",
      "the parameters for deeper models. For example, Wang et al. (2022a) show that, by choosing\n",
      "appropriate values for aandb, a very deep Transformer model can be successfully trained.\n",
      "Eq. (85) assigns gain the same value for all of the sub-layers. However, it is found that\n",
      "the norm of gradients becomes smaller when a sub-layer moves away from the output layer.\n",
      "This consistent application of gain across the entire model could result in under-training of\n",
      "the lower layers due to the gradient vanishing problem. For this reason, one can develop\n",
      "methods that are sensitive to the position of a sub-layer in the neural network. The general\n",
      "form of such methods is given by\n",
      "gain =a\n",
      "lb(86)\n",
      "Here ldenotes the depth of a sub-layer. If lis larger (i.e., the sub-layer is closer to the\n",
      "output), gain will be smaller and the corresponding parameters will be set to smaller values.\n",
      "An example of this method can be found in Zhang et al. (2019)’s work.\n",
      "It is also, of course, straightforward to apply general methods of initializing deep multi-\n",
      "layer neural networks to Transformer models. An example is to consider the Lipschitz\n",
      "constant in parameter initialization, which has been shown to help improve the stability\n",
      "of training deep models (Szegedy et al., 2014; Xu et al., 2020). Another approach is to use\n",
      "second-order methods to estimate the proper values of the parameters. For example, one\n",
      "can compute the Hessian of each parameter matrix to model its curvature (Skorski et al.,\n",
      "2021).\n",
      "For models with a large number of layers, it is also possible to pre-train some of the\n",
      "layers via smaller models and use their trained parameters to initialize bigger models (Chen\n",
      "et al., 2015). That is, we first obtain a rough estimation of the parameters in a cheap way,\n",
      "and then continue the training process on the whole model as usual. These methods fall\n",
      "into a class of training methods, called model growth ordepth growth .\n",
      "As a simple example, consider a Transformer model (e.g., a Transformer encoder) of 2 L\n",
      "sub-layers. We can train this model by using the shallow-to-deep training method (Li\n",
      "et al., 2020b). First, we train an L-sub-layer model (call it the shallow model) in a regular\n",
      "way. Then, we create a 2 L-sub-layer model (call it the deep model) by stacking the shallow\n",
      "model twice, and further train this deep model. To construct deeper models, this procedure\n",
      "36\n",
      "Page 37:\n",
      "Introduction to Transformers\n",
      "can be repeated multiple times, say, we start with a model of Lsub-layers, and obtain a\n",
      "model of L2Iafter Iiterations. Note that many of the pre-training models are used in\n",
      "the same manner. For example, for BERT-like methods, a transformer encoder is trained\n",
      "on large-scale data, and the optimized parameters are then used to initialize downstream\n",
      "systems.\n",
      "4.2.3 Layer Fusion\n",
      "Another problem with training a deep Transformer model is that the prediction is only\n",
      "conditioned on the last layer of the neural network. While the use of residual connections\n",
      "enables the direct access to lower-level layers from a higher-level layer, there is still a “long”\n",
      "path of passing information from the bottom to the top. One simple way to address this\n",
      "is to create residual connections that skip more layers. For example, consider a group of L\n",
      "Transformer sub-layers. For the sub-layer at depth l, we can build l−1 residual connections,\n",
      "each connecting this sub-layer with a previous sub-layer. In this way, we develop a densely\n",
      "connected network where each sub-layer takes the outputs of all previous sub-layers (Huang\n",
      "et al., 2017). The output of the last sub-layer can be seen as some combination of the\n",
      "outputs at different levels of representation of the input.\n",
      "Following the notation used in the previous sub-sections, we denote the output of the\n",
      "sub-layer at depth lbyzl, and denote the function of the sub-layer by Layerl(·). Then, zl\n",
      "can be expressed as\n",
      "zl= Layerl(z1, ...,zl−1) (87)\n",
      "We can simply view Layerl(·) as a function that fuses the information from {z1, ...,zl−1}.\n",
      "There are many possible choices for Layerl(·). For example, a simple form of Layerl(·) is\n",
      "given by\n",
      "Layerl(z1, ...,zl−1) = LNorm( Fl(Zl)) (88)\n",
      "Zl=ϕ(z1, ...,zl−1) (89)\n",
      "Here ϕ(·) takes the layer outputs {z1, ...,zl−1}and fuses them into a single representation\n",
      "Zl. A simple instance of ϕ(·) is average pooling which computes the sum of {z1, ...,zl−1}\n",
      "divided by l−1. See Table 3 for more examples of ϕ(·).\n",
      "Taking a similar architecture of a Transformer sub-layer, we can also consider a post-\n",
      "norm form\n",
      "Layerl(z1, ...,zl−1) = LNorm( Zl) (90)\n",
      "Zl=ϕ(Fl(zl−1),z1, ...,zl−1) (91)\n",
      "or a pre-norm form\n",
      "Layerl(z1, ...,zl−1) = Zl(92)\n",
      "Zl=ϕ(LNorm( Fl(zl−1)),z1, ...,zl−1) (93)\n",
      "These models are very general. For example, a standard post-norm encoder sub-layer\n",
      "can be recovered as a special case of Eqs. (90-91), if we remove the dependencies of sub-\n",
      "layers from 1 to l−2, and define ϕ(·) to be\n",
      "ϕ(Fl(zl−1),z1, ...,zl−1) = Fl(zl−1) +zl−1(94)\n",
      "37\n",
      "Page 38:\n",
      "Xiao and Zhu\n",
      "Entry Function\n",
      "Average Pooling ϕ(z1, ...,zl−1) =1\n",
      "l−1Pl−1\n",
      "k=1zk\n",
      "Weighted Sum ϕ(z1, ...,zl−1) =Pl−1\n",
      "k=1weightk·zk\n",
      "Feedforward Network ϕ(z1, ...,zl−1) = FFN([ z1, ...,zl−1])\n",
      "Self Attention ϕ(z1, ...,zl−1) = FFN([Att self(z1, ...,zl−1)])\n",
      "Table 3: Fusion functions. FFN( ·) = feedforward neural network, [ ·] = concatenating the\n",
      "input vectors, and Att self(·) = self-attention function. All of the fusion functions\n",
      "can be followed by a layer normalization function, for example, we can write the\n",
      "weighted sum of {z1, ...,zl−1}asϕ(z1, ...,zl−1) = LNorm(Pl−1\n",
      "k=1weightk·zk).\n",
      "Densely connected network makes the information easier to flow through direct connec-\n",
      "tions between sub-layers, but the resulting models are a bit more complex, especially when\n",
      "we use parameterized fusion functions. In practice, we typically add dense connections only\n",
      "to some of the sub-layers, and so the overall networks are not very dense. For example, we\n",
      "only add connections from bottom sub-layers to the last few sub-layers. Thus, the predic-\n",
      "tion can be made by having direct access to different levels of representation (Wang et al.,\n",
      "2018a).\n",
      "4.2.4 Regularization\n",
      "In machine learning, regularization is used to avoid overfitting in training deep neural\n",
      "networks. It is therefore straightforward to apply regularization techniques to Transformer\n",
      "models. Since the regularization issue has been extensively discussed in many papers and\n",
      "books on machine learning, here we consider specific methods applicable to training deep\n",
      "Transformer models.\n",
      "One approach to regularizing a deep Transformer model is to randomly skip sub-layers or\n",
      "layers during training (Huang et al., 2016; Pham et al., 2019). In each run of the model, such\n",
      "as running the backpropgation algorithm on a batch of samples, we select each of the sub-\n",
      "layers with a probability ρ, and stack the selected sub-layers to form a “new” model. Thus,\n",
      "we essentially train different neural networks with shared architectures and parameters on\n",
      "the same dataset. In this way, a sub-layer learns to operate somewhat independently, and\n",
      "so overfitting is reduced by preventing the co-adaption of sub-layers. In fact, dropping\n",
      "out sub-layers (or layers) and dropping out neurons are two different methods on a theme.\n",
      "Sometimes, the method described here is called sub-layer dropout orlayer dropout .\n",
      "At test time, we need to combine all the possible networks to make predictions of\n",
      "some output. A simple method to achieve this is to rescale the outputs of the stochastic\n",
      "components of the model (Li et al., 2021). As an example, suppose each sub-layer has a\n",
      "pre-norm architecture. Then, the output of the sub-layer at depth lis given by\n",
      "zl=ρ·LNorm( Fl(zl−1)) +zl−1(95)\n",
      "Another idea is to force the parameters to be shared across sub-layers. One of the\n",
      "simplest methods is to use the same parameters for all the corresponding sub-layers (De-\n",
      "hghani et al., 2018), for example, all the FFN sub-layers are based on the same feedforward\n",
      "38\n",
      "Page 39:\n",
      "Introduction to Transformers\n",
      "network. This method has a similar effect as the methods that add norms of parameter\n",
      "matrices to the loss function for penalizing complex models. For practical systems, there\n",
      "can be significant benefit in adopting a shared architecture because we can reuse the same\n",
      "sub-model to build a multi-layer neural network and reduce the memory footprint. We will\n",
      "see more discussions on the efficiency issue in Section 5.4.\n",
      "4.3 Numerical Method-Inspired Models\n",
      "A residual network computes its output through the sum of the identity mapping and some\n",
      "transformation of the input. Such a model can be interpreted as an Euler discretization\n",
      "ofordinary differential equations (ODEs ) (Ee, 2017; Haber and Ruthotto, 2017). To\n",
      "illustrate this idea, we consider a general form of residual networks\n",
      "zl=fl\u0000\n",
      "zl−1\u0001\n",
      "+zl−1(96)\n",
      "where fl(zl−1) denotes a function takes an input variable zl−1and produces an output\n",
      "variable in the same space. Clearly, a Transformer sub-layer is a special case of this equation.\n",
      "For example, for pre-norm Transformer, we have fl(·) = LNorm( Fl(·)).\n",
      "For notational simplicity, we rewrite the above equation in an equivalent form\n",
      "z(l) = f\u0000\n",
      "z(l−1), l\u0001\n",
      "+z(l−1) (97)\n",
      "We use the notations z(l) and f(z(·, l)) to emphasize that z(·) and f(·) are functions of l.\n",
      "Here we assume that lis a discrete variable. If we relax lto a continuous variable and z(l)\n",
      "to a continuous function of l, then we can express Eq. (97) as\n",
      "z(l) = △l·f\u0000\n",
      "z(l− △l), l\u0001\n",
      "+z(l− △l) (98)\n",
      "This can be further written as\n",
      "z(l)−z(l− △l)\n",
      "△l=f\u0000\n",
      "z(l− △l), l\u0001\n",
      "(99)\n",
      "Taking the limit △l→0, we have an ODE\n",
      "dz(l)\n",
      "dl=f\u0000\n",
      "z(l), l\u0001\n",
      "(100)\n",
      "We say that a pre-norm Transformer sub-layer (i.e., Eqs. (97) and (96)) is an Euler\n",
      "discretization of solutions to the above ODE. This is an interesting result! A sub-layer is\n",
      "actually a solver of the ODE.\n",
      "Eqs. (97) and (96) are standard forms of the Euler method . It computes a new\n",
      "estimation of the solution by moving from an old estimation one step forward along l. In\n",
      "general, two dimensions can be considered in design of numerical methods for ODEs.\n",
      "•Linear Multi-step Methods . A linear multi-step method computes the current\n",
      "estimation of the solutions by taking the estimations and derivative information from\n",
      "multiple previous steps. A general formulation of p-step methods can be expressed as\n",
      "z(l) =pX\n",
      "i=1ai·z(l−i) +hp+1X\n",
      "i=1bi·f\u0000\n",
      "z(l−i), l−i+ 1\u0001\n",
      "(101)\n",
      "39\n",
      "Page 40:\n",
      "Xiao and Zhu\n",
      "where his the size of the step we move each time8, that is, △lin Eqs. (98) and\n",
      "(99).{ai}and{bi}are coefficients of the solution points and derivatives in the linear\n",
      "combination. Given this definition, we can think of the Euler method as a single-step,\n",
      "low-order method of solving ODEs9.\n",
      "•(Higher-order) Runge-Kutta Methods . Runge-Kutta (RK) methods and their\n",
      "variants provide ways to compute the next step solution by taking intermediate results\n",
      "in solving an ODE. As a result, we obtain higher-order methods but still follow the\n",
      "form of single-step methods, that is, the estimated solution is dependent only on\n",
      "z(l−1) rather than on the outputs at multiple previous steps.\n",
      "In fact, linear multi-step methods, though not explicitly mentioned, have been used in\n",
      "layer fusion discussed in Section 4.2. For example, taking Eqs. (92) and (93) and a linear\n",
      "fusion function, a pre-norm sub-layer with dense connections to all previous sub-layers can\n",
      "be expressed as\n",
      "Layerl(z1, ...,zl−1) = a1·zl−1+...+al−1·z1+b1·LNorm( Fl(zl−1)) (102)\n",
      "This equation is an instance of Eq. (101) where we set h= 1 and remove some of the terms\n",
      "on the right-hand side.\n",
      "It is also straightforward to apply Runge-Kutta methods to Transformer (Li et al.,\n",
      "2022a). Given an ODE as described in Eq. (100), an explicit p-order Runge-Kutta solution\n",
      "is given by\n",
      "z(l) = z(l−1) +pX\n",
      "i=1γi·gi (103)\n",
      "gi=h·f\u0000\n",
      "z(l−1) +i−1X\n",
      "j=1βi,j·gj, l−1 +λi·h\u0001\n",
      "(104)\n",
      "Here girepresents an intermediate step which is present only during the above process.\n",
      "{γi},{βi,j}and{λi}are coefficients that are determined by using the Taylor series of z(l).\n",
      "To simplify the model, we assume that the same function fis used for all {gi}. Then, we\n",
      "remove the dependency of the term l−1 +λi·hinf, and rewrite Eq. (104) as\n",
      "gi=h·f\u0000\n",
      "z(l−1) +i−1X\n",
      "j=1βi,j·gj\u0001\n",
      "(105)\n",
      "where f(·) is a function independent of i.\n",
      "8. Let {t0, ..., t i}denote the values of the variable lat steps {0, ..., i}. In linear multi-step methods, it is\n",
      "assumed that ti=t0+ih.\n",
      "9. In numerical analysis, the local truncation error of a method of solving ODEs at a step is defined\n",
      "to be the difference between the approximated solution computed by the method and the true solution.\n",
      "The method is called order pif it has a local truncation error O(hp+1).\n",
      "40\n",
      "Page 41:\n",
      "Introduction to Transformers\n",
      "z(l)\n",
      "f\n",
      "z(l−1)\n",
      "(a) Pre-normz(l)\n",
      "ff\n",
      "z(l−1)1\n",
      "21\n",
      "2\n",
      "(b) RK2z(l)\n",
      "ffff\n",
      "z(l−1)1\n",
      "6\n",
      "1\n",
      "22\n",
      "62\n",
      "61\n",
      "6\n",
      "1\n",
      "2\n",
      "(c) RK4\n",
      "Figure 10: Pre-norm (a) and Runge-Kutta (b and c) sub-layer architectures. z(l−1) denotes\n",
      "the input of a sub-layer at depth l,z(l) denotes the output of the sub-layer, and\n",
      "f(in blue boxes) denotes the function f(·) = LNorm( Fl(·))\n",
      "As an example, consider the 4th-order Runge-Kutta (RK4) solution\n",
      "z(l) = z(l−1) +1\n",
      "6(g1+ 2g2+ 2g3+g4) (106)\n",
      "g1=h·f(z(l−1)) (107)\n",
      "g2=h·f(z(l−1) +1\n",
      "2g1) (108)\n",
      "g3=h·f(z(l−1) +1\n",
      "2g2) (109)\n",
      "g4=h·f(z(l−1) +g3) (110)\n",
      "These equations define a new architecture of sub-layer. For example, by setting h= 1 and\n",
      "f(·) = LNorm( Fl(·)), we obtain an RK4 Transformer sub-layer, as shown in Figure 10. This\n",
      "method leads to a deep model because each sub-layer involves four runs of f(·) in sequence.\n",
      "On the other hand, the resulting model is parameter efficient because we reuse the same\n",
      "function f(·) within the sub-layer, without introducing new parameters.\n",
      "So far in this sub-section our discussion has focused on applying dynamic systems to\n",
      "Transformer models by designing architectures of Transformer sub-layers. While the basic\n",
      "ODE model is continuous with respect to the depth l, these methods still follow the general\n",
      "framework of neural networks in which lis a discrete variable and the representational power\n",
      "of the models is largely determined by this hyper-parameter. An alternative approach is to\n",
      "use neural ODE models to relax the “depth” to a truly continuous variable. In this way,\n",
      "we can have a model with continuous depth for computing the solution of ODEs. However,\n",
      "as the discussion of neural ODE lies beyond the scope of this chapter, we refer the reader\n",
      "to related papers for more details (Chen et al., 2018c; Kidger, 2022).\n",
      "41\n",
      "Page 42:\n",
      "Xiao and Zhu\n",
      "4.4 Wide Models\n",
      "Most of the methods that we have studied so far in this section are examples of learning\n",
      "and using deep models. Another design choice we generally face is to determine the width\n",
      "for a neural network. Typically, the width of a Transformer model can be defined as the\n",
      "number of dimensions of a representation at some position of the input sequence, that is,\n",
      "the parameter d. Increasing this width is a common method to obtain a more complex\n",
      "and more powerful model. For example, in Vaswani et al. (2017)’s work, a wide model\n",
      "(called Transformer big) leads to significant improvements in translation quality for machine\n",
      "translation systems. More recently, wider models have been proposed to boost systems on\n",
      "large-scale tasks (Fedus et al., 2022b; Lepikhin et al., 2021).\n",
      "However, developing very wide Transformer models is difficult. One difficulty is that\n",
      "training such systems is computationally expensive. While the number of the model pa-\n",
      "rameters (or model size) grows linearly with d, the time complexity of the models grows\n",
      "quadratic with d(see Table 1). In some NLP tasks, it is found empirically that the training\n",
      "effort that we need to obtain satisfactory performance is even an exponential function of\n",
      "the model size (Kaplan et al., 2020). These results suggest ways to improve the efficiency\n",
      "of training when we enlarge d.\n",
      "One simple method is to incrementally grow the model along the dimension of d, rather\n",
      "than training the model from scratch. Suppose we have an initial model involving a d1×d1\n",
      "parameter matrix W1, for example, the linear transformation of each query or key in some\n",
      "layer. We can train this model to obtain optimized W1in a regular way. Then, we want to\n",
      "extend this model to a wider model where W1is replaced by a d2×d2parameter matrix\n",
      "W2. Let us assume for simplicity that d2=kd1. There are several ways to expand a d1×d1\n",
      "matrix to a kd1×kd1matrix. The simplest of these may be to use W1to fill W2. We can\n",
      "write W2in the form\n",
      "W2=ktimes\n",
      "W1\n",
      "ρ···W1\n",
      "ρ......\n",
      "W1\n",
      "ρ···W1\n",
      "ρ\n",
      "\n",
      "ktimes(111)\n",
      "where ρis a hyper-parameter that is used to control the norm of W2. For example, if\n",
      "ρ=k,W2will have the same l1norm as W1. The above equation provides a good starting\n",
      "point for training the wide model, and we can train W2as usual after initialization. The\n",
      "procedure can be repeated a number of times for constructing a model with arbitrary width.\n",
      "Both this method and the depth growth method described in Section 4.2 are instances of\n",
      "the general method of model growth. In other words, we can obtain a larger model by\n",
      "extending a small model either vertically or horizontally, or both. Alternative methods\n",
      "for transforming W2toW1involve those considering other mathematical properties of\n",
      "the transformation (Chen et al., 2015). These models can fall under the reusable neural\n",
      "networks where we are concerned with models and algorithms for transferring parameters\n",
      "from small models to (significantly) larger models (Wang et al., 2023).\n",
      "A second difficulty in building a wide Transformer model is the large memory require-\n",
      "ment. Since the feedforward network generally has a larger hidden layer than other parts\n",
      "of the model, it demands relatively more memory as the model becomes wider. Consider\n",
      "42\n",
      "Page 43:\n",
      "Introduction to Transformers\n",
      "the feedforward network described in Section 2.5\n",
      "Hout= FFN( Hin)\n",
      "= ReLU( Hin·Wh+bh)·Wf+bf (112)\n",
      "where Wh∈Rd×dffnandWf∈Rdffn×dare the parameters of the linear transformations.\n",
      "dffnis typically several times larger than d. Therefore, WhandWfwill occupy the model\n",
      "ifdanddffnhave very large values.\n",
      "In some cases, the size of the feedforward network may exceed the memory capacity of\n",
      "a single device. This problem can be addressed by using the mixture-of-experts (MoE )\n",
      "models (Shazeer et al., 2017). An MoE model consists of Mexpert models {e1(·), ..., e M(·)}.\n",
      "Given an input hin∈Rd, each expert model produces an output ek(hin). The output of the\n",
      "MoE model is a linear combination of {e1(hin), ..., e M(hin)}, given by\n",
      "hout=MX\n",
      "i=1gi(hin)·ei(hin) (113)\n",
      "where g(·) is a gating model (also called routing model ). Its output is a vector g(hin) =\u0002\n",
      "g1(hin)... g M(hin)\u0003\n",
      "in which each entry gi(hin) indicates the weight of the corresponding\n",
      "expert model. In many applications, it is assumed that g(hin) is a sparse vector. This means\n",
      "that only a small number of expert models are involved in computing the output. A widely-\n",
      "used form of g(hin) is given by using the Softmax layer\n",
      "g(hin) = Softmax( hin·Wg) (114)\n",
      "where Wg∈Rd×Mis the parameter matrix of the layer. To enforce sparsity on g(hin),\n",
      "we can simply select the top- kentries of g(hin), that is, we set non-top- kentries to 0. An\n",
      "alternative method is to first perform top- kselection on hin·Wgand then normalize the\n",
      "top-kentries using the Softmax function.\n",
      "Letπbe the set of the indices of the top- kexpert models. The MoE model with top- k\n",
      "routing has the following form\n",
      "hout=X\n",
      "i∈πgi(hin)·ei(hin) (115)\n",
      "An advantage of this approach is that we can distribute different expert models to differ-\n",
      "ent processors, making it possible to execute these models on parallel computing machines.\n",
      "In each run of the MoE model, either during training or inference, we only need to activate\n",
      "and use kexpert models rather than all of the expert models. In this way, the MoE approach\n",
      "is automatically learning a sparse model by limiting the number of active expert models\n",
      "each time in training and inference. The sparsity is determined by the hyperparameter k,\n",
      "say, a small value of kleads to a sparse model, and a large value of kleads to a dense model.\n",
      "Let us return to the discussion of Eq. (112). It is straightforward to apply the MoE\n",
      "approach to feedforward neural networks. To simplify the discussion, consider the linear\n",
      "43\n",
      "Page 44:\n",
      "Xiao and Zhu\n",
      "Feed-Forward NetworkAdd & LayerNorm\n",
      "......\n",
      "FFN sub-layer\n",
      "Gating\n",
      "ModelFFN 1(·) FFN 2(·)··· FFN M(·)\n",
      "Hin Hin Hin ··· HinP\n",
      "i∈πgi(Hin)·FFN i(Hin)Hout\n",
      "Figure 11: An illustration of the MoE model applied to an FFN sub-layer. There are M\n",
      "FFNs (call them expert models) and a gating model. Each FFN is weighted by\n",
      "the gating model. The output of the model is the sum of the weighted outputs\n",
      "of the top- kFFNs (denoted by π). Because these FFNs work independently and\n",
      "can be placed on different computing devices, the model can be easily scaled up\n",
      "asMis larger.\n",
      "transformation of the first layer as shown in Eq. (112), that is, Hin·Wh. We can approxi-\n",
      "mate Hin·Whin an MoE form\n",
      "Hin·Wh≈X\n",
      "i∈πgi(Hin)·ei(Hin)\n",
      "=X\n",
      "i∈πgi(Hin)·[Hin·Wi\n",
      "h] (116)\n",
      "HereWhis divided into Mslides (or sub-matrices) {W1\n",
      "h, ...,WM\n",
      "h}, written as\n",
      "Wh=\u0002\n",
      "W1\n",
      "h...WM\n",
      "h\u0003\n",
      "(117)\n",
      "Hence each expert model ei(Hin) =Hin·Wi\n",
      "hsolves a sub-problem of the original linear\n",
      "mapping, and Eq. (116) can be thought of as a divide-and-conquer solution to the matrix\n",
      "multiplication problem.\n",
      "We can, of course, treat any feedforward neural network as an expert model, resulting\n",
      "in the following model\n",
      "Hout=X\n",
      "i∈πgi(Hin)·FFN i(Hin) (118)\n",
      "where FFN i(·) is a “small” feedforward neural network that has the same form as Eq. (112).\n",
      "This model is illustrated with an example in Figure 11. In practical implementations, all\n",
      "these expert models can be run in parallel on different devices, and so the resulting system\n",
      "is efficient.\n",
      "Note that, from a perspective of machine learning, MoE is a general approach to com-\n",
      "bining different neural networks, each of which is developed to address a different aspect\n",
      "44\n",
      "Page 45:\n",
      "Introduction to Transformers\n",
      "of the problem (Masoudnia and Ebrahimpour, 2014; Yuksel et al., 2012). The application\n",
      "here is just a special instance of the general framework of MoE. The approach is also often\n",
      "used to improve the overall performance of predictors, which can be discussed in the field\n",
      "of ensemble learning (Zhou, 2012).\n",
      "Another difficulty in developing large Transformer models is the training instability\n",
      "problem. As with many other large neural networks, straightforward optimization of a\n",
      "Transformer model with a large number of parameters may lead to getting trapped in\n",
      "local minimums, and, occasionally, large spikes in the loss during training (Chowdhery\n",
      "et al., 2022; Fedus et al., 2022b; Lepikhin et al., 2021). Even with careful choices about\n",
      "hyperparameters, training strategies, and initial model parameters, we still encounter the\n",
      "situation that we have to restart the training at some point in order to jump out of the\n",
      "tough regions in optimization. One of the reasons for this training difficulty is that the\n",
      "usual implementations of the linear algebra operations, such as matrix multiplication, will\n",
      "be numerically unstable if they operates on very large vectors and matrices. It is therefore\n",
      "possible to improve the training by considering numerically stable methods instead.\n",
      "5. Efficient Models\n",
      "Efficiency is an important consideration for many practical applications of Transformer\n",
      "models. For example, we may wish to run and/or train a Transformer model given memory\n",
      "and time constraints. Efficiency is not a single problem, but covers a wide range of problems.\n",
      "While these problems can be categorized in several different ways, there are two fundamental\n",
      "aspects one may consider in an efficiency problem.\n",
      "•Time and Space Efficiencies . For a given problem, we wish the model to be small\n",
      "and fast, and meanwhile to be as accurate as possible in solving the problem. For\n",
      "example, in some machine translation applications, we may learn a model with a small\n",
      "number of parameters to fit the model to limited memory, and may develop a fast\n",
      "search algorithm to achieve low-latency translation. A practical difficulty here is that\n",
      "improving efficiency often leads to worse predictions. In many cases, we need to seek\n",
      "a trade-off between efficiency and accuracy.\n",
      "•Scalability . When the problem is scaled up, we wish that the additional effort we\n",
      "made for solving this problem is as small as possible. For example, the training of\n",
      "a neural network is called efficient if it takes a reasonably short time to optimize it\n",
      "as more training samples are involved. Another example of efficiency is that used to\n",
      "measure the amount of resources consumed in processing more inputs. For example,\n",
      "a machine translation system is inefficient in translating long sentences if the memory\n",
      "footprint and latency grow exponentially with the number of input words.\n",
      "In this section, we will not discuss all the issues related to efficiency, which is a very\n",
      "broad topic. We instead consider the widely-used efficient approaches to Transformer-based\n",
      "sequence modeling and generation, some of which are refinements of model architectures,\n",
      "and some of which are model-free approaches and could be used in other systems as well.\n",
      "Most of the discussions here are focused on developing lightweight and fast Transformer\n",
      "models that are relatively robust to long input and output sequences.\n",
      "45\n",
      "Page 46:\n",
      "Xiao and Zhu\n",
      "In general, the same optimization method can be applied to different modules of a\n",
      "Transformer system. To simplify the discussion, we will mostly consider self-attention\n",
      "sub-layers and FFN sub-layers in this section. Our discussion, however, is general and the\n",
      "methods presented here can be applied to other parts of a Transformer system, for example,\n",
      "cross-attention sub-layers.\n",
      "5.1 Sparse Attention\n",
      "In practice, the attention approaches used in Transformer are time consuming, especially\n",
      "when the input sequences are long. To illustrate, consider a Transformer decoder that\n",
      "predicts a distribution of words at a time given the previous words. Suppose the sequence\n",
      "generated by the decoder is of size nand the input of a self-attention sub-layer is an n×d\n",
      "matrix S. First, Sis linearly transformed to obtain the queries Sq∈Rn×d, keys Sk∈Rn×d,\n",
      "and values Sv∈Rn×d. To simplify the notation in this sub-section, we use Q,KandVto\n",
      "represent Sq,Sk, and Sv, respectively.\n",
      "The output of the self-attention sub-layer can then be computed using\n",
      "Attself(S) = AV (119)\n",
      "where Ais an n×nattention matrix or attention map\n",
      "A= Softmax(QKT\n",
      "√\n",
      "d+M) (120)\n",
      "Mis a masking matrix that is used to prevent the model from seeing the right context words\n",
      "at each position, that is, for a position i,M(i, j) = 0 for j≤i, and M(i, j) =−∞otherwise.\n",
      "Both the time and space complexities of the self-attention sub-layer are quadratic functions\n",
      "ofn10. Therefore, if nis large, the model would be computationally expensive.\n",
      "The usual implementation of the above model depends on dense matrix computation, for\n",
      "example, the dense matrix multiplications in Eqs. (119-120). One approach to reducing the\n",
      "amount of memory and the number of floating-point calculations in a dense computation\n",
      "system is to sparsify the problem. To do this, we assume that Ais a sparse matrix, for\n",
      "example, only ϱ·n2entries of Mhave non-zero values, where ϱindicates how sparse the\n",
      "matrix is, also called sparsity ratio . Since we only need to store these non-zero entries, the\n",
      "memory requirement of Acan be reduced by using sparse matrix representations. Another\n",
      "advantage of using a sparse attention matrix is that the models ofQKT\n",
      "√\n",
      "dandAVcan\n",
      "be simplified, as we consider only a “small” number of related positions when learning a\n",
      "representation.\n",
      "Given a position i, we define the attention field πito be the set of positions that\n",
      "are considered in computing the representation at this position. We therefore only need to\n",
      "compute the dot-product attention between the given position iand each position j∈πi.\n",
      "This results in a sparse attention matrix A′where\n",
      "A′(i, j) =(\n",
      "some weight j∈πiandj≤i\n",
      "0 otherwise(121)\n",
      "10. More precisely, the amount of memory used by the self-attention function is n2+n·d, and so it will be\n",
      "dominated by the quadratic term n2ifn >> d .\n",
      "46\n",
      "Page 47:\n",
      "Introduction to Transformers\n",
      "A simple implementation of this model involves a slight modification to M, leading to a\n",
      "new masking variable M′\n",
      "M′(i, j) =(\n",
      "0 j∈πiandj≤i\n",
      "−∞ otherwise(122)\n",
      "In practical implementation, a more efficient approach is to employ sparse operations for\n",
      "QKTandA′Vby considering M′andA′, respectively. That is, we save on computation\n",
      "for pairs of positions whose attention weights are non-zero, and skip the rest.\n",
      "There are several approaches that we can take to the sparse modeling of self-attention.\n",
      "We describe briefly some of them as follows\n",
      "•Span-based Attention /Local Attention . As discussed in Section 4.1, the use of\n",
      "context in sequence modeling is local in many cases. The basic idea of local attention\n",
      "is to span the attention weights to a restricted region of the input sequence. We can\n",
      "then write πias\n",
      "πi= [al\n",
      "i, ar\n",
      "i] (123)\n",
      "where al\n",
      "iandar\n",
      "iand the left and right ends of πi.ar\n",
      "i−al\n",
      "i+ 1 determines how small\n",
      "the region is, and so we can use it to control the sparsity of the attention model,\n",
      "for example, if ar\n",
      "i−al\n",
      "i+ 1<< n , the model would be very sparse. al\n",
      "iandar\n",
      "ican be\n",
      "obtained by using either heuristics or machine learning methods. The reader may refer\n",
      "to related papers for more details (Luong et al., 2015; Sperber et al., 2018; Sukhbaatar\n",
      "et al., 2019; Yang et al., 2018). See Figure 12 (b) for an illustration of local attention.\n",
      "•Chunked Attention . When a problem is too difficult to solve, one can transform it\n",
      "into easier problems and solve each of them separately, as is often the case in practice.\n",
      "This motivates the chunked attention approach in which we segment a sequence into\n",
      "chunks and run the attention model on each of them (Parmar et al., 2018; Qiu et al.,\n",
      "2020). Given a sequence {1, ..., n}, we define {chunk 1, ...,chunk q}to be a segmentation\n",
      "of the sequence. A chunk can be expressed as a span\n",
      "chunk k= [cl\n",
      "k, cr\n",
      "k] (124)\n",
      "In the attention step, we treat each chunk as a sequence and perform self-attention on\n",
      "it as usual. In other words, the representation at position iis computed by using only\n",
      "the context in the chunk that ibelongs to. In this sense, this model can be thought\n",
      "of as some sort of local attention model. Figure 12 (c) shows an illustration of this\n",
      "model. There remains the issue of how to segment the sequence. There are several\n",
      "ways to do this. For example, as discussed in Section 3.4, we can do segmentation\n",
      "from a linguistic perspective, and segment the sequence into linguistically motivated\n",
      "units. In practical systems, it is sometimes more convenient to segment the sequence\n",
      "into chunks that are of equal length. Thus, the sparsity of the model is controlled by\n",
      "the size of these chunks, for example, the use of smaller chunks would lead to a more\n",
      "sparse attention model.\n",
      "•Strided Attention . Since the chunked attention approach enforces a hard segmen-\n",
      "tation on the input sequence, it may lose the ability to learn representations from\n",
      "47\n",
      "Page 48:\n",
      "Xiao and Zhu\n",
      "inputs in different chunks. An alternative way to achieve chunk-wise attention is to\n",
      "allow overlap between chunks (Ainslie et al., 2020; Beltagy et al., 2020; Child et al.,\n",
      "2019). This approach is analogous to the family of approaches that are commonly\n",
      "used to apply a local model to 1D or 2D data to generate outputs of the same shape.\n",
      "Like CNNs, we use a context window to represent the field of input of the attention\n",
      "model. The context window slides along the sequence, each time moving forward a\n",
      "step of size stride . As a special case, if stride equals the size of the context window,\n",
      "this model is the same as the chunked attention model mentioned above. If stride\n",
      "chooses a value smaller than the size of the context window, the attention model will\n",
      "become denser. Figure 12 (d) shows the case of strdie = 1 where the chunk overlap-\n",
      "ping is maximized. A way to achieve relatively sparser attention is to use a dilated\n",
      "context window . Figure 12 (e) shows an example of the dilated strided attention\n",
      "model, where the context window is discontinuous, with gaps of size 1.\n",
      "•Learning Attention Fields . Because the attention field πican be any sub-set of\n",
      "{1, ..., n}, we can develop more general sparse attention models by considering atten-\n",
      "tion maps beyond chunk-based patterns. The only question is how to determine which\n",
      "positions the model attends to for a given position. One simple approach is to use a\n",
      "computationally cheaper model to estimate the “importance” of each position. Then,\n",
      "attention weights are computed only for some of the positions which are thought to\n",
      "be most important (Zhou et al., 2021). A second approach is grouping: positions are\n",
      "grouped, and then the attention weights are computed only for positions in the same\n",
      "group. It is often relatively easy to achieve this by running clustering algorithms on\n",
      "keys and queries. For example, we can cluster keys and queries via k-means clustering.\n",
      "The centroids of the clusters can be treated as additional parameters of the attention\n",
      "model, and so can be learned during optimization (Roy et al., 2021). One benefit of\n",
      "learning attention fields is that the model can spread its attention broader over the\n",
      "sequence. This is a useful property for many NLP problems because word dependen-\n",
      "cies are sometimes long-range, not restricted to a local context window. See Figure 12\n",
      "(f) for an example of the attention map learned through this model. Alternative ap-\n",
      "proaches to learning to attend are to use sorting or hashing functions to group similar\n",
      "key and query vectors (Kitaev et al., 2020; Tay et al., 2020a). These functions can be\n",
      "either heuristically designed functions or neural networks with learnable parameters.\n",
      "By using these functions, we can reorder the sequence so that the inputs in the same\n",
      "group are adjacent in the reordered sequence. In this way, the resulting attention map\n",
      "follows a chunk-wise pattern, and the model is computationally efficient through the\n",
      "use of the chunked attention approach.\n",
      "•Hybrid Methods . Above, we have discussed a range of different sparse attention\n",
      "models. It is natural to explore methods that combine multiple models together to\n",
      "make use of their benefits in some way. A simple way to do this is to combine the\n",
      "attention fields of different models. For example, in Zaheer et al. (2020)’s system,\n",
      "the attention map is generated by considering three different sparse models, including\n",
      "48\n",
      "Page 49:\n",
      "Introduction to Transformers\n",
      "local attention (chunked attention), global attention, and random attention11. The\n",
      "resulting model is still a sparse model, but is somewhat more robust as it involves\n",
      "multiple patterns from different perspectives of attention modeling. Another way of\n",
      "combining multiple attention models is to use different models for different heads in\n",
      "multi-head attention (Beltagy et al., 2020; Child et al., 2019). For example, one can\n",
      "use one head as a local attention model, and use another head as a global attention\n",
      "model (see Figure 12 (g-h)).\n",
      "One disadvantage of sparse models compared to dense models is that they are not\n",
      "computationally efficient on GPUs or CPUs. While sparse models can ideally reduce both\n",
      "the memory and computation requirements, the actual rate at which work can be done by\n",
      "sparse models is much slower than by dense models. In practice, it is difficult for sparse\n",
      "models to approach the peak FLOPS of a GPU or CPU12. Therefore, they are often used for\n",
      "the purpose of high memory efficiency, not really for the purpose of efficient computation.\n",
      "On the other hand, sparse models are still of great use to NLP practitioners in the context\n",
      "of memory-efficient Transformer, especially when Transformer systems are used to deal with\n",
      "extremely long sequences.\n",
      "5.2 Recurrent and Memory Models\n",
      "For sequence generation problems, Transformer can also be thought of as a memory system.\n",
      "Consider again the general setting, in which we are given the states of previous i−1\n",
      "positions, and we wish to predict the next state. In self-attention, this is done by using\n",
      "the query at position i(i.e.,qi) to access the key-value pairs of the previous positions (i.e.,\n",
      "{(k1,v1), ...,(ki−1,vi−1)}). Then, we move to position i+1, and add ( ki,vi) to the collection\n",
      "of key-value pairs. This procedure can be interpreted in terms of memory mechanisms. The\n",
      "Transformer model maintains a memory that retains the information of the past. When\n",
      "moving along the sequence, we repeat the same operation, each time generating some output\n",
      "by reading the memory, and then updating the memory so that new information could be\n",
      "stored in some way. This is illustrated in Figure 13.\n",
      "5.2.1 Cache-based Memory\n",
      "The memory here can be viewed as a datastore of vectors. From a machine learning per-\n",
      "spective, this is a non-parametric model, and the cost of accessing the model grows as a\n",
      "longer sub-sequence is observed. Clearly, such a variable-length memory will generally be\n",
      "infeasible if the model deals with a very, very long sequence. For the modeling problem of\n",
      "arbitrary length sequences, it is common to use a fix-length memory instead. As in many\n",
      "NLP problems, one of the simplest ways to do this is to consider a cache saving recent\n",
      "information, that is, we restrict the modeling to a context window. Let ncbe the size of the\n",
      "context window. The model keeps track of the nc−1 latest states to the current position,\n",
      "so that its closest successors can be considered at each step. This means that, for each\n",
      "position, a self-attention sub-layer attends to nc−1 positions ahead, like this\n",
      "11. Here the global attention model attends each word only to a special word which accounts for the entire\n",
      "sequence and is often placed at the beginning of the sequence. The random attention model attends\n",
      "each word to a random set of the words of the sequence.\n",
      "12. FLOPS = floating point operations per second.\n",
      "49\n",
      "Page 50:\n",
      "Xiao and Zhu\n",
      "(a) Standard Attention (b) Span-based Attention\n",
      "(c) Chunked Attention (d) Strided Attention\n",
      "(e) Dilated Strided Attention (f) Learning Attention Fields\n",
      "(g) Global Attention (h) Hybrid Methods\n",
      "Figure 12: Illustration of the attention maps of different models (self-attention on the de-\n",
      "coder side). Dark cells mean A′(i, j)̸= 0 (i.e., iattends to j), and light cells\n",
      "mean A′(i, j) = 0 (i.e., idoes not attend to j). In all these attention maps, we\n",
      "assume that every position attends to itself by default (see diagonals).\n",
      "50\n",
      "Page 51:\n",
      "Introduction to Transformers\n",
      "1 2 3 ... i−2 i−1 i i+ 1Memory ( {1, ..., i−1}) StateRead (self-attention)\n",
      "Update\n",
      "Position i\n",
      "1 2 3 ... i−2 i−1 i i+ 1Memory ( {1, ..., i−1, i}) StateRead (self-attention)\n",
      "Update\n",
      "Position i+ 1\n",
      "Figure 13: Transformer as a memory system. At position i, the collection of the key-value\n",
      "pairs of positions {1, ..., i−1}is used as a memory of the past information.\n",
      "The Transformer model accesses this memory to generate some output, and\n",
      "then adds the key-value pair of position ito the memory. Moving to the next\n",
      "position, we repeat the same procedure of memory access and update.\n",
      "51\n",
      "Page 52:\n",
      "Xiao and Zhu\n",
      "InputOutput\n",
      "i i−1 i−2 i−3 i−4 i−5\n",
      "If we stack multiple self-attention sub-layers, a larger context window would be considered.\n",
      "For example, a model involving two self-attention sub-layers has a context window of size\n",
      "2nc−1, as follows\n",
      "Layer 1Layer 2Output\n",
      "i i−1 i−2 i−3 i−4 i−5\n",
      "Therefore, we can take a sufficiently large context by using a multi-layer Transformer\n",
      "model. Note that the context window model here is essentially the same as the strided\n",
      "attention model presented in the preceding section. Systems of this type are often easy to\n",
      "implement: we slide a window along the sequence, and, in each move, we make predictions\n",
      "at the last position of the window (for inference), or back-propagate errors (for training).\n",
      "An alternative way to train this context window model is by chunked attention. We\n",
      "divide the sequence into chunks (or sub-sequences) which are of the same length nc. Then,\n",
      "we treat these chunks as individual training samples, and run the training program on each of\n",
      "them as usual. This approach, however, completely ignores the relationship between inputs\n",
      "in different chunks. One way to address this issue is to introduce dependence between\n",
      "chunks. For example, the Transformer-XL model allows every chunk to access one or\n",
      "more preceding chunks (Dai et al., 2019). In the simplest case, consider an example in\n",
      "which chunk kcan see its successor chunk k−1. Each position in chunk kcan attend to all its\n",
      "preceding positions in both chunk kand chunk k−1.\n",
      "In Transformer-XL, this approach is implemented in a simplified form. First, each\n",
      "position is constrained to attend to nc−1 previous positions so that the size of the attention\n",
      "field of a position is the same in the training and inference stages. Such a method turns\n",
      "the problem back to strided attention, making the implementation of the attention model\n",
      "straightforward. On the other hand, the difference between the standard strided attention\n",
      "model and the Transformer-XL model is that in Transformer-XL, we perform training in\n",
      "a chunk-wise manner. Once we finish the training on a chunk, we directly move to the\n",
      "next chunk, rather than sliding the context window a small step forward. Second, while\n",
      "this approach allows for connections between chunks, the parameters of the sub-network on\n",
      "chunk k−1are fixed, and we only update the parameters of the sub-network on chunk kin\n",
      "thek-th step. See Figure 14 for an illustration.\n",
      "The above model is similar in spirit to recurrent models because all of them require\n",
      "the computation in one step to depend on the states of the preceding steps. However, it is\n",
      "52\n",
      "Page 53:\n",
      "Introduction to Transformers\n",
      "Layer 1Layer 2Output\n",
      "i+ 3 i+ 2 i+ 1 i i−1 i−2 i−3 i−4 i−5\n",
      "(a) Step kof chunk-wise trainingchunk k+1 chunk k chunk k−1\n",
      "Layer 1Layer 2Output\n",
      "i+ 3 i+ 2 i+ 1 i i−1 i−2 i−3 i−4 i−5\n",
      "(b) Step k+ 1 of chunk-wise trainingchunk k+1 chunk k chunk k−1\n",
      "Figure 14: Illustration of chunk-wise training (Dai et al., 2019). The input sequence is\n",
      "divided into chunks of the same length nc. Training is performed on these\n",
      "chunks, each time dealing with a chunk. In chunk k, the attention field for every\n",
      "position in this chunk is a left context window of size nc. Hence this model allows\n",
      "for attention across chunks, for example, position i−2 in chunk kcan attend to\n",
      "positions i−3 and i−4 in chunk k−1(see sub-figure (a)). For training, errors are\n",
      "back-propagated only in the sub-network for chunk k, leaving other parts of the\n",
      "model unchanged. Here we use dashed lines to denote information flow that we\n",
      "consider in the forward pass but not in the backward pass. Once we finish the\n",
      "training on chunk k, we move to the next chunk, and repeat the same training\n",
      "procedure.\n",
      "53\n",
      "Page 54:\n",
      "Xiao and Zhu\n",
      "not in the standard form of a recurrent model, in which the output of a recurrent unit in\n",
      "one step is the input in the next step. Instead, the “recurrence” is expressed by involving\n",
      "connections between two different layers, that is, the output of one layer in chunk k−1is\n",
      "used as the input of a higher-level layer in chunk k.\n",
      "5.2.2 Encoding Long-term Memory\n",
      "Another idea for representing the states of a sequence is to frame the task as an encoding\n",
      "problem. Instead of storing all the key-value vectors during left-to-right generation, we\n",
      "construct the memory of the entire “history” as a fixed number of encoded key-value vectors.\n",
      "These encoded key-value vectors can be either a small sub-set of {(k1,v1), ...,(ki−1,vi−1)}\n",
      "or a small set of newly-generated vectors that encodes {(k1,v1), ...,(ki−1,vi−1)}.\n",
      "One way to do the encoding is to apply a pooling operation to {(k1,v1), ...,(ki−1,vi−1)}\n",
      "(Rae et al., 2019). For example, by using average pooling, the memory contains only one\n",
      "key-value pair ( ¯k,¯v)\n",
      "¯k=1\n",
      "i−1i−1X\n",
      "j=1kj (125)\n",
      "¯v=1\n",
      "i−1i−1X\n",
      "j=1vj (126)\n",
      "This leads to a very efficient model, and we only need to update the vectors ( ¯k,¯v) at a\n",
      "time (Zhang et al., 2018). Let ( ¯k[i],¯v[i]) be the state of the memory at position i. A more\n",
      "general definition of ( ¯k[i],¯v[i]) is given in a recursive form\n",
      "¯k[i] = KMem( ¯k[i−1],ki−1) (127)\n",
      "¯v[i] = VMem( ¯v[i−1],vi−1) (128)\n",
      "where KMem( ·) and VMem( ·) are functions that update the memory by taking both the\n",
      "states of the memory at the previous position (i.e., ¯k[i−1] and ¯v[i−1]) and the new states\n",
      "(i.e.,ki−1andvi−1). There are many forms of the functions like KMem( ·) and VMem( ·) in\n",
      "common use. For example, if KMem( ·) and VMem( ·) are weighted sum functions, we can\n",
      "derive the same forms as Eqs. (125) and (126). If KMem( ·) and VMem( ·) are recurrent\n",
      "cells in RNNs or LSTM, we obtain a recurrent model of memory.\n",
      "Extension of the above model to memories having more than one key-value pair is\n",
      "straightforward. One approach is to use the memory to represent sub-sequences. Let\n",
      "{(¯k1,¯v1), ...,(¯kκ,¯vκ)}be a memory of size κ. Each ( ¯kj,¯vj) is a snapshot of a chunk of length\n",
      "nc. Thus, this memory can encode a sequence with maximum length κ·nc. Then, we can\n",
      "compute ( ¯kj,¯vj) on the corresponding chunk using Eqs. (127) and (128). A second approach\n",
      "is to organize {(¯k1,¯v1), ...,(¯kκ,¯vκ)}into a priority queue. We design some function to assign\n",
      "a score to any given key-value pair. The key-value pair can be inserted into the priority\n",
      "queue through the push operation. Ideally, we wish to develop a scoring function to estimate\n",
      "the value of a key-value pair, for example, we use another neural network to evaluate the\n",
      "key-value pair. In this way, the memory is a collection of the most valuable key-value pairs\n",
      "over the input sequence.\n",
      "54\n",
      "Page 55:\n",
      "Introduction to Transformers\n",
      "Although representing the memory as a set of vectors is an obvious choice for the model\n",
      "design in Transformer, the memory is discrete and its capacity is determined by the number\n",
      "of the vectors. An alternative form of memory is continuous memory . This type of\n",
      "model typically builds on the idea of function approximation, in which {k1, ...,ki−1}or\n",
      "{v1, ...,vi−1}is viewed as a series of data points, and a continuous function is developed\n",
      "to fit these data points. Then, we no longer need to store {k1, ...,ki−1}and{v1, ...,vi−1}.\n",
      "Instead, the memory is represented by the functions fitting these vectors. A simple method\n",
      "is to combine simple functions to fit complex curves of data points. For example, we can\n",
      "develop a set of basis functions and use a linear combination of them to approximate the\n",
      "key or value vectors (Martins et al., 2022). The resulting model is parameterized by these\n",
      "basis functions and the corresponding weights in the combination.\n",
      "It is also straightforward to use a short-term memory and a long-term memory simul-\n",
      "taneously so that we can combine the merits of both. For example, we use a cache-based\n",
      "memory to capture local context, and use an efficient long-term memory that encodes the\n",
      "entire history to model long-range dependency. This idea is also similar to that used in\n",
      "combining different sparse attention models as discussed in the previous sub-section.\n",
      "5.2.3 Retrieval-based Methods\n",
      "So far in this sub-section, we have discussed approaches based on fixed-length models. It\n",
      "is also possible to develop efficient memory models by improving the efficiency of accessing\n",
      "the memories, instead of just reducing the memory capacities. One way to achieve this is\n",
      "to store the past key-value pairs in a database (call it a vector database ), and to find\n",
      "the most similar ones when querying the database. To be more precise, given a query q,\n",
      "we use the database to find a set of top- prelevant key-value pairs (denoted by Ω p) by\n",
      "performing similarity search based on the dot-product similarity measure between query\n",
      "and key vectors. Then, we attend qto Ω pas in standard self-attention models. The\n",
      "idea behind this method is to consider only a small number of elements that contribute\n",
      "most to the attention result. Therefore, the model is essentially a sparse attention model\n",
      "which is computationally efficient. Another advantage of this method is that it allows\n",
      "for fast similarity search over a very large set of vectors because of the highly optimized\n",
      "implementation of vector databases. Building a memory as a retrieval system can fall under\n",
      "the general framework called the retrieval-augmented approach . It provides a simple\n",
      "way to incorporate external memories into neural models like Transformer (Guu et al., 2020;\n",
      "Lewis et al., 2020; Wu et al., 2021).\n",
      "5.3 Low-dimensional Models\n",
      "In many practical applications, Transformer models are “high-dimensional” models. This is\n",
      "not only because the input and/or output data is in high-dimensional spaces, but also be-\n",
      "cause some of the intermediate representations of the data in the model are high-dimensional.\n",
      "As discussed in Section 5.1, this high dimensionality arises in part from the steps of com-\n",
      "puting the attention matrix as in Eq. (119) (for ease of presentation, we repeat the equation\n",
      "here)\n",
      "Attself(S) = AV (129)\n",
      "55\n",
      "Page 56:\n",
      "Xiao and Zhu\n",
      "and the weighted sum of value vectors as in Eq. (120)\n",
      "A= Softmax(QKT\n",
      "√\n",
      "d+M) (130)\n",
      "which involves large matrix multiplications QKTandAVwhen the length nand the hidden\n",
      "dimensionality dhave large values.\n",
      "TheAVandQKToperations have a time complexity of O(n2·d) and a space complexity\n",
      "ofO(n2+n·d). Several previously described approaches have reduced this complexity by\n",
      "using sparse models. In this sub-section, we focus on methods that approximate these\n",
      "operations via dense computation. One simple idea is to transform Q,K, and Vinto\n",
      "smaller matrices, and thus to reduce the computational burden of matrix multiplication.\n",
      "Since Q,K, and Vare all in Rn×d, we can achieve this by reducing either the ndimension\n",
      "or the ddimension, or both.\n",
      "5.3.1 Reducing n\n",
      "Note that the output Att self(S) is required to be an n×dmatrix, and so we cannot reduce the\n",
      "number of queries. We instead consider reducing the number of keys and values. Suppose\n",
      "n′is a number less than n, and KandVcan be transformed into n′×dmatrices K′and\n",
      "V′in some way. We can obtain a “smaller” model simply by replacing KandVwithK′\n",
      "andV′, giving\n",
      "Attself(S) = AV′(131)\n",
      "A= Softmax(Q[K′]T\n",
      "√\n",
      "d+M) (132)\n",
      "This model is in the standard form of self-attention, but has lower time and space complex-\n",
      "ities, that is, O(n′·n·d)< O(n2·d) and O(n′·n+n′·d)< O(n2+n·d). Ifn′<< n , the\n",
      "resulting model will be linear with n.\n",
      "The key problem here is how to obtain K′andV′in a way that retains much of the\n",
      "information in KandV. There are several ways to do so. One simple method is to select\n",
      "the keys and values that are thought to be important. The importance of a key (or value)\n",
      "can be computed in terms of some computationally cheap measure. For example, we can\n",
      "sample a small number of query-key dot-products and estimate the importance of a key by\n",
      "collecting these dot-product results.\n",
      "The above method is straightforward but still requires sparse operations, such as sam-\n",
      "pling and collection. As an alternative, we can use dense computation to transform Kand\n",
      "VtoK′andV′. A typical choice is to use CNNs (Liu et al., 2018). Let Conv( ·) be a\n",
      "function describing a set of filters that slide along the ndimension. K′is then given by\n",
      "K′= Conv( K,Wc, size r, stride ) (133)\n",
      "where Wcis the parameter matrix of the filters, size ris the size of the receptive field, and\n",
      "stride is the number of units the filters are translated at a time. In general, we can achieve\n",
      "a high compression rate by choosing large values for size randstride . Likewise, we can\n",
      "compute V′using another convolutional function. It is worth noting that, if the parameter\n",
      "56\n",
      "Page 57:\n",
      "Introduction to Transformers\n",
      "n′is fixed for all samples, compression of KandValong the length dimension is essentially\n",
      "the same as the fixed-length memory model as described in the preceding sub-section. The\n",
      "methods presented here are more general and could be applied to variable-length memories.\n",
      "We might also be tempted to model the attention function by considering the attention\n",
      "matrix Aas a high-dimensional representation of data and then applying conventional\n",
      "dimensionality reduction methods. For many problems, it is found that A(or more precisely\n",
      "QKT) is a low-rank matrix. In this case, we can compress Awhile retaining as much\n",
      "information as possible. There are many ways to do so. For example, we might use a\n",
      "product of smaller matrices as an approximation to Avia the SVD technique. However,\n",
      "this introduces computational overhead in using SVD compared with the standard attention\n",
      "model. A simpler idea to directly transform KandVinto smaller-sized matrices via linear\n",
      "mappings, given by\n",
      "K′=UkK (134)\n",
      "V′=UvV (135)\n",
      "where Uk∈Rn′×nandUv∈Rn′×nare parameter matrices. Clearly, this leads to a model\n",
      "which is equivalent to that described in Eqs. (131) and (132). While such a method is\n",
      "intuitive and simple, it is proven to obtain a sufficiently small approximation error ϵifn′is\n",
      "a linear function of d/ϵ2(Wang et al., 2020b).\n",
      "5.3.2 Reducing d\n",
      "Another approach to working in a low-dimensional space is to reduce the ddimension. One\n",
      "of the simplest methods is to project all queries and keys onto a d′-dimensional space ( d′<\n",
      "d), and to compute the dot-product of any key-value pair in the new space. For modeling,\n",
      "we only need to replace Q∈Rn×dandK∈Rn×dby new representations Q′∈Rn×d′and\n",
      "K′∈Rn×d′. We can easily modify Eq. (130) to use Q′andK′in computing the attention\n",
      "matrix\n",
      "A= Softmax(Q′[K′]T\n",
      "√\n",
      "d+M) (136)\n",
      "Q′andK′are given by\n",
      "Q′=QUq(137)\n",
      "K′=KUk(138)\n",
      "where Uq∈Rd×d′andUk∈Rd×d′are parameter matrices of linear transformations.\n",
      "It is also possible to exploit kernel methods to obtain an efficient dot-product attention\n",
      "model. The basic idea is to map all data points (represented as vectors) from one space to\n",
      "another space, so that the problem, which might be difficult to solve in the original space,\n",
      "is easier to solve in the new space. The “trick” of kernel methods is that we actually do\n",
      "not need to know the mapping function, but only need to know how to compute the inner\n",
      "product of vectors in the new space in one operation13. This operation of the inner product\n",
      "is usually called the kernel and denoted by K(·,·).\n",
      "13. In mathematical analysis, the inner product is a generalized notion of the dot-product. It is typically\n",
      "denoted by ⟨·,·⟩. A formal definition of the inner product requires that ⟨·,·⟩satisfies several properties\n",
      "57\n",
      "Page 58:\n",
      "Xiao and Zhu\n",
      "It is interesting to approximate Ain a fashion analogous to K(·,·) in kernel methods.\n",
      "To illustrate, note in Eq. (130) Ais a fraction denoting the normalized attention weights.\n",
      "The numerator can be written in the form\n",
      "eA= Mask(exp(QKT\n",
      "√\n",
      "d)) (140)\n",
      "(141)\n",
      "Here Mask( ·) is a function which has the same effect as using the additive masking variable\n",
      "M. Then, Acan be expressed as\n",
      "A=D−1eA (142)\n",
      "where Dis an n×ndiagonal matrix. Each entry of the main diagonal is the sum of\n",
      "the entries of the corresponding row in eA, denoting the normalization factor of Softmax.\n",
      "Substituting this equation into Eq. (130), we have\n",
      "Attself(S) = D−1eAV (143)\n",
      "In this model, eA(i, j) can be viewed as a similarity function over all query-key pairs\n",
      "in ad-dimensional space. Here we assume that this function, which is in the form of the\n",
      "dot-product of vectors, can be approximated by a kernel function\n",
      "eA(i, j) = K(qi,kj)\n",
      "=⟨ϕ(qi), ϕ(kj)⟩\n",
      "ϕ(·) is a mapping from RdtoRd′. We can represent the queries and keys in the following\n",
      "form\n",
      "Q′=ϕ(Q)\n",
      "=\n",
      "ϕ(q1)\n",
      "...\n",
      "ϕ(qn)\n",
      " (144)\n",
      "K′=ϕ(K)\n",
      "=\n",
      "ϕ(k1)\n",
      "...\n",
      "ϕ(kn)\n",
      " (145)\n",
      "in a vector space. Although the inner product has different forms in different contexts, in the Euclidean\n",
      "space Rd, it is the same thing as the dot-product, that is, given two vectors a∈Rdandb∈Rd, we have\n",
      "⟨a,b⟩=a·b\n",
      "=dX\n",
      "i=1ai·bi (139)\n",
      "58\n",
      "Page 59:\n",
      "Introduction to Transformers\n",
      "Then, we develop a kernelized attention model by approximating the attention weight\n",
      "αi,jin the form\n",
      "αi,j≈ϕ(qi)ϕ(kj)T\n",
      "Pn\n",
      "j′=1ϕ(qi)ϕ(kj′)T(146)\n",
      "The key idea behind this kernelized attention model is that we can remove the Softmax\n",
      "function if the queries and keys are mapped to a new space. Using this approximation, the\n",
      "i-th output vector of the attention model (i.e., the i-th row vector of Att self(S)) is given by\n",
      "ci=nX\n",
      "j=1αi,j·vj\n",
      "≈nX\n",
      "j=1\u0010ϕ(qi)ϕ(kj)T\n",
      "Pn\n",
      "j′=1ϕ(qi)ϕ(kj′)T·vj\u0011\n",
      "=Pn\n",
      "j=1ϕ(qi)ϕ(kj)TvjPn\n",
      "j′=1ϕ(qi)ϕ(kj′)T\n",
      "=ϕ(qi)(Pn\n",
      "j=1ϕ(kj)Tvj)\n",
      "ϕ(qi)(Pn\n",
      "j′=1ϕ(kj′)T)(147)\n",
      "Although the equation appears a bit complicated, the idea is simple: instead of attending\n",
      "the query to all keys to obtain the attention weight αi,j, we can compute the sum of the\n",
      "multiplicationsPn\n",
      "j=1ϕ(kj)Tvj∈Rd′×dand then multiply it with the kernelized query\n",
      "ϕ(qi). Returning to the notation used in Eq. (143), we define the i-th entry of Dto be\n",
      "ϕ(qi)Pn\n",
      "j′=1ϕ(kj′)T. Then, the attention model can be re-expressed in the form\n",
      "Attself(S) = D−1ϕ(Q)ϕ(K)TV\n",
      "=D−1Q′K′TV\n",
      "=D−1\u0000\n",
      "Q′(K′TV)\u0001\n",
      "(148)\n",
      "Here we change the order of computation from left-to-right to right-to-left using parenthe-\n",
      "ses. Given that Q′∈Rn×d′andK′∈Rn×d′, this model has time and space complexities of\n",
      "O(n·d·d′) and O(n·d+n·d′+d·d′), respectively. Therefore, the model is linear with respect\n",
      "to the sequence length n, and is sometimes called the linear attention model . One com-\n",
      "putational advantage of this model is that we need only compute the multiplication K′TV\n",
      "(i.e.,Pn\n",
      "j=1ϕ(kj)Tvj) and the corresponding normalization factor (i.e.,Pn\n",
      "j′=1ϕ(kj′)T) once.\n",
      "The results can then be used for any query (Katharopoulos et al., 2020). The memory needs\n",
      "to maintainPn\n",
      "j=1ϕ(kj)TvjandPn\n",
      "j′=1ϕ(kj′)Tand update them when new key and value\n",
      "vectors come.\n",
      "Still, there are several problems regarding this kernelized model, for example, how to\n",
      "develop the feature map ϕ(·) to obtain a good approximation to the standard attention\n",
      "model. Interested readers may refer to Choromanski et al. (2020)’s work for more details.\n",
      "A second idea for reducing dis to take sub-space models, in which a problem in a d-\n",
      "dimensional space is transformed into sub-problems in lower-dimensional spaces, and the\n",
      "solution to the original problem is approximated by some combination of the solutions to\n",
      "59\n",
      "Page 60:\n",
      "Xiao and Zhu\n",
      "these sub-problems. In a general sub-space model, a d-dimensional key vector kcan be\n",
      "mapped into a set of d′-dimensional vectors {K′1, ...,K′η}. To simplify modeling, we can do\n",
      "this by vector segmentation, that is, we segment kintoηsub-vectors, each having d′=d\n",
      "η\n",
      "dimensions. We can transform all query and value vectors in the same way. Then, the\n",
      "attention model is applied in each of these sub-spaces.\n",
      "This method, however, does not reduce the total amount of computation. As presented\n",
      "in Lample et al. (2019)’s work, we can instead approximate the dot-product attention over\n",
      "a set of key-value pairs by considering top- pcandidates in each sub-space. More precisely,\n",
      "we find p-best key-value pairs in each sub-space, which is computationally cheaper. The\n",
      "Cartesian product of these p-best key sets consists of pηproduct keys. Likewise, we obtain\n",
      "pηproduct values. The remaining work is simple: the d-dimensional queries attend to these\n",
      "d-dimensional product keys and values. An interesting difference between this sub-space\n",
      "model and the d-dimensional space model is that the generated product keys and values may\n",
      "be different from any of the original key-values {(k1,v1), ...,(ki−1,vi−1)}. This provides a\n",
      "way for learning new representations of the past information.\n",
      "So far we have discussed approaches to dimensionality reduction along either the nord\n",
      "dimension. It is straightforward to combine them to develop a “lower-dimensional” model.\n",
      "As an example, suppose that we have the n→n′reduction for keys and values, and the\n",
      "d→d′reduction for queries and keys. The model takes the form\n",
      "Attself(S) = AV′\n",
      "A= Softmax(Q′K′T\n",
      "√\n",
      "d′+M) (149)\n",
      "where Q′∈Rn×d′,K′∈Rn′×d′, and V′∈Rn′×d′are low-dimensional representations for\n",
      "queries, keys and values. As usual, we can easily obtain these representations through\n",
      "the linear mappings of Q,KandV. The time and space complexities of this model are\n",
      "O(n′·n·d′) and O(n′·n+n′·d′).\n",
      "5.4 Parameter and Activation Sharing\n",
      "Redundancy is common to most large-scale neural networks. As a result, many of these\n",
      "models are over-parameterized, making the training and inference less efficient. One com-\n",
      "mon approach to redundancy reduction is to simplify the modeling by removing useless\n",
      "components of the models, for example, we can either prune a complex model or share\n",
      "sub-models among different components of it to obtain a reasonably small model. In this\n",
      "sub-section, we discuss methods of parameter and intermediate state sharing in Transformer\n",
      "models. We leave the discussion of model transfer and pruning to Section 5.7.\n",
      "Shared-parameter architectures are widely used in neural network-based systems. Well-\n",
      "known examples include CNNs and RNNs, where the same set of parameters (or layers) is\n",
      "applied across different regions of the input. This produces a “big” neural network, parts\n",
      "of which have the same architecture and the same shared parameters. For Transformers\n",
      "as well as other sequence models, the sharing mechanism can be applied to different levels\n",
      "of modeling. A simple example, which might be not related to architecture design, is\n",
      "shared embedding. In machine translation, a typical strategy for dealing with words in two\n",
      "languages is to develop two separate embedding models. Alternatively, one can use a single\n",
      "60\n",
      "Page 61:\n",
      "Introduction to Transformers\n",
      "embedding model for both languages. The parameters of the model are then learned during\n",
      "the training of both the source-side and target-side networks. Such a strategy is also often\n",
      "adopted in multi-lingual sequence models, such as language models that are able to deal\n",
      "with texts in many different languages.\n",
      "For multi-layer neural networks, a popular method is layer-wise sharing. Suppose there\n",
      "is a stack of layers, all of which have the same form\n",
      "Sl= Layer( Sl−1;θl) (150)\n",
      "We can tie the parameters for some or all of these layers. For example, given a set of layers\n",
      "{l1, l2, ..., l n}, we enforce the constraint θl1=θl2=...=θln, so that we can obtain a smaller\n",
      "model and the optimization of the model can be easier. In practice, this shared-layer model\n",
      "is highly advantageous if many layers are involved, because we can repeat the same process\n",
      "many times to construct a very deep neural network (Dehghani et al., 2018). For example,\n",
      "sharing a single FFN sub-layer across the Transformer encoder is found to be effective in\n",
      "reducing the redundancy in machine translation systems (Pires et al., 2023).\n",
      "For Transformers, sharing can also be performed in multi-head attention. An example\n",
      "of this is multi-query attention (Shazeer, 2019). Recall from Section 2.3 that the output\n",
      "of a head hin standard multi-head self-attention can be written as\n",
      "Chead\n",
      "h = Att qkv(Sq\n",
      "h,Sk\n",
      "h,Sv\n",
      "h)\n",
      "= Att qkv(SWq\n",
      "h,SWk\n",
      "h,SWv\n",
      "h) (151)\n",
      "Here Sq\n",
      "h=SWq\n",
      "h,Sk\n",
      "h=SWk\n",
      "h, and Sv\n",
      "h=SWv\n",
      "vare the query, key, and value, which are\n",
      "obtained by linearly transforming the input Swith distinct parameter matrices Wq\n",
      "h,Wk\n",
      "h,\n",
      "andWv\n",
      "h. In multi-query attention, we share the same key and value across all the heads,\n",
      "but use different queries for different heads. The form of this model is given by\n",
      "Chead\n",
      "h = Att qkv(SWq\n",
      "h,SWk\n",
      "0,SWv\n",
      "0) (152)\n",
      "Here the key SWk\n",
      "0and value SWv\n",
      "0are irrelevant to h. Hence we need only compute them\n",
      "once rather than computing them several times. As a result, we can make a significant saving\n",
      "in computational cost, especially if the number of heads is large. Multi-query attention has\n",
      "been successfully incorporated into recent large language models, such as Llama 2 (Touvron\n",
      "et al., 2023b) and Falcon14.\n",
      "By extending the idea of sharing to more general situations, any intermediate states can\n",
      "be shared across a neural network. For example, reusing neuron activations allows a sub-\n",
      "model to be applied multiple times. For Transformers, sharing can be considered inside the\n",
      "process of self-attention. It is found that the attention maps of different layers are similar\n",
      "in some NLP tasks (Xiao et al., 2019). Therefore, it is reasonable to compute the attention\n",
      "map only once and then use it in the following layers.\n",
      "If we make a further generalization of the sharing mechanism, we can view it as a\n",
      "process by which we use the result produced previously rather than computing it on the\n",
      "fly. It is thus possible to reuse the information across different runs of a neural network.\n",
      "A related example is reversible residual networks , in which activations of one layer can\n",
      "14. https://falconllm.tii.ae/index.html\n",
      "61\n",
      "Page 62:\n",
      "Xiao and Zhu\n",
      "be recovered from the activations of the following layer (Gomez et al., 2017). Hence we\n",
      "only keep the output of the latest layer in the forward pass. Then, in the backward pass\n",
      "of training, we reconstruct the output of each layer from its successor. One advantage of\n",
      "this reversible treatment is that the information produced in the forward pass is shared\n",
      "implicitly, and the model is memory-efficient (Kitaev et al., 2020).\n",
      "5.5 Alternatives to Self-Attention\n",
      "We have seen that the use of self-attention is a primary source of the large computation and\n",
      "memory requirements for Transformer systems. It is natural to wonder if there are efficient\n",
      "alternatives to self-attention models. Here we present briefly some of the Transformer\n",
      "variants in which self-attention sub-layers are not required and we instead replace them\n",
      "with other types of neural networks.\n",
      "5.5.1 CNN as A Replacement of Self-Attention\n",
      "CNNs are simple and widely used neural networks, and are considered as potential alterna-\n",
      "tives to self-attention models. To apply CNNs to Transformers, all we need is to construct a\n",
      "convolutional sub-layer to replace the self-attention sub-layer in a Transformer block. While\n",
      "a filter of CNNs has a restricted receptive field and thus takes inputs from a “local” context\n",
      "window, large contexts can be easily modeled by stacking multiple convolutional sub-layers.\n",
      "One key advantage of CNNs is that the number of elementary operations required to run\n",
      "CNNs is a linear function of the sequence length n, compared with the quadratic function\n",
      "for self-attention networks. In practical systems, there have been many highly-optimized\n",
      "implementations for CNNs, making it easier to apply them to sequence modeling. For fur-\n",
      "ther improvements to memory efficiency, we can use lightweight CNN variants, for example,\n",
      "depth-wise CNNs (Wu et al., 2018a).15\n",
      "5.5.2 Linear Attention\n",
      "As with many practical approaches to sequence modeling, there is also considerable interest\n",
      "in developing linear models in order to speed up the processing of long sequences. While\n",
      "there are many ways to define a linear model, one general form that is commonly used in\n",
      "15. In CNNs, a filter (or a set of filters) combines the input variables in the receptive field into an output\n",
      "variable (or a set of output variables) via linear mapping. Suppose that the input and output of a\n",
      "problem are represented as sequences of feature vectors. Given a filter having a d×kreceptive field, we\n",
      "slide it along the sequence. At each step, the filter takes d×kinput features and produces an output\n",
      "feature. This procedure is typically expressed by\n",
      "y= ReduceSum( x⊙W) (153)\n",
      "where x∈Rk×dis the vector representation of the input, y∈Ris the output feature, and W∈Rk×dis\n",
      "the weight matrix. The function ReduceSum( ·) computes the sum of all element-wise products between\n",
      "xandW. If we want the input and output to have the same number of features, we can design dfilters\n",
      "and the number of parameters will be d2·k.\n",
      "In depth-wise CNNs, we tie the weights across different feature dimensions. More precisely, all the\n",
      "column vectors of Ware the same. Thus, the number of the unique parameters of the model is reduced\n",
      "tod·k(each Wcorresponding to a filter having kunique parameters).\n",
      "62\n",
      "Page 63:\n",
      "Introduction to Transformers\n",
      "sequence models is\n",
      "zi=f(a·zi−1+b·si) (154)\n",
      "Here sirepresents some intermediate states of the model at step i, and zirepresents the\n",
      "summary of the history states up to step i. It is easy to see that this is a recurrent model:\n",
      "the output at step idepends only on the input at the current step and the output at the\n",
      "previous step. As with the popular design choices in neural network-based systems, the\n",
      "linear part is followed by a transformation f(·) which can be either an activation function\n",
      "or a feedforward neural network. Note that, Eq. (154) defines a standard linear model only\n",
      "iff(·) is a linear function. The use of f(·) gives greater flexibility in modeling the problem,\n",
      "although the term linear model may not be applied if f(·) chooses a non-linear form.\n",
      "The above formula describes a linearly structured model which can be seen as an instance\n",
      "of a general family of mathematical models. Typically, it can be represented as a chain\n",
      "structure, or an ordered set of nodes. The model repeats the same computation process\n",
      "from the first node to the last, each time taking the information from the current and\n",
      "previous steps and producing an output vector that is used in the following time steps. As\n",
      "a result, the space and time cost of the model scales linearly with the length of the chain.\n",
      "We can extend Eq. (154) to a standard RNN model by simply making a linear trans-\n",
      "formation of the current input and the previous state, that is, zi=f(zi−1·Wz+si·Ws).\n",
      "It is thus straightforward to apply RNN and its variants to Transformer to obtain a hybrid\n",
      "model. For example, we can use LSTM and GRUs in building some of the Transformer lay-\n",
      "ers to combine the merits of both recurrent models and self-attentive models (Chen et al.,\n",
      "2018b).\n",
      "In fact, we may be more interested in developing linear attention models, so that we\n",
      "can obtain an efficient system, while still retaining the benefit of globally attentive sequence\n",
      "modeling. Part of the difficulty in doing this is that the form of self-attention is not linear.\n",
      "Let us take a moment to see how this difficulty arises. Recall that the result of self-attention\n",
      "can be written in the following form\n",
      "Attself=A·V\n",
      "=ψ(Q·KT)·V (155)\n",
      "Here ψ(·) is a function that is composed by taking the scaling, exponentiating, masking\n",
      "and normalization operations (i.e., ψ(a) = Normalize(Mask(exp(a√\n",
      "d)))). Because ψ(·) is a\n",
      "complex non-linear function, there is no obvious equivalent that simplifies the computation,\n",
      "and we have to calculate the two matrix multiplications separately (one inside ψ(·) and one\n",
      "outside ψ(·)). As a consequence, we need to store all the key-value pairs explicitly, and visit\n",
      "each of them given a query. Not surprisingly, this leads to a model whose computational\n",
      "cost grows quadratically with the sequence length n.\n",
      "Although in self-attention keys and values are coupled, they are used in separate steps.\n",
      "An elegant form of this model might be that allows for a direct interaction between the\n",
      "keys and queries, so that we can encode the context information in a way that is irrelevant\n",
      "to the queries. A trick here is that we can remove the non-linearity from ψ(·) by using a\n",
      "feature space mapping ϕ(·) on the queries and keys, and reformulate ψ(Q·KT) (i.e., A) in\n",
      "a form of matrix products. For example, recall from Section 5.3 that we can transform Q\n",
      "63\n",
      "Page 64:\n",
      "Xiao and Zhu\n",
      "andKtoQ′=ϕ(Q)∈Rn×d′andK′=ϕ(K)∈Rn×d′through the mapping ϕ(·). Then, we\n",
      "define the form of the attention model to be\n",
      "Attself≡ψ′(Q′·K′T)·V\n",
      "=Q′·K′T\n",
      "D·V\n",
      "=Q′·\u0000\n",
      "K′T·V\u0001\n",
      "D(156)\n",
      "where ψ′(a) =a\n",
      "D. From this definition, we see that, in the case of transformed queries\n",
      "and keys, the query-key product needs not be normalized via Softmax, but needs only be\n",
      "normalized via a simple factor D. Hence the model has a very simple form involving only\n",
      "matrix multiplication and division, allowing us to change the order of the operations using\n",
      "the associativity of matrix multiplication.\n",
      "This leads to an interesting procedure: keys and values are first encoded via K′T·V,\n",
      "and then each query attends to this encoding result. Given that K′T·V=Pn\n",
      "j=1k′T\n",
      "j·vj,\n",
      "we can write K′T·Vin the form of Eq. (154), as follows\n",
      "µj=µj−1+k′T\n",
      "j·vj (157)\n",
      "Here µj∈Rd′×dis a variable that adds k′T\n",
      "j·vjat a time. Likewise, we can define another\n",
      "variable νj∈Rd′\n",
      "νj=νj−1+k′T\n",
      "j (158)\n",
      "Then, the output of self-attention for the j-th query can be written as (see also Eq.\n",
      "(147))\n",
      "Attself,j=q′\n",
      "j·µn\n",
      "q′\n",
      "j·νn(159)\n",
      "Clearly, this is a linear model, because µnandνnare linear with respect to n. In\n",
      "simple implementations of this model, only µjandνjare kept. Each time a new query is\n",
      "encountered, we update µjandνjusing Eqs. (157) and (158), and then compute Att self,j=\n",
      "q′\n",
      "j·µj\n",
      "q′\n",
      "j·νj16.\n",
      "One straightforward extension to the linear attention model is to allow Eqs. (157) and\n",
      "(158) to combine different terms with different weights. For example, we can redefine µj\n",
      "andνjas\n",
      "µj=a·µj−1+ (1−a)·k′T\n",
      "j·vj (160)\n",
      "νj=a·νj−1+ (1−a)·k′T\n",
      "j (161)\n",
      "and train the parameter aas usual. Also, we can treat aas a gate and use another neural\n",
      "network to compute a(Peng et al., 2021). Another model design is to add more terms to\n",
      "16. In autoregressive generation, we generate a sequence from left to right. In this case, we need not consider\n",
      "the keys and values for positions > j.\n",
      "64\n",
      "Page 65:\n",
      "Introduction to Transformers\n",
      "Eqs. (157) and (158) in order to give a more powerful treatment of the linear attention\n",
      "approach (Bello, 2020; Schlag et al., 2021).\n",
      "We have seen a general idea of designing linear models for the attention mechanism. The\n",
      "key design choice of such models is to remove the Softmax-based normalization, thereby\n",
      "taking linear forms of representations based on various intermediate states of the models.\n",
      "This motivates several recently developed alternatives to self-attention in which efficient\n",
      "inference systems are developed on the basis of recurrent models of sequence modeling\n",
      "(Peng et al., 2023; Sun et al., 2023). While these systems have different architectures, the\n",
      "underlying models have a similar form, as described in Eq. (154). Note that, by using the\n",
      "general formulation of recurrent models, we need not restrict the modeling to the standard\n",
      "QKV attention. Instead we may give new meanings and forms to the queries, keys, and\n",
      "values.\n",
      "The discussion here is also related to the memory models discussed in Section 5.2. From\n",
      "the memory viewpoint, the keys and values can be treated as encodings of the context.\n",
      "Therefore, in the linear attention model above we have a memory system in which two\n",
      "simple variables µjandνjare used to represent all the context information up to position\n",
      "j. This results in a fixed-length memory which is very useful in practice. There are also\n",
      "other linear approaches to encoding long sequences. For example, we can view the moving\n",
      "average model as an instance of Eq. (154), and average a series of state vectors of a\n",
      "Transformer system, either weighted or unweighted.\n",
      "5.5.3 State-Space Models\n",
      "In control systems, state-space models (SSMs ) are representations of a system whose\n",
      "input and output are related by some state variables (or states for short), and whose\n",
      "dynamics is described by first-order differential equations of these states. As a simple\n",
      "example, we consider a continuous time-invariant linear system which is given in the form\n",
      "of the state-space representation\n",
      "dz(t)\n",
      "dt=z(t)·A+s(t)·B (162)\n",
      "o(t) = z(t)·C+s(t)·D (163)\n",
      "Here s(t),o(t), and z(t) are the values of the input variable, output variable and state\n",
      "variable at time t17. In a general setting, s(t),o(t), and z(t) may have different numbers of\n",
      "dimensions. To simplify the discussion here, we assume that s(t),o(t)∈Rdandz(t)∈Rdz18.\n",
      "Eq. (162) is called the state equation , where A∈Rdz×dzis the state matrix and B∈Rd×dz\n",
      "is the input matrix. Eq. (163) is called the output equation , where C∈Rdz×dis the\n",
      "output matrix and D∈Rd×dis the feedforward matrix.\n",
      "These equations describe a continuous mapping from the variable s(t) to the variable\n",
      "o(t) over time. They are, therefore, often used to deal with continuous time series data.\n",
      "To apply this model to the sequence modeling problem discussed in this chapter, we need\n",
      "to modify the above equations to give a discrete form of the state-space representation.\n",
      "17. We use boldface letters to emphasize that the variables are vectors.\n",
      "18. In a general state-space model, all these variables are represented as vectors of complex numbers. Because\n",
      "the models defined on the field of complex numbers is applicable to case of real number-based state-\n",
      "spaces, we restrict our discussion to variables in the multi-dimensional real number field.\n",
      "65\n",
      "Page 66:\n",
      "Xiao and Zhu\n",
      "Suppose that {s0,s1, ...,sn}is a sequence of input data points sampled from s(t) with time\n",
      "step ∆ t. Similarly, we define {z0,z1, ...,zn}and{o0,o1, ...,on}as sequences of the state\n",
      "and output vectors. Given this notation, we now have a discretized version of the SSM,\n",
      "written as\n",
      "zt=zt−1·A+st·B (164)\n",
      "ot=zt·C+st·D (165)\n",
      "This formulation of the SSM defines an RNN with a residual connection. To be more\n",
      "precise, Eq. (164) describes a recurrent unit that reads the input at step tand the state at\n",
      "stept−1, without using any activation function. Eq. (165) describes an output layer that\n",
      "sums both the linear transformations of the state ztand the identity mapping st.\n",
      "The parameters A,B,C, and Dcan be induced from A,B,CandDin several different\n",
      "ways, depending on how Eq. (162) is approximated by Eq. (164)19. One approach to time\n",
      "discretization, called bilinear transform orTustin’s method , gives a model in which\n",
      "the parameters take the form\n",
      "A= (I−∆t\n",
      "2·A)·(I−∆t\n",
      "2·A)−1(172)\n",
      "B= ∆ t·B·(I−∆t\n",
      "2·A)−1(173)\n",
      "C=C (174)\n",
      "D=D (175)\n",
      "19. The discretization process can be interpreted as a numerical method of solving the differential equation.\n",
      "Note that Eq. (162) is an ODE\n",
      "dz(t)\n",
      "dt=g(z(t), t) (166)\n",
      "where\n",
      "g(z(t), t) = z(t)·A+s(t)·B (167)\n",
      "There are many numerical approximations to the solutions to the ODE. For example, the Euler\n",
      "method of solving the ODE can be expressed in the form (see in Section 4.3)\n",
      "zt=zt−1+ ∆t·g(zt−1, t) (168)\n",
      "Substituting Eq. (167) into Eq. (168) yields\n",
      "zt=zt−1+ ∆t(zt−1·A+st·B)\n",
      "=zt−1·(I+ ∆t·A) +st·(∆t·B) (169)\n",
      "This gives one of the simplest forms of the discretized state equations (Gu et al., 2022b), that is,\n",
      "A=I+ ∆t·A (170)\n",
      "B= ∆ t·B (171)\n",
      "66\n",
      "Page 67:\n",
      "Introduction to Transformers\n",
      "An alternative approach is to use the Zero-Order-Hold (ZOH) discretization which has the\n",
      "form\n",
      "A= exp(∆ t·A) (176)\n",
      "B= ∆ t·B·(exp(∆ t·A)−I)·(∆t·A)−1(177)\n",
      "C=C (178)\n",
      "D=D (179)\n",
      "A detailed discussion of these approaches lies beyond the scope of this paper, and we refer\n",
      "the interested reader to standard textbooks on control theory for further details ( ˚Astr¨ om\n",
      "and Wittenmark, 2013).\n",
      "The recurrent form of Eq. (164) makes it easy to compute the states and outputs over\n",
      "a sequence of discrete time steps. We can unroll ztandotin a feedforward fashion\n",
      "z0=s0·B o0=s0·B·C+s0·D\n",
      "z1=s0·B·A+s1·B o1=s0·B·A·C+s1·B·C+s1·D\n",
      "z2=s0·B·A2+s1·B·A+s2·Bo2=s0·B·A2·C+s1·B·A·C+\n",
      "s2·B·C+s2·D\n",
      "...... ......\n",
      "It is easy to write\n",
      "zt=tX\n",
      "i=0si·B·At−i(180)\n",
      "ot=tX\n",
      "i=0si·B·At−i·C+st·D (181)\n",
      "Clearly, the right-hand side of Eq. (181) can be interpreted as a merged output of a\n",
      "convolutional layer and a linear layer. Given that\n",
      "tX\n",
      "i=0si·B·At−i·C=\u0002\n",
      "s0s1...st\u0003\n",
      "·\n",
      "h\n",
      "B·At·CB·At−1·C...B·Ci\n",
      "(182)\n",
      "we define a filter having the parameters\n",
      "Wssm =h\n",
      "B·Anmax·CB·Anmax−1·C...B·Ci\n",
      "(183)\n",
      "where nmaxis the maximum length of the sequence20. Then, the output of the state-space\n",
      "model for a sequence S=\n",
      "s0\n",
      "...\n",
      "sn\n",
      "can be expressed as\n",
      "O= Conv( S,Wssm) + Linear( S,D) (184)\n",
      "20. Here Wssmcan be represented as an nmax×d×dtensor.\n",
      "67\n",
      "Page 68:\n",
      "Xiao and Zhu\n",
      "where Conv( ·) is the convolution operation, and Linear( ·) is the linear transformation op-\n",
      "eration. Such a treatment of the state-space model enables the system to be efficiently\n",
      "implemented using fast parallel convolution algorithms.\n",
      "Unfortunately, the above model performs poorly in many cases. As with many deep\n",
      "neural networks, careful initialization of the model parameters plays an important role in\n",
      "such models. For example, restricting the state matrix to particular types of matrices is\n",
      "found to be useful for learning and generalizing on long sequences (Gu et al., 2022a).\n",
      "Another problem with the basic state-space model is that it involves multiplication of\n",
      "multiple matrices. If the sequence is long (i.e., nis a large number), computing Anwill be\n",
      "computationally expensive and numerically unstable. One of the most popular approaches\n",
      "to developing practical state-space models for sequence modeling is diagonalization . The\n",
      "basic idea is that we can transform a state-space model into a new state-space where A(or\n",
      "A) is diagonalized. Given a state-space model parameterized by ( A,B,C,D), we can define\n",
      "a new state-space model ( UAU−1,BU−1,UC,D) by introducing an invertible matrix U.\n",
      "It is easy to prove that the two models are equivalent under the state-space transformation\n",
      "U21. By using this state-space transformation, and by noting that A(orA) can be written\n",
      "as a canonical form P−1ΛP22, we can enforce the constraint that A(orA) is a diagonal\n",
      "matrix, giving rise to diagonal state-space models . To illustrate, consider the filter used\n",
      "in the convolutional representation of the state-space model (see Eq. (182)). Assuming that\n",
      "A=P−1ΛP, we can write B·At·Cas\n",
      "B·At·C=B·(P−1ΛP)t·C\n",
      "=B·(P−1ΛP)·(P−1ΛP)···(P−1ΛP)·C\n",
      "= (B·P−1)·Λt·(P·C) (186)\n",
      "Since Λ is a diagonal matrix, we can efficiently compute Λtby simply raising all the\n",
      "entries of Λ to the t-th power. We then have a computationally cheaper model, in which\n",
      "A′= Λ (187)\n",
      "B′=B·P−1(188)\n",
      "C′=P·C (189)\n",
      "D′=D (190)\n",
      "More detailed discussions of diagonal state-space models in sequence modeling can be found\n",
      "in Gu et al. (2021)’s work.\n",
      "The application of state-space models to Transformer is simple. Each self-attention sub-\n",
      "layer is replaced in this case by an SSM sub-layer as described in Eqs. (164) and (165). As\n",
      "we have seen there is a close relationship between state-space models and both CNNs and\n",
      "RNNs. For sequence modeling, we can deal with a sequence of tokens either sequentially\n",
      "21. A state space transformation can be seen as a process of mapping all states from the old space to the\n",
      "new space, by\n",
      "s′(t) = s(t)·U (185)\n",
      "22. Λ denotes a diagonal matrix.\n",
      "68\n",
      "Page 69:\n",
      "Introduction to Transformers\n",
      "as in RNNs, or in parallel as in CNNs. This leads to a new paradigm that takes both the\n",
      "sequential view and the parallel view of the sequence modeling problem — for training, the\n",
      "system operates like CNNs to make use of fast parallel training algorithms; for prediction,\n",
      "the problem is re-cast as a sequential update problem which can be efficiently solved by\n",
      "using RNN-like models. It should be noted, however, that state-space models are found to\n",
      "underperform Transformer models for NLP problems, such as language modeling, although\n",
      "they have achieved promising results in several other fields. Further refinements are often\n",
      "needed to make them competitive with other widely used sequence models (Fu et al., 2022).\n",
      "While the formalism of state-space models is different from those we discussed in this\n",
      "chapter, it provides a general framework of sequence modeling in which the problem can be\n",
      "viewed from either of two different perspectives and we choose different ones for different\n",
      "purposes. Several recent sequence models were motivated by this idea, leading to systems\n",
      "exhibiting properties of both parallel training and RNN-style inference (Orvieto et al., 2023;\n",
      "Sun et al., 2023).\n",
      "5.6 Conditional Computation\n",
      "So far in our discussion of efficient Transformer models, we have assumed that the model\n",
      "architecture is given before beginning the training of a model and is then fixed throughout.\n",
      "We now turn to the case of learning efficient model architectures. Without loss of generality,\n",
      "we can write a model in the form\n",
      "y= Model( x, g(x)) (191)\n",
      "where xandyare the input and output of the model. g(x) is a model function that\n",
      "returns the model architecture and corresponding parameters for the given input x. In\n",
      "general, we adopt the convention prevalent in learning problems of using a fixed model\n",
      "architecture and learning only the parameters, say, g(x) =θ. In this case, the goal of\n",
      "learning is to find the optimal values of the parameters given the model architecture and\n",
      "training data. On test data, we make predictions using the same model architecture along\n",
      "with the optimized parameters.\n",
      "A natural extension of this approach is to consider the learning of both the model\n",
      "architecture and parameters. In architecture learning, we would like to find a model function\n",
      "ˆg(x) that produces the optimal model architecture and parameter values given the input\n",
      "x. However, searching a hypothesis space of all possible combinations of architectures and\n",
      "parameter choices is extremely difficult, and so we need practical methods to achieve the\n",
      "goal. Two classes of methods can be applied.\n",
      "•Neural Architecture Search (NAS ). Inautomated machine learning (AutoML ),\n",
      "neural architecture search is the process of exploring a space of neural networks to\n",
      "find one that best fits some criterion (Elsken et al., 2019; Zoph and Le, 2016). Once\n",
      "the optimal neural network is determined, its parameters will be trained as usual, and\n",
      "then be applied to new data. In order to make search tractable, several additional\n",
      "techniques, such as search space pruning and fast search algorithms, are typically\n",
      "used. Applying neural architecture search to the development of efficient neural net-\n",
      "works is straightforward (Howard et al., 2019; Tan and Le, 2019). We need only\n",
      "incorporate efficiency measures into the performance estimation of neural networks,\n",
      "69\n",
      "Page 70:\n",
      "Xiao and Zhu\n",
      "for example, the search can be guided by a criterion that penalizes neural networks\n",
      "with high latency or excessive memory requirements.\n",
      "•Dynamic Neural Networks . The key idea of dynamic neural networks is to adapt\n",
      "a neural network dynamically to various inputs (Gupta et al., 2004; Han et al., 2021).\n",
      "Ideally, we would like to learn ˆ g(·), and then, for any input xnew, we apply the model\n",
      "Model( xnew,ˆg(xnew)). As a result, at test time we may have different model structures\n",
      "and/or different parameters for different inputs. However, it is infeasible to develop\n",
      "a function ˆ g(·) that can model arbitrary neural networks. In practice, ˆ g(·) is often\n",
      "considered to represent a family of sub-networks of a super-network. The problem is\n",
      "therefore reframed as a simpler problem to learn to choose which sub-network is used\n",
      "for a given input.\n",
      "From a machine learning perspective, the approaches to neural architecture search are\n",
      "general and can be applied to any neural network. On the other hand, from a practical\n",
      "perspective, it is still difficult to find an efficient neural network that is sufficiently pow-\n",
      "erful and generalizes well. While neural architecture search provides interesting ideas for\n",
      "developing efficient Transformer models, we make no attempt to discuss it here. Instead,\n",
      "the reader can refer to the above papers to have a general idea of it, and refer to So et al.\n",
      "(2019), Wang et al. (2020a), and Hu et al. (2021)’s work for its application to Transformers.\n",
      "In this sub-section we focus on a particular family of approaches to dynamic neural\n",
      "networks, called conditional computation . This concept was originally motivated by\n",
      "the dynamic selection of neurons of a neural network (Bengio et al., 2015, 2013). More\n",
      "recently, it has often been used to refer to as a process of dynamically selecting parts of\n",
      "a neural network. A narrow view of conditional computation is to see g(·) as an adaptive\n",
      "neural network which dynamically reduces or grows the number of computation units (such\n",
      "as neurons and layers). As a result, computation can adapt to changing conditions, and we\n",
      "can seek a good accuracy-latency trade-off by this adaptation mechanism.\n",
      "A common way to achieve this is to learn how to skip some computation steps so that\n",
      "we can work with a necessary sub-set of the network (Xu and Mcauley, 2023). One of the\n",
      "simplest methods, sometimes called early stopping , is to stop the computation at some\n",
      "point during reading or generating a sequence. This technique is often used in practical\n",
      "sequence generation applications where a low latency is required. Suppose y1...ynmaxis\n",
      "the longest sequence that the system can generate, and s1...snmaxis the corresponding\n",
      "sequence of the states of the top-most Transformer layer. Then we develop a model fstop(·)\n",
      "that takes one hidden state siat a time and produces a distribution of a binary variable\n",
      "c∈ {stop,nonstop }\n",
      "Pr(c|si) = fstop(si) (192)\n",
      "The generation process terminates if Pr(stop |si) is sufficiently large, for example\n",
      "Pr(stop |si)≥Pr(nonstop |si) +θstop (193)\n",
      "where θstopdenotes the minimal margin for distinguishing the two actions23. This formula-\n",
      "tion is also related to the stopping criterion problem that is frequently discussed in search\n",
      "23. An equivalent form of Eq. (193) is Pr(stop |si)≥1+θstop\n",
      "2.\n",
      "70\n",
      "Page 71:\n",
      "Introduction to Transformers\n",
      "algorithms for sequence generation. fstop(·) can be designed in several different ways. For\n",
      "example, in many practical applications, the stopping criterion is based on simple heuris-\n",
      "tics. Alternatively, we can define the function fstop(·) as a neural network and train it using\n",
      "labeled data.\n",
      "The above approach can be easily extended to handle situations in which some of the\n",
      "tokens are skipped. This learning-to-skip approach is typically used in the encoding stage\n",
      "in which all input tokens are given in advance. Let h1...hmbe low-level representations of a\n",
      "sequence x1...xm. Like Eq. (192), we can develop a model Pr( c|si) (c∈ {skip,nonskip }) to\n",
      "determine whether the token xican be skipped. Figure 15 (a) and (b) show illustrations of\n",
      "early stopping and skipping. Note that the learning-to-skip method has overlap with other\n",
      "lines of research on training neural networks. For example, erasing some of the input tokens\n",
      "in training is found to be useful for achieving higher generalization of Transformer models\n",
      "(Kim and Cho, 2021; Shen et al., 2020). This method is also related to the downsampling\n",
      "methods which will be discussed in Section 5.8.\n",
      "A second approach to conditional computation is to resort to sparse expert models ,\n",
      "or its popular instance — MoE (Yuksel et al., 2012). In deep learning, a model of this\n",
      "kind is typically built from a number of experts which are neural networks having the same\n",
      "structure but with different parameters. In this way, we can construct a big model by\n",
      "simply increasing the number of experts. When running this model, during either training\n",
      "or prediction, we activate only a small number of the experts by some routing algorithm\n",
      "(see Figure 15 (c)). An MoE model is an adaptive network since each time we have a new\n",
      "input, the model routes it to different experts. In Section 4.4, we presented the basic form\n",
      "of MoE, and showed how Transformer models can be scaled up by this sparse method. For\n",
      "a comprehensive review of the recent advances in MoE, we refer the interested reader to\n",
      "Fedus et al. (2022a)’s work.\n",
      "A third approach that can be used to adapt a Transformer model to changing input is to\n",
      "dynamically shrink the number of layers. Several methods have been proposed to do this in\n",
      "an attempt to improve inference efficiency. The simplest of these is to exit at some hidden\n",
      "layer by which we can still make accurate predictions for the sample (see Figure 15 (d) and\n",
      "(e)). To do this, we can either determine the appropriate depth for the entire sequence (call\n",
      "it asentence-level depth-adaptive model ), or use an adaptive depth for each token (call\n",
      "it atoken-level depth-adaptive model ). Here we consider token-level depth-adaptive\n",
      "models but the methods can be easily extended to sequence-level depth-adaptive models.\n",
      "Suppose there are Lstacked layers at position i24. We would ideally like to find a layer\n",
      "in the stack, which can be used as the last hidden layer for making predictions, and whose\n",
      "depth is as low as possible. However, we cannot simply use the L-th layer of the stack as\n",
      "the oracle for this problem, because we never know in advance what the last layer generates\n",
      "during inference. Instead, we need to determine whether the network should stop growing\n",
      "at depth i, considering the layers generated so far.\n",
      "Now suppose we have a Transformer decoder which produces a distribution over a vo-\n",
      "cabulary Vat each step. As usual, we denote the output of the l-th layer at step ibysl\n",
      "i.\n",
      "For each sl\n",
      "i, we create an output layer that produces a distribution pl\n",
      "iover the vocabulary\n",
      "24. A layer is a standard Transformer block consisting of a few sub-layers.\n",
      "71\n",
      "Page 72:\n",
      "Xiao and Zhu\n",
      "y0 y1 y2 − −y1 y2 y3 stop\n",
      "(a) Early Stopping\n",
      "(Decoder)x1 x2 x3 x4 x5h1 h2 h3 h4 h5... ... ... ... ...\n",
      "(b) Token Skipping\n",
      "(Encoder)\n",
      "x1 x2 x3 x4 x5... ... ... ... ...\n",
      "expert1 expert2 expert3 expert4\n",
      "(c) MoE\n",
      "(Encoder)y0 y1 y2 y3 y4y1 y2 y3 y4 y5\n",
      "(d) Sentence-level Depth Adaptation\n",
      "(Decoder)\n",
      "y0 y1 y2 y3 y4y1 y2 y3 y4 y5\n",
      "(e) Token-level Depth Adaptation\n",
      "(Decoder)y0 y1 y2 y3 y4y1 y2 y3 y4 y5\n",
      "(f) Layer Skipping\n",
      "(Decoder)\n",
      "Figure 15: Methods of conditional computation, including early stopping, token skipping,\n",
      "MoE, sentence-level depth adaptation, token-level depth adaptation, and layer\n",
      "skipping. While these methods are illustrated using either the encoding or de-\n",
      "coding process, most of them can be applied to both Transformer encoders and\n",
      "decoders.\n",
      "72\n",
      "Page 73:\n",
      "Introduction to Transformers\n",
      "(call it an early exit classifier ), given by\n",
      "pl\n",
      "i= Softmax( sl\n",
      "i·Wl\n",
      "o) (194)\n",
      "where Wl\n",
      "o∈Rd×|V|is the parameter matrix. Hence we have L−1 additional output\n",
      "layers, each corresponding to a hidden layer from depth 1 to L−1. At training time, we\n",
      "consider the cross-entropy losses of {p1\n",
      "i, ...,pL−1\n",
      "i}, and train these layers together with the\n",
      "Transformer model. At test time, the depth of the network grows as usual, and we use\n",
      "{p1\n",
      "i, ...,pl\n",
      "i}and/or {s1\n",
      "i, ...,sl\n",
      "i}to determine whether we should exit at the l-th layer. There\n",
      "are several exit criteria, for example,\n",
      "•Common criteria are based on measures of the confidence of predictions. A simple\n",
      "method is to compute the entropy of pl\n",
      "i, and exit if this entropy is above a pre-defined\n",
      "value.\n",
      "•Alternatively, one can view the maximum probability of the entries of pl\n",
      "ias the con-\n",
      "fidence of the prediction.\n",
      "•Instead of considering the output of a single layer, we can also examine the change in\n",
      "the outputs or hidden states over a number of layers. For example, we can measure\n",
      "the similarity between pl−1\n",
      "iandpl\n",
      "ior between sl−1\n",
      "iandsl\n",
      "i. If the similarity is above a\n",
      "given threshold, then we say that the output of the neural tends to converge and the\n",
      "number of layers can stop growing.\n",
      "•The above method can be extended to examine the change in the predictions made by\n",
      "the classifiers associated with the layers. For example, the model can choose to exit\n",
      "if the predictions made by the classifiers remain unchanged for a number of layers.\n",
      "Discussions of these criteria can be found in the related papers (Schuster et al., 2022;\n",
      "Xin et al., 2020; Zhou et al., 2020). There are a variety of ways to improve these early exit\n",
      "methods. One is to explore other forms of the prediction for each layer. For example, we\n",
      "can develop a model that directly predicts how many layers we need to model the input\n",
      "(Elbayad et al., 2020). Another line of research on early exit focuses on better training for\n",
      "these models, for example, we can consider various loss functions for training the classifiers\n",
      "(Schuster et al., 2022; Schwartz et al., 2020). In addition, there is also interest in learning\n",
      "the combination of the outputs of multiple layers so that we can make predictions by using\n",
      "multiple levels of representation (Liao et al., 2021; Zhou et al., 2020).\n",
      "A problem with token-level adaptive-depth models is that the representations at certain\n",
      "depths may be absent in the previous steps. In this case, standard self-attention is not\n",
      "directly applicable, because we may not attend to the previous tokens in the same level\n",
      "of representation. For training, this can be addressed by using all the Llayers of the full\n",
      "model. For inference, we can either duplicate the layer from which we exit to fill up the\n",
      "layer stack, or modify the self-attention model to enable it to attend to the representations\n",
      "of the previous tokens at different depths.\n",
      "It is also possible to select any sub-set of the layers for constructing a shallow network.\n",
      "The adaptive models therefore can be generalized to skipping models (see Figure 15 (f)). As\n",
      "with the early exit problem, the skipping problem can be framed as a learning task, in which\n",
      "a classifier is trained to decide whether a layer should be dropped. The learning-to-skip\n",
      "problem has been studied in the field of computer vision (Wang et al., 2018b; Wu et al.,\n",
      "73\n",
      "Page 74:\n",
      "Xiao and Zhu\n",
      "2018b). However, learning a skipping model for large-scale, deep neural networks is difficult.\n",
      "For practical systems, it still seems reasonable to use heuristics or cheap models to obtain a\n",
      "neural network having skipped layers, which has been discussed in recent pre-trained NLP\n",
      "models (Del Corro et al., 2023; Wang et al., 2022c).\n",
      "5.7 Model Transfer and Pruning\n",
      "Many large Transformer models have been successfully developed to address NLP problems.\n",
      "A common question is: can we transform a large, well-trained model into a smaller one that\n",
      "allows for more efficient inference? At a high level, this can be thought of as a transfer\n",
      "learning problem in which the knowledge is transferred from one model to another. But\n",
      "we will not discuss this general topic, which spans a broad range of issues and models,\n",
      "many outside the scope of this chapter. Instead, we narrow our discussion to two kinds\n",
      "of approaches that are widely used in learning small neural networks from large neural\n",
      "networks.\n",
      "5.7.1 Knowledge Distillation\n",
      "Knowledge distillation is a process of compressing the knowledge in a large neural net-\n",
      "work (or an ensemble of neural networks) into a small neural network (Hinton et al., 2015).\n",
      "In supervised learning of neural networks, the objective functions are generally designed\n",
      "to represent some loss of replacing the true answer with the predicted answer. Hence we\n",
      "can minimize this loss so that the models are trained to output the true answer. While\n",
      "models are typically optimized on the training data in this manner, what we really want is\n",
      "to generalize them to new data. This is, however, difficult because we have no information\n",
      "about generalization in training with the ground-truth. In knowledge distillation, instead\n",
      "of forcing a model to stay close to the ground-truth output, we train this model to gener-\n",
      "alize. To do this, we directly transfer the knowledge (i.e., the generalization ability) of a\n",
      "pre-trained model to the model that we want to train.\n",
      "A frequently used approach to knowledge distillation is teacher-student training . A\n",
      "teacher model is typically a relatively large neural network that has already been trained\n",
      "and can generalize well. A student model is a relatively small neural network, such as a\n",
      "neural network with fewer layers, to which we transfer the knowledge. A simple way to\n",
      "distill the knowledge from the teacher model into the student model is to use the out-\n",
      "put of the teacher model as the “correct” answer for training the student model. Sup-\n",
      "pose we have a teacher Transformer model that can generate a sequence of distributions\n",
      "{Pr(·|y0,x), ...,Pr(·|y0...yn−1,x)}for the input x. To keep the notation simple, we denote\n",
      "the distribution Pr( ·|y0...yi−1,x) asepi. Similarly, we denote the output of the student Trans-\n",
      "former model for the same input as pi. As usual, we consider a loss function Loss(epi,pi)\n",
      "(such as the cross-entropy function) for computing some distance between epiandpi. Then,\n",
      "we can define the loss over the entire sequence as\n",
      "L(x, θ) =1\n",
      "nnX\n",
      "i=1Loss(epi,pi) (195)\n",
      "74\n",
      "Page 75:\n",
      "Introduction to Transformers\n",
      "where θdenotes the parameters of the student model25. Using this loss, we can optimize θ,\n",
      "for any given set of source sequences {x1, ...,xK}, in such a way as to minimize the qualityPN\n",
      "k=1L(xk, θ).\n",
      "Several different extensions to this basic method have been developed to model the\n",
      "problem of knowledge transfer between two models. A simple way is to use the hidden\n",
      "states instead of the output probabilities as the training targets (Romero et al., 2014). In\n",
      "this case, the objective is to minimize the difference between some hidden states of the\n",
      "teacher model and the corresponding states of the student model. Rather than using the\n",
      "outputs of various layers as the targets for training the student model, another technique\n",
      "is to model the relations between samples and train the student model by minimizing some\n",
      "difference between the relation encodings of the teacher and student models (Park et al.,\n",
      "2019; Peng et al., 2019). For example, we can develop a relation encoding model based on\n",
      "the Transformer architecture. The goal is then to optimize the student model so that its\n",
      "corresponding relation encoding of a group of samples is as close as possible to that of the\n",
      "teacher model.\n",
      "For sequence generation problems, a special case of knowledge distillation, which can be\n",
      "viewed as a means of data augmentation , is often used for developing lightweight models\n",
      "(Kim and Rush, 2016). For example, consider the problem of transferring the translation\n",
      "ability of a well-developed machine translation model (i.e., the teacher model) to a new\n",
      "model (i.e., the student model). Given a set of source-side sentences {x1, ...,xK}, we can use\n",
      "the teacher model to translate each xkto a target-side sentence eyk. Then, by treating xkand\n",
      "eykas paired sentences, we obtain a bilingual dataset consisting of {(x1,ey1), ...,(xK,eyK)}.\n",
      "We can use this bilingual dataset as the labeled dataset to train the student model as usual.\n",
      "One advantage of this data argumentation method is that it is architecture free, and we do\n",
      "not even need to understand the internal architectures of the teacher and student models.\n",
      "Hence we can apply this method if we have a black-box teacher model. More detailed\n",
      "discussions of knowledge distillation can be found in Gou et al. (2021) and Wang and Yoon\n",
      "(2021)’s surveys.\n",
      "5.7.2 Structured Pruning\n",
      "Pruning is among the most popular of the model compression methods and has been applied\n",
      "to a broad range of systems. One common approach to pruning is unstructured pruning ,\n",
      "by which we activate only some of the connections between neurons. However, as with most\n",
      "sparse models, models pruned in this way typically require special implementations and\n",
      "hardware support, which in turn reduces their efficiency in some applications. A simple\n",
      "but more aggressive way to do pruning is to use structured pruning . In deep learning,\n",
      "structured pruning is a technique that removes a group of neurons or connections together.\n",
      "For example, we can remove an entire layer of neuron from a neural network to obtain a\n",
      "shallower model. As multi-layer, multi-head neural networks, Transformers are naturally\n",
      "suited to structured pruning, and we can prune a Transformer network in several different\n",
      "ways. For example, we can prune some of the heads in multi-head attention (Michel et al.,\n",
      "2019; Voita et al., 2019), or some of the layers in the layer stack (Hou et al., 2020; Kim and\n",
      "Awadalla, 2020).\n",
      "25. We omit the parameters of the teacher model because they are fixed throughout the training process.\n",
      "75\n",
      "Page 76:\n",
      "Xiao and Zhu\n",
      "Formally, we can represent a neural network as a set of parameter groups {θ1, ..., θ R},\n",
      "each corresponding to a component or sub-model of the model. Our goal is to find a sub-set\n",
      "of{θ1, ..., θ R}by which we can build a model that yields good performance, while having\n",
      "a lower model complexity. However, a simple search of such a model is infeasible because\n",
      "there are a combinatorially large number of possible model candidates and evaluating all of\n",
      "these models is computationally expensive.\n",
      "One approach to structured pruning is to randomly prune components of a model.\n",
      "One can run the random pruning process a number of times to generate a pool of model\n",
      "candidates and select the best one from the pool. Another approach is to use heuristics to\n",
      "decide which components are not important and can be removed. Common measures of the\n",
      "importance of a parameter group θrinclude various qualities based on norms of the weights\n",
      "or gradients of θr(Santacroce et al., 2023). We can prune θrif the values of these measures\n",
      "are below (or above) given thresholds. A third approach is to frame the pruning problem\n",
      "as an optimization task by introducing trainable gates indicating the presence of different\n",
      "components (Lagunas et al., 2021; McCarley et al., 2019; Wang et al., 2020c). The pruned\n",
      "model can be induced by using the trained gates. Note that, in many cases, pruning is not\n",
      "a post-processing step for a given trained model, but part of the training.\n",
      "5.8 Sequence Compression\n",
      "In sequence modeling and generation problems, the time and space complexities are strongly\n",
      "influenced by the length of the input or output sequence, and we prefer the sequence to\n",
      "be short. This is particularly important for Transformer models, as their time and space\n",
      "complexities are quadratic with the sequence length, and the memory footprint and latency\n",
      "can be heavy burdens if the sequence is very long. In the previous subsections, we have\n",
      "discussed modifications to the Transformer architecture for dealing with long sequences.\n",
      "Here we instead consider methods for compressing the sequences into ones with acceptable\n",
      "lengths.\n",
      "One simple approach is to map the input sequence to a fixed-size representation. For\n",
      "example, using the recurrent models discussed in Section 5.2, we can encode a sequence\n",
      "of vectors into a single vector. This method can be easily extended to generate a “larger”\n",
      "representation so that this representation can retain more information of the original input.\n",
      "For example, we can select a fixed number of the hidden states over the sequence to form\n",
      "a new sequence of fixed-length. Another way to represent a variable-length sequence as a\n",
      "fixed-length sequence is to attend the input vectors to some hidden states, usually a fixed\n",
      "number of learnable hidden representations. In Jaegle et al. (2021)’s work, this is done\n",
      "by introducing rhidden representations {u1, ...,ur}, and then attending the input vectors\n",
      "{x1, ...,xm}to these hidden representations. The attention model can be a standard QKV\n",
      "attention model in which we view {u1, ...,ur}as queries and {x1, ...,xm}as keys and values.\n",
      "The output of this model is a sequence of rvectors, which can be used as fixed-length input\n",
      "to downstream systems.\n",
      "A second approach is to use downsampling to compress the sequence into a shorter one.\n",
      "A typical method of downsampling is strided convolution, which has been widely used in\n",
      "computer vision and speech processing. For example, suppose there is a sequence of m\n",
      "vectors ∈Rd. We can develop a filter with a width of 2 and a stride of 2. By taking the\n",
      "76\n",
      "Page 77:\n",
      "Introduction to Transformers\n",
      "sequence as input, the filter produces a sequence ofm\n",
      "2new vectors ∈Rd, and so we have\n",
      "a reduction rate of 2. Also, we can stack multiple convolutional layers or pooling layers to\n",
      "achieve a desired level of length reduction, called progressive downsampling . However,\n",
      "it seems inevitable that downsampling will lead to information loss (Burchi and Vielzeuf,\n",
      "2021; Han et al., 2020). We need to consider a trade-off between the compressed sequence\n",
      "length and the performance of downstream systems (Xu et al., 2023b).\n",
      "In NLP, the problem of sequence compression is also closely related to the problem of\n",
      "tokenizing input strings. Therefore, tokenization is a practical approach that can be taken\n",
      "to address the length issue. Segmenting a string into small tokens (such as characters)\n",
      "generally reduces the sparsity of the data, which makes it easier to learn the embeddings\n",
      "of these tokens, but such approaches often lead to a long sequence. By contrast, we will\n",
      "have a shorter sequence if we segment the input string into larger units, but this will\n",
      "suffer from sparse data. In deterministic tokenization methods, which produce tokenization\n",
      "results using statistics collected from the entire dataset, the sequence length can be somehow\n",
      "controlled by adjusting some hyper-parameter, for example, in byte pair encoding (Sennrich\n",
      "et al., 2016), increasing the size of the vocabulary generally reduces the number of the\n",
      "resulting tokens. Another way to obtain an appropriate sequence of tokens is to use a\n",
      "model for choosing among tokenization candidates (Kudo, 2018; Provilkov et al., 2020). As\n",
      "with many probabilistic models for text generation, in this case, we can add priors to the\n",
      "criterion for tokenization selection so that we can express a preference for shorter sequences\n",
      "over longer sequences.\n",
      "A fourth approach to sequence compression is to drop some of the tokens in the sequence.\n",
      "For example, in many practical applications, we chop the sequence when its length exceeds\n",
      "a threshold. We can relate this to the early stopping and skipping approaches in conditional\n",
      "computation. Thus the methods discussed in Section 5.6 are directly applied. The token\n",
      "dropping methods can also be viewed as pruning methods, called token pruning . By\n",
      "discarding tokens that are less important for representing the entire sequence, token pruning\n",
      "can significantly reduce the sequence length while maintaining the performance of NLP\n",
      "systems on downstream tasks (Kim et al., 2023).\n",
      "5.9 High Performance Computing Methods\n",
      "So far in this section, we have discussed efficient Transformer models from the perspectives\n",
      "of deep learning and NLP. However, we have not considered their efficiency on hardware.\n",
      "As modern hardware provides a variety of modes for running a program, the practical time\n",
      "and memory footprint savings generally depend on the specifications of hardware systems.\n",
      "One line of research on efficient use of computing resources explores methods of parallel\n",
      "computing. There have been many attempts to develop large-scale Transformer models by\n",
      "using a cluster of machines. Typically, scaling Transformers to models with billions or even\n",
      "tens of billions of parameters requires a careful design of parallelism strategies for sharding\n",
      "the big networks. More efficient implementations of such systems also need considerations\n",
      "of networking and communication in the cluster, as well as the utilization of sparse models\n",
      "that activate only a small sub-set of the parameters for each sample, enabling the use of\n",
      "very large models. Most of these methods have been studied in an extensive literature on\n",
      "how to scale up the training of deep neural networks like Transformers efficiently (Barham\n",
      "77\n",
      "Page 78:\n",
      "Xiao and Zhu\n",
      "et al., 2022; Fedus et al., 2022b; Lepikhin et al., 2021). The results of these studies were\n",
      "foundational to many follow-on works on investigating the scaling laws for large language\n",
      "models (Brown et al., 2020; Chowdhery et al., 2022). Since large-scale distributed models\n",
      "are generic and not specialized to the case of Transformers, we skip the discussion of them\n",
      "here. The interested readers can refer to the above papers for more detailed discussions.\n",
      "In this sub-section we consider hardware-aware methods to seek greater computational\n",
      "efficiency for Transformer models. We first consider a simple but widely used method\n",
      "that aims to store and execute neural networks using lower or mixed-precision number\n",
      "representations (Gholami et al., 2022). Conventional neural networks are typically based on\n",
      "single-precision and/or double-precision floating-point representations of data. While single-\n",
      "precision floating-point data types provide a sufficiently precise way to represent parameters\n",
      "and intermediate states in most cases, in some applications, they are not essential. As an\n",
      "alternative, one can use half-precision (or even lower-precision) formats in storing floating-\n",
      "point numbers for neural networks. The size of the resulting model is thus half the size of the\n",
      "original model. One advantage of using half-precision floating-point representations is that,\n",
      "although processing such data types requires new APIs of linear algebra operations and\n",
      "hardware support, it does not change the model architecture, and so we need only a slight\n",
      "modification to the systems. For example, half-precision floating-point representations can\n",
      "be applied to either training or inference of Transformers, or both.\n",
      "Recently, the deployment of large Transformer models has been further improved by\n",
      "quantizing these models. In signal processing, quantization is a process of mapping con-\n",
      "tinuous values (i.e., floating-point representations) to a set of discrete values (i.e., fix-point\n",
      "representations) . This process is in general implemented using a system called quantizer.\n",
      "In the context of neural networks, a quantizer involves two functions — the quantization\n",
      "function and the de-quantization function. The quantization function maps a floating point\n",
      "number to a (lower-bit) integer. A simple quantization function is given by\n",
      "Q(x) = ⌊x\n",
      "s⌉ (196)\n",
      "where ⌊·⌉is a rounding function26,xis the real-valued input, and sis the quantization step\n",
      "size that controls the level of quantization. The quantization function is coupled with a\n",
      "de-quantization function\n",
      "D(r) = s·r (197)\n",
      "With this notation, the quantizer can be expressed as D(Q(x)) = s· ⌊x\n",
      "s⌉. The difference\n",
      "between D(Q(x)) and xis called quantization error. A smaller value of stypically reduces\n",
      "the quantization error. In practice, however, we wish to choose an appropriate value of s\n",
      "in order to spread possible values of Q(r) evenly across values of an integer, for example,\n",
      "s=max{D(r)}\n",
      "2p−1where pis the number of bits used to represent an integer and max {D(r)}\n",
      "is the maximum value for D(r). The above equations show one of the simplest cases of\n",
      "quantization. More general discussions of quantization can be found in books on digital\n",
      "signal processing and related surveys (Gray, 1998; Oppenheim and Schafer, 1975; Rabiner\n",
      "and Gold, 1975).\n",
      "26.⌊a⌉returns the integer closest to a.\n",
      "78\n",
      "Page 79:\n",
      "Introduction to Transformers\n",
      "Applying quantization to Transformers is relatively straightforward. The idea is that\n",
      "we quantize the inputs and model parameters using Q(x), and feed them to a quantized\n",
      "Transformer model in which all the layers operate on integer-valued tensors. In other words,\n",
      "we implement the model using integer-only arithmetic. However, the price to be paid for\n",
      "this compressed model, as with many approximation approaches to deep learning, is that\n",
      "its prediction is not as accurate as that of the standard Transformer model. Using integer\n",
      "operations to approximate continuous-valued operations generally leads to approximation\n",
      "errors. These errors will be accumulated if the quantized neural network is deep. Fur-\n",
      "thermore, Transformer models involve components (such as self-attention sub-layers) that\n",
      "require relatively complex linear algebra operations. Simply applying quantization to these\n",
      "sub-models will lead to high accuracy loss. One solution is to simplify the model architec-\n",
      "ture and develop new sub-models that is more feasible for quantization. Alternatively, a\n",
      "more common paradigm in quantized neural networks is to add de-quantization functions\n",
      "to the neural networks so that the output of a layer is floating-point tensors and can be used\n",
      "as usual in the following steps. Consider a simple example where we multiply a real-valued\n",
      "input matrix awith a real-valued parameter matrix A. We first quantize aandA, and\n",
      "multiply them using integer-based matrix multiplication. The result is then de-quantized\n",
      "to a real-valued matrix. In this way, we obtain an approximation D(Q(a)·Q(A)) toa·A\n",
      "in a very cheap way.\n",
      "However, sandwiching each layer between Q(·) and D(·) will lead to additional cost\n",
      "of running Q(·) and D(·). In some practical applications, the computational overhead\n",
      "introduced by Q(·) and D(·) is even bigger than the time saving of performing integer-\n",
      "based operations. In general, the benefit of quantizing neural networks would be larger\n",
      "than its cost if the neural networks are large. Therefore, in practice it is common to\n",
      "perform quantized computation only for operations whose computational costs are high.\n",
      "For example, in recent large language models, quantization is primarily applied to the\n",
      "multiplication of large matrices, yielding significant time and memory savings.\n",
      "While the quantization approaches can be used in both training and inference, a widely-\n",
      "used approach is to get Transformer models quantized after training (call it post-training\n",
      "quantization ). In this approach, quantization is performed on well-trained floating-point-\n",
      "based neural networks and there will be fewer quantization-related errors. However, these\n",
      "errors cannot be compensated for because they exist after training. A more promising\n",
      "idea is to involve quantization in training so that the model can learn to compensate for\n",
      "quantization-related errors (Jacob et al., 2018; Nagel et al., 2021). There have been several\n",
      "attempts to apply quantization-aware training to Transformers (Bondarenko et al., 2021;\n",
      "Stock et al., 2021; Yang et al., 2023b). In addition to computational efficiency, another\n",
      "important consideration for high-performance systems is the restrictions of the memory\n",
      "hierarchy. In general, better system design requires considering the speeds and sizes of\n",
      "different levels of memory. The problem is even more complicated when we train large\n",
      "Transformer models on modern hardware where both GPUs and CPUs are used. A general\n",
      "principle of system design is that memory transfer between different memory levels should be\n",
      "minimized. While we would ideally like to have a large high-level memory on which we can\n",
      "store all the data that we need to process, in many practical situations the size of the fast,\n",
      "on-chip memory is orders of magnitude smaller than the size of data. In this case, we can re-\n",
      "order the memory access in the algorithms so that the data used in nearby computation steps\n",
      "79\n",
      "Page 80:\n",
      "Xiao and Zhu\n",
      "can be loaded into the high-speed memory at one time. This idea motivates the development\n",
      "of many fast linear algebra libraries. For example, there are matrix multiplication algorithms\n",
      "that are highly optimized for different shapes of input matrices.\n",
      "It is relatively straightforward to use these optimized linear algebra algorithms to build\n",
      "a Transformer system. But the modules of this system are not optimized as a whole for ef-\n",
      "ficiency improvement. For example, a self-attention sub-layer involves a series of operations\n",
      "of scaling, normalization, and matrix multiplication. Although each of these operations\n",
      "has been implemented in several supported and efficient libraries of linear algebra, succes-\n",
      "sive calls to them still require multiple times of memory transfer when we switch from one\n",
      "operation to another. In practice, a better approach would be that we keep some of the\n",
      "intermediate states in the on-chip memory, and reuse them in the following computation\n",
      "steps instead of fetching them again from the slow memory. For example, on modern GPUs,\n",
      "a simple way to achieve this is to merge multiple operations into a single operation, known\n",
      "askernel fusion . For Transformer models, a general idea is to design data partitioning\n",
      "and layout strategies by which we maximize the computation on each data block loaded\n",
      "into the high-performance memory, while at the same time minimizing the memory transfer.\n",
      "There have been several attempts to use these strategies to improve the attention models\n",
      "in Transformers (Ivanov et al., 2021; Pope et al., 2023). Some of these methods, such as\n",
      "flash attention and paged attention, have been successfully incorporated into recent large\n",
      "language models (Dao et al., 2022; Kwon et al., 2023).\n",
      "6. Applications\n",
      "Transformers have a wide range of applications, covering numerous NLP problems. While\n",
      "the Transformer model introduced by Vaswani et al. (2017) is based on a standard encoder-\n",
      "decoder architecture, it is mainly used in three different ways.\n",
      "•Decoder-only Models . By removing the cross-attention sub-layers from a Trans-\n",
      "former decoder, the decoder becomes a standard language model. Hence this decoder-\n",
      "only model can be applied to text generation problems. For example, given a sequence\n",
      "of left-context tokens, we use the model to predict the next and following tokens.\n",
      "•Encoder-only Models . Transformer encoders can be treated as sequence models\n",
      "that take a sequence of tokens at once and produce a sequence of representations, each\n",
      "of which corresponds to an input token. These representations can be seen as some\n",
      "sort of encoding of the input sequence, and are often taken as input to a prediction\n",
      "model. This encoder+predictor architecture forms the basis of many NLP systems, for\n",
      "example, systems of sentence classification, sequence labeling, and so on. Pre-trained\n",
      "Transformer encoders can also be used to map texts into the same vector space so\n",
      "that we can compute the distance or similarity between any two texts.\n",
      "•Encoder-Decoder Models . Encoder-decoder models are typically used to model\n",
      "sequence-to-sequence problems. Applications include many tasks in NLP and related\n",
      "fields, such as machine translation and image captioning.\n",
      "Note that while most Transformer-based systems can fall into the above three categories,\n",
      "the same NLP problem can generally be addressed using different types of models. For\n",
      "example, recent decoder-only models have demonstrated good performance on a broad\n",
      "80\n",
      "Page 81:\n",
      "Introduction to Transformers\n",
      "range of problems by framing them as text generation tasks, though some of these problems\n",
      "were often addressed by using encoder-decoder or encoder-only models. To illustrate how\n",
      "the above models are applied, this section considers a few applications where Transformers\n",
      "as chosen as the backbone models.\n",
      "6.1 Language Modeling\n",
      "Language modeling is an NLP task in which we predict the next token given its preceding\n",
      "tokens. This is generally formulated as a problem of estimating the distribution of tokens\n",
      "at position i+1 given tokens at positions 0 ∼i(denoted by Pr( ·|x0, ..., x i) where {x0, ..., x i}\n",
      "denote the tokens up to position i). The best predicted token is the one which maximizes\n",
      "the probability, given by\n",
      "ˆxi+1= arg max\n",
      "xi+1∈VPr (xi|x0, . . . , x i) (198)\n",
      "where Vis the vocabulary. The prediction can be extended to the tokens following ˆ xi+1\n",
      "ˆxk+1= arg max\n",
      "xk+1∈VPr (xk|x0, . . . , x i,ˆxi+1, . . . , ˆxk) (199)\n",
      "This model forms the basis of many systems for text generation: given the context tokens\n",
      "x1...xi, we generate the remaining tokens ˆ xi+1...ˆxk+1to make the sequence complete and\n",
      "coherent.\n",
      "As discussed in Section 2.1, Transformer decoders are essentially language models. The\n",
      "only difference between the problem of decoding in an encoder-decoder Transformer and\n",
      "the problem of language modeling is that the Transformer decoder makes predictions con-\n",
      "ditioned on the “context” tokens on both the encoder and decoder sides, rather than being\n",
      "conditioned on preceding tokens solely on one side. To modify the Transformer decoder to\n",
      "implement a standard language model, the cross-attention sub-layers are simply removed\n",
      "and a Transformer decoding block can be expressed as\n",
      "Sl= Layerffn(Sl\n",
      "self) (200)\n",
      "Sl\n",
      "self= Layerself(Sl−1) (201)\n",
      "HereSldenotes the output of the block at depth l. Layerself(·) denotes the self-attention\n",
      "sub-layer, and Layerffn(·) denotes the FFN sub-layer. We see that this decoding block has\n",
      "the same form as an encoding block. The difference between the decoding and encoding\n",
      "blocks arises from the masking strategies adopted in training, because the former masks\n",
      "the attention from a position ito any right-context position k > i whereas the latter\n",
      "has no such restriction. A Softmax layer is stacked on the top of the last block, and is\n",
      "used to produce the distribution over the vocabulary at each position. For inference, the\n",
      "Transformer decoder works in an auto-regressive manner, as described in Eq. (199).\n",
      "The training of this model is standard. We learn the model by repeatedly updating the\n",
      "parameters, based on the gradients of the loss on the training samples. This paradigm can\n",
      "be extended to the training of large Transformer-based language models, which have been\n",
      "widely applied in generative AI. However, training Transformer models at scale, including\n",
      "81\n",
      "Page 82:\n",
      "Xiao and Zhu\n",
      "decoder-only, encoder-only, and encoder-decoder models, may lead to new difficulties, such\n",
      "as training instabilities. We will discuss these issues further in the following chapters, where\n",
      "large-scale pre-training is the primary focus.\n",
      "6.2 Text Encoding\n",
      "For many NLP problems, a widely used paradigm is to first represent an input sequence in\n",
      "some form, and then make predictions for downstream tasks based on this representation.\n",
      "As a result, we separate sequence modeling or sequence representation from NLP tasks.\n",
      "One of the advantages of this paradigm is that we can train a sequence model that is not\n",
      "specialized to particular tasks, thereby generalizing well.\n",
      "Clearly, Transformer encoders are a type of sequence model, and can be used as text\n",
      "encoders. Consider a Transformer encoder with Lencoding blocks. The output of the last\n",
      "encoding block can be seen as the encoding result. Here add a special token x0to any\n",
      "sequence, indicating the beginning of a sequence (written as ⟨SOS⟩or [CLS]). If there is a\n",
      "sequence of m+ 1 input tokens x0x1...xm, the output of the encoder will be a sequence of\n",
      "m+1 vectors hL\n",
      "0hL\n",
      "1...hL\n",
      "m. Since x0is not a real token and has a fixed positional embedding,\n",
      "it serves as a tag for collecting information from other positions using the self-attention\n",
      "mechanism. Hence hL\n",
      "0is a representation of the entire sequence, with no biases for any\n",
      "specific tokens or positions. In many cases, we need a single representation of a sequence\n",
      "and take it as input to downstream components of the system, for example, we can construct\n",
      "a sentence classification system based on a single vector generated from {hL\n",
      "0, ...,hL\n",
      "m}. In\n",
      "this case, we can simply use hL\n",
      "0as the representation of the sequence. A more general\n",
      "approach is to add a pooling layer to the encoder. This allows us to explore various pooling\n",
      "methods to generate the sequence embedding from {hL\n",
      "0, ...,hL\n",
      "m}.\n",
      "In text encoding, token sequences are represented by real-valued vectors, often referred\n",
      "to as sentence representations or sentence embeddings, which can be seen as points in a\n",
      "multi-dimensional space (Hill et al., 2016). Another way to make use of text encoding, there-\n",
      "fore, is to obtain semantic or syntactic similarities of token sequences based on their relative\n",
      "positions or proximity in this space. A straightforward method for this is to compute the\n",
      "Euclidean distances between sequence embeddings. The shorter the distance between two\n",
      "sequences, the more similar they are considered to be. There are many distance metrics\n",
      "we can choose, and it is possible to combine them to obtain a better measure of sequence\n",
      "similarity. Such similarity computations are applied in areas such as text entailment, in-\n",
      "formation retrieval, translation evaluation, among others (Cer et al., 2018; Reimers and\n",
      "Gurevych, 2019). Additionally, they are often used to assess the quality of text encoding\n",
      "models.\n",
      "Text encoding is also a crucial component of sequence-to-sequence models. Given this,\n",
      "we can develop a separate Transformer encoder for source-side sequence modeling in an\n",
      "encoder-decoder system (see Figure 16). For example, we can pre-train a Transformer\n",
      "encoder on large-scale source-side texts, and use it as the encoder in a downstream encoder-\n",
      "decoder model. It is worth noting that while the encoder is designed based on the Trans-\n",
      "former architecture, the decoder is not confined to just Transformers. Such flexibility en-\n",
      "ables us to incorporate pre-trained Transformer encoders into hybrid sequence-to-sequence\n",
      "architectures, such as systems that combine a Transformer encoder with an LSTM decoder.\n",
      "82\n",
      "Page 83:\n",
      "Introduction to Transformers\n",
      "Encoder\n",
      "⟨CLS⟩Never give up.PoolingClassifier (e.g., Softmax)\n",
      "(a) ClassificationEncoder\n",
      "⟨CLS⟩Never give up.Pooling\n",
      "Encoder\n",
      "⟨CLS⟩Never say die !PoolingSimilarity Computation (e.g., FFN)Similarity = 0 .7\n",
      "(b) Similarity Computation\n",
      "Encoder\n",
      "⟨CLS⟩Never give up.Decoder\n",
      "⟨SOS⟩永不放弃永不放弃。\n",
      "(c) Sequence-to-Sequence Modeling\n",
      "Figure 16: Integrating Transformer encoders as components of different systems. A com-\n",
      "mon approach is to feed the output of the encoder (with pooling) into a classifier\n",
      "to obtain a sequence classification system. Another way to utilize Transformer\n",
      "encoders is to compute the similarity between two sequences. We use the same\n",
      "encoder to represent the two sequences, and then construct a neural network on\n",
      "top of the two representations for producing a similarity score between them.\n",
      "As usual, Transformer encoders can also be used in encoder-decoder systems to\n",
      "model sequence-to-sequence problems.\n",
      "83\n",
      "Page 84:\n",
      "Xiao and Zhu\n",
      "In supervised learning scenarios, training a Transformer encoder is straightforward. We\n",
      "can treat it as a regular component of the target model and train this model on task-specific\n",
      "labeled data. However, such a method requires the encoder to be optimized on each task,\n",
      "and the resulting encoder might not always generalize well to other tasks, especially given\n",
      "that labeled data is scarce in most cases. A more prevalent approach is to frame the training\n",
      "of text encoders as an independent task in which supervision signals are derived solely from\n",
      "raw text. This led researchers to develop self-supervised Transformer encoders, such as\n",
      "BERT, which make use of large-scale unlabeled text, and these encoders were found to\n",
      "generalize well across many downstream tasks.\n",
      "6.3 Speech Translation\n",
      "As illustrated in Section 2, the standard encoder-decoder Transformer model was proposed\n",
      "to model sequence-to-sequence problems. Here we consider the problem of translating\n",
      "speech in one language to text in another language — a problem that is conventionally\n",
      "addressed using both automatic speech recognition (ASR) and machine translation tech-\n",
      "niques. Instead of cascading an automatic speech recognition system and a machine trans-\n",
      "lation system, we can use Transformer models to build an end-to-end speech-to-text (S2T)\n",
      "translation system to directly translate the input speech to the output text.\n",
      "To simplify the discussion, we assume that the input of an S2T translation system is a\n",
      "sequence of source-side acoustic feature vectors, denoted by a1...am, and the output of the\n",
      "system is a sequence of target-side tokens, denoted by y1...yn.27Mapping a1...amtoy1...yn\n",
      "is a sequence-to-sequence problem. Thus it is straightforward to model the problem using\n",
      "an encoder-decoder Transformer model, and the training and inference of this model are\n",
      "standard, like in neural machine translation.\n",
      "In S2T translation, however, we have to deal with sequence mappings between modalities\n",
      "and between languages simultaneously. This poses new challenges compared with conven-\n",
      "tional machine translation problems and influences the design of S2T translation models.\n",
      "There have been several improvements to Transformer models for adapting them better to\n",
      "S2T translation tasks. Some of the improvements concern the design of Transformer blocks\n",
      "(Di Gangi et al., 2019). For example, in Gulati et al. (2020)’s system, a CNN sub-layer\n",
      "and relative positional embeddings are integrated into each Transformer block, enabling the\n",
      "model to efficiently capture both local and global features.\n",
      "Another line of research on S2T translation focuses on improving the encoder-decoder\n",
      "architecture. This involves modifications to either encoders or decoders, or both. To illus-\n",
      "trate, Figure 17 shows the architectures of three S2T translation models. All of them are\n",
      "based on Transformers, but have different encoder architectures. As shown in the figure,\n",
      "the standard encoder-decoder architecture has one Transformer encoder for reading the\n",
      "source-side input a1...amand one Transformer decoder for producing the target-side output\n",
      "y1...yn. By contrast, the decoupled encoder model separates the encoder into two stacked\n",
      "encoders — one for acoustic modeling (call it the speech encoder ), and one for textual\n",
      "27. In order to obtain the input sequence to the system, we need to discretize continuous speech into signals\n",
      "represented by feature vectors. This process is typically nontrivial, requiring either a feature extractor\n",
      "based on a variety of signal processing operations or a neural network that learns feature mappings in\n",
      "an end-to-end manner. But we will not dive into the details of these methods and simply treat the input\n",
      "feature extractor as an upstream system.\n",
      "84\n",
      "Page 85:\n",
      "Introduction to Transformers\n",
      "modeling (call it the text encoder ) (Liu et al., 2020c; Xu et al., 2021a). This design\n",
      "reflects a modeling hierarchy in which representations in different levels of the network are\n",
      "concerned with different aspects of the problem, for example, the speech encoder models\n",
      "low-level features in mapping acoustic embeddings into larger language units, and the text\n",
      "encoder models the semantic or syntactic features in representing the entire input sequence.\n",
      "An advantage of separating out the text encoder is that the encoding process follows our\n",
      "prior knowledge that we need to first transcribe the speech input and then translate the\n",
      "transcript into the target language. Therefore, we can train the speech encoder in some\n",
      "way we train an ASR system. This enables us to pre-train the speech encoder and the text\n",
      "encoder on unlabeled data, and incorporate the pre-trained encoders into S2T translation\n",
      "systems.\n",
      "An alternative encoding architecture is the two-stream architecture, as shown in Figure\n",
      "17 (c). Like the decoupled encoder architecture, this architecture has a speech encoder and\n",
      "a text encoder, but the two encoders work in parallel rather than in sequence (Ye et al.,\n",
      "2021). The speech encoder takes acoustic features as input and the text encoder takes\n",
      "tokens (or their embeddings) as input. A third encoder, called shared encoder , integrates\n",
      "the outputs from both the speech and text encoders, merging the representations from the\n",
      "two modalities. This two-stream architecture is flexible because it provides multiple ways\n",
      "to train S2T translation models. A common approach is to train each branch individually.\n",
      "For example, if we mask the speech encoder, then the model will transform into a machine\n",
      "translation model which can be trained using bilingual texts. Conversely, if we mask the text\n",
      "encoder, then we can train the model as a standard S2T translation model. For inference,\n",
      "the text encoder can be dropped, and the speech input is modeled using the speech encoder\n",
      "and the shared encoder.\n",
      "In deep learning, training is often related to architecture design. Here, we have data in\n",
      "two modalities and two languages, and so we can develop multiple supervision signals for\n",
      "multi-task learning of S2T translation models. A widely used method is to introduce ASR-\n",
      "related loss into the training of speech encoders. For example, in the decoupled encoder\n",
      "model, a classifier can be constructed based on the output from the speech encoder. By\n",
      "minimizing the connectionist temporal classification (CTC) loss for this classifier, the speech\n",
      "encoder can be optimized in a manner similar to ASR. In general, training S2T translation\n",
      "models is challenging because speech-to-text aligned data is scarce. Among typical responses\n",
      "to this challenge are data augmentation, pre-training, knowledge distillation with machine\n",
      "translation, and so on. However, an in-depth discussion of these methods goes beyond the\n",
      "scope of this discussion on Transformers. The interested reader can refer to a recent survey\n",
      "on speech translation for more information (Xu et al., 2023a).\n",
      "6.4 Vision Models\n",
      "While Transformers were first used in NLP, their application to other domains has been\n",
      "a prominent research topic. In computer vision, for instance, there is a notable trend\n",
      "of shifting from CNNs to Transformers as the backbone models. In this sub-section, we\n",
      "consider Vision Transformer (ViT) - an interesting application of Transformers to image\n",
      "classification (Dosovitskiy et al., 2021). Vision Transformer is a milestone model which\n",
      "opens the door to purely Transformer-based vision models. Here we consider the basic\n",
      "85\n",
      "Page 86:\n",
      "Xiao and Zhu\n",
      "Encoder\n",
      "(Speech)\n",
      "Speech (Source)DecoderText (Target)\n",
      "(a) Single Encoder + Single Decoder\n",
      "Encoder\n",
      "(Speech)\n",
      "Speech (Source)Encoder\n",
      "(Text)\n",
      "Transcript (Source)DecoderText (Target)\n",
      "(b) Decoupled Encoder + Single Decoder\n",
      "Encoder\n",
      "(Speech)\n",
      "Speech (Source)Encoder\n",
      "(Text)\n",
      "Transcript (Source)Shared\n",
      "EncoderDecoderText (Target)\n",
      "(c) Two-stream Encoder + Single Decoder\n",
      "Figure 17: Architectures of speech-to-text translation models based on Transformers. In\n",
      "addition to the standard encoder-decoder architecture, we can explicitly model\n",
      "the acoustic and textual (semantic) information using two separate encoders,\n",
      "called the speech encoder and the text encoder. In the decoupled encoder ar-\n",
      "chitecture, the two encoders are stacked, that is, text encoding is a subsequent\n",
      "process after speech encoding. In the two-stream encoder architecture, the two\n",
      "encoders work in parallel, and their outputs are merged using an additional en-\n",
      "coder, called the shared encoder. The dotted line indicates the potential for\n",
      "interaction between the two encoders. For example, we could define a loss func-\n",
      "tion to minimize the difference between their outputs, thereby guiding the model\n",
      "towards more aligned representations.\n",
      "86\n",
      "Page 87:\n",
      "Introduction to Transformers\n",
      "Image\n",
      "Flattened Image PatchesPatch Embedding + Positional Embedding\n",
      "CC = Extra Learnable\n",
      "[CLS] embeddingTransformer EncoderClassifier Is it a building?\n",
      "Figure 18: Illustration of Vision Transformer for image classification(Dosovitskiy et al.,\n",
      "2021). There are three steps. In the first step, the input image is segmented into\n",
      "patches, which are then flattened and mapped into embeddings. In the second\n",
      "step, a Transformer encoder is employed to process the sequence of embeddings,\n",
      "representing the image as a real-valued vector (e.g., the output of the encoder\n",
      "at the first position). In the last step, a classifier is built on top of this image\n",
      "representation.\n",
      "structure of Vision transformer to make this section concentrated and coherent, although\n",
      "there has been an extensive literature on Vision transformer and its variants. More detailed\n",
      "discussions of vision transformer can be found in recent surveys (Han et al., 2022; Liu et al.,\n",
      "2023b).\n",
      "The core idea behind Vision Transformer is to transform an image into a sequence of\n",
      "visual tokens, and input this sequence into a Transformer encoder to generate a representa-\n",
      "tion of the image. The Transformer encoder is standard, and so we will not discuss it here,\n",
      "given the introduction to Transformers we have presented so far in this chapter. Mapping\n",
      "a 2D image into a sequence of tokens needs some additional work. Suppose we have an\n",
      "image represented as an H×W×Cfeature map, where His the height of the image, W\n",
      "is the width of the image, and Cis the number of channels. The first step is to segment\n",
      "this image into a number of patches . Suppose all patches are squares of side length P.\n",
      "Then the resulting patches can be represented by feature maps of shape P×P×C. By\n",
      "ordering these patches in some way, we obtain a sequence ofHW\n",
      "P2patches, with each patch\n",
      "being treated as a “token”.\n",
      "Given this patch sequence, the subsequent steps are straightforward. For the patch at\n",
      "each position, we obtain a d-dimensional embedding by a linear transformation of the input\n",
      "feature map. The input of the Transformer encoder is a sequence of d-dimensional vectors,\n",
      "each of which is the sum of the corresponding patch and positional embeddings. Figure 18\n",
      "illustrates the patching and embedding steps in Vision Transformer.\n",
      "87\n",
      "Page 88:\n",
      "Xiao and Zhu\n",
      "Once we have a sequence of vectors for representing the image, we can employ the\n",
      "Transformer encoder to encode the sequence. The encoding process is exactly the same as\n",
      "that in text encoding as discussed in Section 6.2. For classification problems, we need only\n",
      "a single representation of the input. It is convenient to take the output of the encoder at\n",
      "position 0 (denoted by hL\n",
      "0) and feed it into a classifier. Given that the first token [CLS]\n",
      "serves as a special token that would be attended to by all other tokens, hL\n",
      "0provides an\n",
      "unbiased representation of the entire sequence.\n",
      "Typically, a standard way to train Vision Transformer is to minimize some loss on labeled\n",
      "data, such as ImageNet. More recently, inspired by self-supervised learning in BERT-like\n",
      "models, there have been successful attempts to train Transformer-based image encoders\n",
      "on large-scale unlabeled data (Bao et al., 2021; Caron et al., 2021; He et al., 2022). Note\n",
      "that one of the most significant contributions of Vision Transformer is that it unifies the\n",
      "representation models for different modalities. This suggests that if an object, whether an\n",
      "image or text, is represented as a sequence of embeddings, it can be easily modeled using\n",
      "the Transformer architecture.\n",
      "6.5 Multimodal Models\n",
      "The above discussion of Vision Transformer offers the possibility of unifying the represen-\n",
      "tations from multiple modalities using the same Transformer architecture. In fact, many\n",
      "recent multimodal systems draw inspiration largely from Transformers (Xu et al., 2023c).\n",
      "Such systems convert objects from different modalities into vector sequences and feed these\n",
      "vectors into a single Transformer model. The output is a fused representation of all inputs,\n",
      "which can then be used in downstream systems.\n",
      "As a simple example, consider the task of encoding a pair consisting of text and its\n",
      "corresponding image. First, we represent both the text and the image as sequences of\n",
      "embeddings that have the same dimensionality. This is a common step in sequence modeling,\n",
      "which we have confronted many times so far. We can do this by using either a simple\n",
      "embedding model (e.g., a word or patch embedding model) or a well-trained sequence\n",
      "model (e.g., a vision model). Then, these two sequences are concatenated into a long\n",
      "sequence involving both textual and visual embeddings. The follow-on step is standard: a\n",
      "Transformer encoder takes the concatenated sequence of embeddings as input and produces\n",
      "representations of the text and image as output. Note that concatenating textual and\n",
      "visual sequences is one of the simplest methods for vision-text modeling. There are several\n",
      "alternative ways to merge information from different modalities, for example, we can feed\n",
      "visual representations into the attention layers of a text encoder or decoder (Alayrac et al.,\n",
      "2022; Li et al., 2022d).\n",
      "The above multimodal encoder can be used in both encoder-only and encoder-decoder\n",
      "systems. For encoder-only systems, consider an example where, given an image and a\n",
      "description of it, we predict the class of the image using a classifier built on top of the\n",
      "encoder (Kim et al., 2021). For encoder-decoder systems, we pair the encoder with a\n",
      "decoder, as in sequence-to-sequence modeling (Cho et al., 2021). For example, we might\n",
      "employ a Transformer decoder to generate text based on the output of the encoder. A\n",
      "common application of this architecture is visual question answering (VQA ), where\n",
      "an image and a question about the image are provided, and the system is tasked with\n",
      "88\n",
      "Page 89:\n",
      "Introduction to Transformers\n",
      "generating an answer (Antol et al., 2015). The architectures of these models are illustrated\n",
      "in Figure 19 (a-b).\n",
      "More recently, NLP has seen new advances by using large language models to deal with\n",
      "both textual and other forms of data, such as images, videos, and audio, leading to new\n",
      "breakthroughs in multimodal processing (Liu et al., 2023a; Yin et al., 2023). By representing\n",
      "all inputs as a sequence of token embeddings, the problem will be simple: we predict the\n",
      "next token given its context. This can be done by using decoder-only systems, as shown in\n",
      "Figure 19 (c).\n",
      "7. Summary\n",
      "Transformer models have achieved widespread use over the past few years since the concept\n",
      "ofTransformer was proposed by Vaswani et al. (2017). This has accelerated the develop-\n",
      "ment of these models, leading to a great variety of new algorithms, systems and concepts. A\n",
      "thorough discussion of Transformers requires a broad scope, and so it is impossible to cover\n",
      "every problem and to provide a complete list of the corresponding references. While this\n",
      "chapter has presented a detailed introduction to Transformers, there are still topics that\n",
      "we did not mention, such as the theoretical aspects of these models. Figure 20 shows an\n",
      "overview of Transformer models, where we attempt to give a big picture. Note that these\n",
      "models and related techniques can be classified in many different ways, and we just show\n",
      "one of them. To summarize, we would like to highlight the following points.\n",
      "•Foundations of Transformers . Although the impact of Transformers has been rev-\n",
      "olutionary, they are not completely ”new” models. From a deep learning perspective,\n",
      "Transformers are composed of common building blocks, including word and positional\n",
      "embeddings (Bengio et al., 2003; Gehring et al., 2017; Mikolov et al., 2013), attention\n",
      "mechanisms (Bahdanau et al., 2014; Luong et al., 2015), residual connections (He\n",
      "et al., 2016b), layer-normalization (Ba et al., 2016), and so on. Many of these compo-\n",
      "nents were presented in earlier systems, for example, similar ideas with QKV attention\n",
      "can be found in memory networks (Sukhbaatar et al., 2015) and hierarchical attention\n",
      "networks (Yang et al., 2016). Transformers offer a novel approach to integrating these\n",
      "components, resulting in a unique architecture. For example, in Transformers, the\n",
      "combination of multi-head attention and dot-product QKV attention, along with the\n",
      "incorporation of layer-normalization and residual connections, gives rise to a distinc-\n",
      "tive neural network block, specifically a self-attention sub-layer. This design has since\n",
      "become a de facto standard in many follow-on sequence modeling systems.\n",
      "•Attention Models . The success of Transformers on NLP tasks has largely been at-\n",
      "tributed to the use of multi-head self-attention for sequence modeling. This has led to\n",
      "a surge of interest in enhancing the attention mechanisms within Transformers. While\n",
      "it is impossible to detail every attention model, there are several notable research di-\n",
      "rections. One prominent direction involves modifying the forms of QKV attention\n",
      "and multi-head attention for improved performance. The scope of this direction is\n",
      "vast, as there are numerous aspects to consider when enhancing Transformers (Lin\n",
      "et al., 2022a). For example, one may add new components to self-attention sub-layers\n",
      "to adapt them to specific tasks, resulting in various Transformer variants. A second\n",
      "89\n",
      "Page 90:\n",
      "Xiao and Zhu\n",
      "⟨CLS⟩A cube with green color.Word and Positional EmbeddingsC\n",
      "Patch and Positional EmbeddingsTransformer EncoderClassifier Is it an animal?\n",
      "(a) Multi-modal Encoder + Classifier\n",
      "⟨CLS⟩A cube with green color.Word and Positional EmbeddingsC\n",
      "Patch and Positional EmbeddingsTransformer Encoder Decoder\n",
      "⟨SOS⟩一个绿色的立方体一个绿色的立方体。\n",
      "(b) Multi-modal Encoder + Text Decoder (Translation)\n",
      "⟨CLS⟩What’s the color of the cube?Word and Positional EmbeddingsC\n",
      "Patch and Positional EmbeddingsTransformer Decoder\n",
      "⟨SOS⟩The color isgreenThe color isgreen .\n",
      "(c) Multi-modal Decoder (Language Modeling)\n",
      "Figure 19: Vision-text models. Blue boxes represent word+position embeddings, and red\n",
      "boxes represent image patch+position embeddings.\n",
      "90\n",
      "Page 91:\n",
      "Introduction to Transformers\n",
      "Attention\n",
      "Scalability\n",
      "EfficiencySyntax -\n",
      "aware \n",
      "Attention\n",
      "Deep ModelsWide Models\n",
      "•Mixture of \n",
      "ExportsSparse Attention\n",
      "•Span -based / Local \n",
      "Attention\n",
      "•Chunked Attention\n",
      "•Strided Attention\n",
      "•Learning Attention Fields\n",
      "Recurrent and Memory \n",
      "Models\n",
      "•Cache -based Memory\n",
      "•Encoding Long -term \n",
      "Memory\n",
      "•Retrieval -based MethodsAlternatives\n",
      "•Convolutional Attention\n",
      "•Linear AttentionData EfficiencyAbsolute Positional Encoding\n",
      "Regularization\n",
      "•Layer Skipping / \n",
      "Stochastic LayersRotational Positional Encoding \n",
      "Theoretical Analysis\n",
      "•Linguistics\n",
      "•Machine Learning\n",
      "•Formal Systems\n",
      "Foundations of Transformers\n",
      "Transformer Sub -models\n",
      "•Word Encoding and Positional Encoding\n",
      "•Multi -head Self -attention\n",
      "•Feed -forward Networks\n",
      "•Layer Normalization and Residual ConnectionsArchitectures\n",
      "•Encoder Only\n",
      "•Decoder Only\n",
      "•Encoder -DecoderSequence Compression\n",
      "•Token Pruning\n",
      "•Progressive DownsamplingModel Compression\n",
      "•Knowledge Distillation\n",
      "•Structured PruningSearch\n",
      "Algorithms \n",
      "Quantization\n",
      "Early Stop\n",
      "Length \n",
      "ExtrapolationParameter and \n",
      "Activation Sharing\n",
      "•Layer Fusion\n",
      "•Multi -scale Models\n",
      "•Multi -branch Models\n",
      "•Numerical Method -inspired Models\n",
      "Architecture \n",
      "Improvement\n",
      "Training\n",
      " Positional Encoding\n",
      " Inference\n",
      "IO-aware AttentionCaching\n",
      "Efficiency Foundations of Transformers Training Attention Inference\n",
      "Architecture Improvement Positional Embedding Scalability Applications\n",
      "Applications\n",
      "Self-supervised Transformers in NLPSupervised Transformers in NLP\n",
      "Natural Language \n",
      "Generation:\n",
      "GPT Series, T5, BART , \n",
      "MASS, PaLM , LaMDA , \n",
      "Megatron -Turing NLG, \n",
      "BLOOM, LLaMA ,etc.Natural Language \n",
      "Understanding:\n",
      "BERT , RoBERTa , \n",
      "ALBERT , SpanBERT , \n",
      "ERNIE, XLM, XLNet ,\n",
      "etc.Neural Machine Translation, \n",
      "Summarization, Sentiment Analysis, \n",
      "Question Answering, Named Entity \n",
      "Recognition, Syntactic Analysis, etc. Vision\n",
      "Speech\n",
      "Multi -modal\n",
      "Time Series AnalysisBioinformatics\n",
      "Recommendation SystemsRoboticsViT, Swin , MAE, BEiT, DETR, \n",
      "iGPT ,etc.\n",
      "Wav2vec 2.0, Whisper, \n",
      "CLIP , ViLBERT , VisualBERT , VL-\n",
      "BERT, UNITER, LXMERT, ViLT, \n",
      "VLT5, Data2vec, etc.HuBERT , Conformer, etc.\n",
      "Figure 20: An overview of Transformers.\n",
      "91\n",
      "Page 92:\n",
      "Xiao and Zhu\n",
      "direction is to incorporate prior knowledge into the design of attention models. This\n",
      "makes sense, because much of the emphasis in traditional NLP has been on using\n",
      "linguistic insights to guide system design, and we generally want NLP systems to be\n",
      "linguistically explainable. For example, many Transformer-based systems take syn-\n",
      "tactic parses as input in various forms and make use of syntax in sequence modeling.\n",
      "A third direction is to develop efficient attention models (Tay et al., 2020b). Self-\n",
      "attention has long been criticized for its quadratic time complexity and dependency\n",
      "on all previous tokens for each new token. In response, many researchers have focused\n",
      "on simplifying the structure of self-attention, or on approximating it using sparse or\n",
      "recurrent models. This concern for efficiency also motivates the development of al-\n",
      "ternatives to self-attention, such as attention models with linear time complexity. In\n",
      "addition to exploring stronger and more efficient attention models, it is natural to\n",
      "examine what knowledge is learned by such models. Interestingly, researchers have\n",
      "found that the underlying structure of languages can be learned by multi-head self-\n",
      "attention models, although these models are not trained to represent such knowledge\n",
      "(Manning et al., 2020).\n",
      "•Word and Positional Embeddings . Transformers represent each input word as a\n",
      "word embedding, along with its positional embedding. Learning these word embed-\n",
      "dings is not a specific problem for Transformers. We can either resort to well-trained\n",
      "word embeddings, such as the Word2Vec or GloVe embeddings, or treat them as\n",
      "learnable parameters of Transformers. A related issue is tokenization of the input\n",
      "sequences. In general, tokenization impacts the number of resulting tokens and the\n",
      "difficulty of learning the corresponding embeddings. In many applications, therefore,\n",
      "one needs to carefully choose a tokenization method. Furthermore, positional em-\n",
      "bedding plays an important role in Transformers, as the attention mechanisms are\n",
      "order-insensitive by design (Dufter et al., 2022). Although positional embedding is\n",
      "a general problem, much of the research is focused on improving Transformers, lead-\n",
      "ing to modifications to Transformer models (Huang et al., 2018; Shaw et al., 2018).\n",
      "Additionally, studies show that, when we deal with sequences that are much longer\n",
      "than those in training data, extrapolation can be achieved by replacing sinusoidal\n",
      "positional embeddings with rotary positional embeddings or simply scaling attention\n",
      "weights with a positional scalar (Press et al., 2021; Raffel et al., 2020; Su et al., 2021).\n",
      "•Training and Model Scaling . In the era of deep learning, powerful systems are\n",
      "typically obtained by using large neural networks. A simple approach to increasing\n",
      "the model capacity of Transformers is to stack more layers and/or enlarge the size\n",
      "of each representation. We can see many cases where deep and wide Transformer\n",
      "models consistently outperform small models. However, challenges arise when we at-\n",
      "tempt to train extremely large Transformer models, especially when gradient descent\n",
      "is applied over vast amounts of data, demanding substantial computational resources.\n",
      "An engineering solution is to distribute the training across a cluster of computers\n",
      "(Chowdhery et al., 2022; Lepikhin et al., 2021). While distributed training is a very\n",
      "general method and is not restricted to Transformers, it indeed influences the design\n",
      "of model architectures, for example, sparse expert models can ease the training with\n",
      "distributed parameters, serving as the foundation for many expansive Transformer-\n",
      "92\n",
      "Page 93:\n",
      "Introduction to Transformers\n",
      "based systems. Scaling up the training of Transformers allows us to study the scaling\n",
      "law of large neural networks: how model performance relates to model size, training\n",
      "data size, and training cost (Hestness et al., 2017; Kaplan et al., 2020). This is some-\n",
      "times accompanied by an interesting behavior, known as emergence (Wei et al., 2022).\n",
      "In recent NLP research, the acquisition of emergent abilities has been considered one\n",
      "of the prerequisites for developing strong language models.\n",
      "•Efficient Models . There are different goals for efficiency. For example, one may\n",
      "wish a system to be memory efficient when the problem is memory bound, or one may\n",
      "wish it to be speed efficient when latency is an important consideration. In general,\n",
      "we need to seek a balance between these goals, resulting in different efficiency opti-\n",
      "mizations. In the context of Transformers, many of these optimizations are achieved\n",
      "by modifying the attention models, as mentioned above. For example, several vari-\n",
      "ants of the self-attention models are proposed to reduce the memory footprint when\n",
      "processing long sequences (Tay et al., 2020b). Similarly, other variants aim to re-\n",
      "duce computation and thus give lower latency. Furthermore, being a type of neural\n",
      "network, Transformers can be optimized in ways independent of model architectures.\n",
      "Typical methods include but are not limited to conditional computation, knowledge\n",
      "distillation, structured pruning, and sequence compression. Efficiency optimizations\n",
      "can also be considered from the perspective of computer architecture (Kim et al.,\n",
      "2023). For example, when applying Transformers to sequence-to-sequence problems,\n",
      "the encoding and decoding processes are generally compute-intensive and IO-intensive,\n",
      "respectively. Therefore, we can employ different optimization methods for different\n",
      "components of Transformers.\n",
      "•Inference . The inference problem is commonly discussed in sequence generation. In\n",
      "NLP, we often need to find the “best” hypothesis in a space involving sequences of\n",
      "tens or even hundreds of tokens over a vocabulary. Considering this an instance of\n",
      "the search problem in artificial intelligence, many algorithms can be applied, such as\n",
      "breadth-first search, depth-first search and A* search. In many practical applications\n",
      "of NLP, the efficiency of the search systems is an important consideration. As a result,\n",
      "optimized search algorithms are required. Most of these algorithms have been explored\n",
      "in machine translation and ASR, and are directly applicable to neural text generation\n",
      "models like Transformer. There are also optimizations of conventional decoding meth-\n",
      "ods tailored to Transformers (Leviathan et al., 2023). Moreover, the above-mentioned\n",
      "efficient approaches, such as the efficient attention models, are also in widespread use,\n",
      "with many successful examples in deploying neural machine translation systems and\n",
      "large language models (Dao et al., 2023; Heafield et al., 2021).\n",
      "•Applications . Applications of Transformers cover a wide variety of NLP problems.\n",
      "During the development of Transformers, they were at first used to build supervised\n",
      "models that perform particular tasks. Later, a greater success was achieved by using\n",
      "them as backbone networks for large scale self-supervised learning of foundation mod-\n",
      "els (Bommasani et al., 2021). This markedly changed the paradigm in NLP. We need\n",
      "only pre-train a model to obtain general knowledge of languages on huge amounts\n",
      "of text. Then, we adapt this model to downstream tasks using methods with little\n",
      "effort, such as fine-tuning or prompting. Over the past few years, we have also seen an\n",
      "93\n",
      "Page 94:\n",
      "Xiao and Zhu\n",
      "explosion of applications for Transformers in fields other than NLP, such as computer\n",
      "vision, speech processing, and bioinformatics. The idea behind these applications is\n",
      "that we can represent any input data as a sequence of tokens and directly employ\n",
      "Transformers to model this sequence. This approach extends Transformers to general\n",
      "representation models across different modalities, making it easier to use Transformers\n",
      "for handling multi-modal data.\n",
      "•Large Language Models as Foundation Models . Transformers form the ba-\n",
      "sis of recent large language models, such as the GPT series, which show surprising\n",
      "breakthroughs in NLP, and even in artificial general intelligence (AGI ) (Bubeck\n",
      "et al., 2023; Yang et al., 2023a). Much of the research in large language models is\n",
      "more or less related to Transformers. For example, as discussed in Section 6.1, the\n",
      "problem of training these language models is the same as that of training Transformer\n",
      "decoders. And the modifications to Transformer decoders can be directly applied to\n",
      "large language models. On the other hand, the rapid development of large language\n",
      "models has also driven further improvements in various techniques for Transformers,\n",
      "such as efficient and low-cost adaptation of large Transformers to different tasks.\n",
      "•Theoretical Analysis . Although Transformers have shown strong empirical results\n",
      "in various fields, their theoretical aspects have received relatively less attention com-\n",
      "pared to the extensive research on model improvement and engineering. This is not\n",
      "a specific problem for Transformers, but a common problem for the NLP and ma-\n",
      "chine learning communities. In response, researchers have made attempts to analyze\n",
      "Transformers more deeply. One way is to view Transformers as deep neural net-\n",
      "works and interpret them via mathematical tools. For example, the residual networks\n",
      "in Transformers are mathematically equivalent to the Euler solvers for ODEs. This\n",
      "equivalence suggests that we can leverage insights from numerical ODE methods to\n",
      "inform model design. Another promising avenue of research aims to develop a theoret-\n",
      "ical understanding of the self-attention mechanism, which distinguishes Transformers\n",
      "from other deep learning models. For example, there have been studies on interpret-\n",
      "ing self-attention and Transformers from machine learning perspectives, such as data\n",
      "compression (Yu et al., 2023), optimization (Li et al., 2022c), and function approxima-\n",
      "tion (Yun et al., 2019). Moreover, Transformers can also be related to formal systems,\n",
      "including Turing machines (P´ erez et al., 2018), counter machines (Bhattamishra et al.,\n",
      "2020), regular and context-free languages (Hahn, 2020), Boolean circuits (Hao et al.,\n",
      "2022; Merrill et al., 2022), programming languages (Weiss et al., 2021), first-order\n",
      "logic (Chiang et al., 2023), and so on. These provide tools to study the expressivity\n",
      "of Transformers. It is, however, worth noting that, while we can understand Trans-\n",
      "formers in several different ways, there are no general theories to explain the nature\n",
      "of these models. Perhaps this is a challenge for the field of machine learning, and\n",
      "many researchers are working on this issue. But it is indeed an important issue, as\n",
      "the development of the theories behind complex neural networks like Transformers\n",
      "can help develop systems with explainable and predictable behaviors.\n",
      "94\n",
      "Page 95:\n",
      "Introduction to Transformers\n",
      "Acknowledgements\n",
      "We would like to thank Yongyu Mu, Chenglong Wang, Bei Li, Weiqiao Shan, Yuchun Fan,\n",
      "Kaiyan Chang, Tong Zheng, and Huiwen Bao for their suggestions on improving the early\n",
      "version of this work.\n",
      "95\n",
      "Page 96:\n",
      "Xiao and Zhu\n",
      "A. Sinusoidal Positional Encoding\n",
      "In Transformers, positions are represented as vectors. Although vectorizing the represen-\n",
      "tations of positions sounds complicated, a simple idea is to use a carrying system which\n",
      "describes how a natural number is expressed by a polynomial with respect to a base (Kernes,\n",
      "2021). For example, ican be written as\n",
      "i=kmaxX\n",
      "k=0a(i, k)bk(202)\n",
      "where a(i, k) is the k-th digit, kmax+ 1 is the maximum number of digits, and bis the base\n",
      "of the system. The carrying occurs when a(i, k) reaches b: we increase a(i, k+ 1) by 1 and\n",
      "roll back a(i, k) to 0. In this way we can change a(i, k) with a period of bk, that is, a(i,0)\n",
      "changes with a period of b0,a(i,1) changes with a period of b1,a(i,2) changes with a period\n",
      "ofb2, and so on.\n",
      "Using this system, ican be represented as a vector\n",
      "PE(i) =\u0002\n",
      "a(i,0)a(i,1)... a (i, kmax)\u0003\n",
      "(203)\n",
      "For example, when b= 2, PE(11) =\u0002\n",
      "1 1 0 1\u0003\n",
      ". However, in Eq. (203), PE( i) is still\n",
      "a discrete function. We may want a continuous vector representation that can describe\n",
      "intermediate states between discrete events. Considering a(i, k) as a periodic function, a\n",
      "common choice is the sine function. Thus a(i, k) can be re-defined, as follows\n",
      "a(i, k) = sin( i·ωk) (204)\n",
      "This function has an amplitude of 1 and a period of2π\n",
      "ωk. Using an analogous form of periods\n",
      "to that used in Eq. (202), we define ωkas\n",
      "ωk=1\n",
      "(bmodel)k/dmodel(205)\n",
      "where bmodel >0 and dmodel >0 are hyper-parameters of the model. Obviously, we have\n",
      "2π\n",
      "ω0<2π\n",
      "ω1< ... <2π\n",
      "ωkmax.\n",
      "Similarly, we can define a(i, k) via the cosine function\n",
      "a(i, k) = cos( i·ωk) (206)\n",
      "Taking both Eqs. (204) and (206), we create a new representation of i, as follows\n",
      "PE(i) =\u0002\n",
      "sin(i·ω0) cos( i·ω0)...sin(i·ωkmax) cos( i·ωkmax)\u0003\n",
      "(207)\n",
      "Vaswani et al. (2017) instantiated the above form by setting bmodel = 10 ,000. Let\n",
      "PE(i, k) be the k-th dimension of PE( i). Vaswani et al. (2017)’s version of positional\n",
      "encoding is written as\n",
      "PE(i,2k) = sin( i·1\n",
      "100002k/dmodel) (208)\n",
      "PE(i,2k+ 1) = cos( i·1\n",
      "100002k/dmodel) (209)\n",
      "96\n",
      "Page 97:\n",
      "Introduction to Transformers\n",
      "Figure 21: A heat map of the positional embedding model of Eqs. (208) and (209)\n",
      "(bmodel = 10,000 and dmodel = 512). Consider a position i(i.e., the i-th row),\n",
      "then move another position jfrom iupwards or downwards. Intuitively, when\n",
      "iandjare closer, the corresponding row vectors are more similar. By contrast,\n",
      "when jmoves away from i, the similarity is not that obvious. This property helps\n",
      "explain the idea behind the positional embedding model: the “distance” between\n",
      "two positions is implicitly modeled by comparing their multi-dimensional repre-\n",
      "sentations.\n",
      "Choosing bmodel = 10,000 is empirical. One can adjust it for specific tasks. Figure 21\n",
      "plots the positional encoding for different positions. We see that, when kbecomes larger,\n",
      "the change of the color follows a larger period.\n",
      "Note that Eqs. (208) and (209) have a useful property that PE( i+µ) can be easily\n",
      "expressed by a linear function of PE( i) for a given offset µ28\n",
      "PE(i+µ,2k) = PE( i,2k)·PE(µ,2k+ 1) +\n",
      "PE(i,2k+ 1)·PE(µ,2k) (212)\n",
      "PE(i+µ,2k+ 1) = PE( i,2k+ 1)·PE(µ,2k+ 1) +\n",
      "PE(i,2k)·PE(µ,2k) (213)\n",
      "The resulting benefit is that the encoding can somewhat model relative positions. That is,\n",
      "the state at position i+µcan be described by starting with iand then appending it with\n",
      "the offset µ.\n",
      "28. One can derive these by taking\n",
      "sin(α+β) = sin( α)·cos(β) + cos( α)·sin(β) (210)\n",
      "cos(α+β) = cos( α)·cos(β)−sin(α)·sin(β) (211)\n",
      "97\n",
      "Page 98:\n",
      "Xiao and Zhu\n",
      "When applying the sinusoidal positional encoding, one way is to concatenate xiand\n",
      "PE(i). In Vaswani et al. (2017)’s work, they instead assume PE( i) to be a vector of the\n",
      "same size as xi(i.e.,|PE(i)|=|xi|=de), and add PE( i) toxi, like this\n",
      "xpi=xi+ PE( i) (214)\n",
      "This sinusoidal addictive model has been the basis of many positional encoding approaches\n",
      "(Dehghani et al., 2018; Likhomanenko et al., 2021; Su et al., 2021).\n",
      "98\n",
      "Page 99:\n",
      "Introduction to Transformers\n",
      "References\n",
      "Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. Fine-grained\n",
      "analysis of sentence embeddings using auxiliary prediction tasks. In Proceedings of Inter-\n",
      "national Conference on Learning Representations , 2016.\n",
      "Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip\n",
      "Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. Etc: Encoding long\n",
      "and structured inputs in transformers. In Proceedings of the 2020 Conference on Empirical\n",
      "Methods in Natural Language Processing (EMNLP) , pages 268–284, 2020.\n",
      "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,\n",
      "Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a\n",
      "visual language model for few-shot learning. Advances in Neural Information Processing\n",
      "Systems , 35:23716–23736, 2022.\n",
      "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,\n",
      "C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings\n",
      "of the IEEE international conference on computer vision , pages 2425–2433, 2015.\n",
      "Karl J ˚Astr¨ om and Bj¨ orn Wittenmark. Computer-controlled systems: theory and design .\n",
      "Courier Corporation, 2013.\n",
      "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv\n",
      "preprint arXiv:1607.06450 , 2016.\n",
      "Thomas Bachlechner, Bodhisattwa Prasad Majumder, Henry Mao, Gary Cottrell, and Ju-\n",
      "lian McAuley. Rezero is all you need: Fast convergence at large depth. In Proceedings of\n",
      "Uncertainty in Artificial Intelligence , pages 1352–1361. PMLR, 2021.\n",
      "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by\n",
      "jointly learning to align and translate. arXiv preprint arXiv:1409.0473 , 2014.\n",
      "Jiangang Bai, Yujing Wang, Yiren Chen, Yaming Yang, Jing Bai, Jing Yu, and Yunhai\n",
      "Tong. Syntax-bert: Improving pre-trained transformers with syntax trees. In Proceedings\n",
      "of the 16th Conference of the European Chapter of the Association for Computational\n",
      "Linguistics: Main Volume , pages 3011–3020, 2021.\n",
      "Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image\n",
      "transformers. In Proceedings of International Conference on Learning Representations ,\n",
      "2021.\n",
      "Paul Barham, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, Steven Hand, Daniel\n",
      "Hurt, Michael Isard, Hyeontaek Lim, Ruoming Pang, Sudip Roy, et al. Pathways: Asyn-\n",
      "chronous distributed dataflow for ml. In Proceedings of Machine Learning and Systems ,\n",
      "volume 4, pages 430–449, 2022.\n",
      "Yonatan Belinkov. Probing classifiers: Promises, shortcomings, and advances. Computa-\n",
      "tional Linguistics , 48(1):207–219, 2022.\n",
      "99\n",
      "Page 100:\n",
      "Xiao and Zhu\n",
      "Irwan Bello. Lambdanetworks: Modeling long-range interactions without attention. In\n",
      "Proceedings of International Conference on Learning Representations , 2020.\n",
      "Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document trans-\n",
      "former. arXiv:2004.05150 , 2020.\n",
      "Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional com-\n",
      "putation in neural networks for faster models. arXiv preprint arXiv:1511.06297 , 2015.\n",
      "Yoshua Bengio, R éjean Ducharme, Pascal Vincent, and Christian Jauvin. A neural prob-\n",
      "abilistic language model. Journal of Machine Learning Research , 3:11371155, 2003.\n",
      "Yoshua Bengio, Nicholas L´ eonard, and Aaron Courville. Estimating or propagating\n",
      "gradients through stochastic neurons for conditional computation. arXiv preprint\n",
      "arXiv:1308.3432 , 2013.\n",
      "Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal. On the ability and limitations of\n",
      "transformers to recognize formal languages. In Proceedings of the 2020 Conference on\n",
      "Empirical Methods in Natural Language Processing (EMNLP) , pages 7096–7116, 2020.\n",
      "Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney\n",
      "von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik\n",
      "Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S.\n",
      "Chen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris Donahue, Moussa Doum-\n",
      "bouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei,\n",
      "Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, Shelby\n",
      "Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E.\n",
      "Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky,\n",
      "Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, O. Khattab,\n",
      "Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar,\n",
      "Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li,\n",
      "Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir P. Mirchandani, Eric\n",
      "Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Benjamin\n",
      "Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, J. F. Nyarko, Giray Ogut,\n",
      "Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christo-\n",
      "pher Potts, Aditi Raghunathan, Robert Reich, Hongyu Ren, Frieda Rong, Yusuf H.\n",
      "Roohani, Camilo Ruiz, Jack Ryan, Christopher R’e, Dorsa Sadigh, Shiori Sagawa, Ke-\n",
      "shav Santhanam, Andy Shih, Krishna Parasuram Srinivasan, Alex Tamkin, Rohan Taori,\n",
      "Armin W. Thomas, Florian Tram` er, Rose E. Wang, William Wang, Bohan Wu, Jiajun\n",
      "Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei A. Zaharia,\n",
      "Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou,\n",
      "and Percy Liang. On the opportunities and risks of foundation models. ArXiv , 2021.\n",
      "Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcom-\n",
      "ing the challenges of efficient transformer quantization. In Proceedings of the 2021 Con-\n",
      "ference on Empirical Methods in Natural Language Processing , pages 7947–7969, 2021.\n",
      "100\n",
      "Page 101:\n",
      "Introduction to Transformers\n",
      "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\n",
      "Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\n",
      "guage models are few-shot learners. Advances in neural information processing systems ,\n",
      "33:1877–1901, 2020.\n",
      "S´ ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz,\n",
      "Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial\n",
      "general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 ,\n",
      "2023.\n",
      "Maxime Burchi and Valentin Vielzeuf. Efficient conformer: Progressive downsampling and\n",
      "grouped attention for automatic speech recognition. In Proceedings of 2021 IEEE Au-\n",
      "tomatic Speech Recognition and Understanding Workshop (ASRU) , pages 8–15. IEEE,\n",
      "2021.\n",
      "Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´ e J´ egou, Julien Mairal, Piotr Bojanowski,\n",
      "and Armand Joulin. Emerging properties in self-supervised vision transformers. In Pro-\n",
      "ceedings of the IEEE/CVF international conference on computer vision , pages 9650–9660,\n",
      "2021.\n",
      "Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah\n",
      "Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, et al. Universal sentence\n",
      "encoder. arXiv preprint arXiv:1803.11175 , 2018.\n",
      "Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita, and Tiejun Zhao. Syntax-directed\n",
      "attention for neural machine translation. In Proceedings of the AAAI conference on\n",
      "artificial intelligence , 2018a.\n",
      "Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George\n",
      "Foster, Llion Jones, Mike Schuster, Noam Shazeer, Niki Parmar, Ashish Vaswani, Jakob\n",
      "Uszkoreit, Lukasz Kaiser, Zhifeng Chen, Yonghui Wu, and Macduff Hughes. The best of\n",
      "both worlds: Combining recent advances in neural machine translation. In Proceedings\n",
      "of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:\n",
      "Long Papers) , pages 76–86, 2018b.\n",
      "Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural\n",
      "ordinary differential equations. Advances in neural information processing systems , 31,\n",
      "2018c.\n",
      "Tianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via\n",
      "knowledge transfer. arXiv preprint arXiv:1511.05641 , 2015.\n",
      "David Chiang, Peter Cholak, and Anand Pillay. Tighter bounds on the expressivity of\n",
      "transformer encoders. arXiv preprint arXiv:2301.10743 , 2023.\n",
      "Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences\n",
      "with sparse transformers. arXiv preprint arXiv:1904.10509 , 2019.\n",
      "101\n",
      "Page 102:\n",
      "Xiao and Zhu\n",
      "Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via\n",
      "text generation. In International Conference on Machine Learning , pages 1931–1942.\n",
      "PMLR, 2021.\n",
      "Do Kook Choe and Eugene Charniak. Parsing as language modeling. In Proceedings of the\n",
      "2016 Conference on Empirical Methods in Natural Language Processing , pages 2331–2336,\n",
      "2016.\n",
      "Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, An-\n",
      "dreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz\n",
      "Kaiser, et al. Rethinking attention with performers. In Proceedings of International Con-\n",
      "ference on Learning Representations , 2020.\n",
      "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\n",
      "Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann,\n",
      "et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 ,\n",
      "2022.\n",
      "Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. What does\n",
      "bert look at? an analysis of bert ’s attention. In Proceedings of the 2019 ACL Workshop\n",
      "BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pages 276–286, 2019.\n",
      "Alexis Conneau, Germ´ an Kruszewski, Guillaume Lample, Lo¨ ıc Barrault, and Marco Baroni.\n",
      "What you can cram into a single vector: Probing sentence embeddings for linguistic prop-\n",
      "erties. In Proceedings of the 56th Annual Meeting of the Association for Computational\n",
      "Linguistics (Volume 1: Long Papers) , pages 2126–2136, 2018.\n",
      "Anna Currey and Kenneth Heafield. Multi-source syntactic neural machine translation. In\n",
      "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Process-\n",
      "ing, pages 2961–2966, 2018.\n",
      "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhut-\n",
      "dinov. Transformer-xl: Attentive language models beyond a fixed-length context. In\n",
      "Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-\n",
      "tics, pages 2978–2988, 2019.\n",
      "Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R´ e. Flashattention: Fast\n",
      "and memory-efficient exact attention with io-awareness. Advances in Neural Information\n",
      "Processing Systems , 35:16344–16359, 2022.\n",
      "Tri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov. Flash-decoding for long-\n",
      "context inference. https://pytorch.org/blog/flash-decoding/ , 2023. Retrieved\n",
      "2023-10-23.\n",
      "Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and  Lukasz Kaiser.\n",
      "Universal transformers. arXiv preprint arXiv:1807.03819 , 2018.\n",
      "Luciano Del Corro, Allie Del Giorno, Sahaj Agarwal, Bin Yu, Ahmed Awadallah, and\n",
      "Subhabrata Mukherjee. Skipdecode: Autoregressive skip decoding with batching and\n",
      "caching for efficient llm inference. arXiv preprint arXiv:2307.02628 , 2023.\n",
      "102\n",
      "Page 103:\n",
      "Introduction to Transformers\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training\n",
      "of deep bidirectional transformers for language understanding. In Proceedings of the\n",
      "2019 Conference of the North American Chapter of the Association for Computational\n",
      "Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages\n",
      "4171–4186, 2019.\n",
      "Mattia Antonino Di Gangi, Matteo Negri, Roldano Cattoni, Roberto Dessi, and Marco\n",
      "Turchi. Enhancing transformer for end-to-end speech-to-text translation. In Proceedings\n",
      "of Machine Translation Summit XVII: Research Track , pages 21–31, 2019.\n",
      "Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin,\n",
      "Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation\n",
      "via transformers. Advances in Neural Information Processing Systems , 34:19822–19835,\n",
      "2021.\n",
      "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\n",
      "Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain\n",
      "Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at\n",
      "scale. In Proceedings of ICLR 2021 , 2021.\n",
      "Philipp Dufter, Martin Schmitt, and Hinrich Sch¨ utze. Position information in transformers:\n",
      "An overview. Computational Linguistics , 48(3):733–763, 2022.\n",
      "Weinan Ee. A proposal on machine learning via dynamical systems. Communications in\n",
      "Mathematics and Statistics , 5:1–11, 02 2017.\n",
      "Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli. Depth-adaptive transformer.\n",
      "InProceedings of International Conference on Learning Representations , 2020.\n",
      "Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A\n",
      "survey. The Journal of Machine Learning Research , 20(1):1997–2017, 2019.\n",
      "Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Ma-\n",
      "lik, and Christoph Feichtenhofer. Multiscale vision transformers. In Proceedings of the\n",
      "IEEE/CVF International Conference on Computer Vision , pages 6824–6835, 2021.\n",
      "Yang Fan, Shufang Xie, Yingce Xia, Lijun Wu, Tao Qin, Xiang-Yang Li, and Tie-Yan Liu.\n",
      "Multi-branch attentive transformer. arXiv preprint arXiv:2006.10270 , 2020.\n",
      "William Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep\n",
      "learning. arXiv preprint arXiv:2209.01667 , 2022a.\n",
      "William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion\n",
      "parameter models with simple and efficient sparsity. The Journal of Machine Learning\n",
      "Research , 23(1):5232–5270, 2022b.\n",
      "Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher\n",
      "Re. Hungry hungry hippos: Towards language modeling with state space models. In\n",
      "Proceedings of The Eleventh International Conference on Learning Representations , 2022.\n",
      "103\n",
      "Page 104:\n",
      "Xiao and Zhu\n",
      "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convo-\n",
      "lutional sequence to sequence learning. In International conference on machine learning ,\n",
      "pages 1243–1252. PMLR, 2017.\n",
      "Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt\n",
      "Keutzer. A survey of quantization methods for efficient neural network inference. In\n",
      "Low-Power Computer Vision , pages 291–326. Chapman and Hall/CRC, 2022.\n",
      "Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward\n",
      "neural networks. In Proceedings of the thirteenth international conference on artificial\n",
      "intelligence and statistics , pages 249–256. JMLR Workshop and Conference Proceedings,\n",
      "2010.\n",
      "Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. The reversible residual\n",
      "network: Backpropagation without storing activations. Advances in neural information\n",
      "processing systems , 30, 2017.\n",
      "Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation:\n",
      "A survey. International Journal of Computer Vision , 129:1789–1819, 2021.\n",
      "Robert M. Gray. Quantization. IEEE transactions on information theory , 44(6):2325–2383,\n",
      "1998.\n",
      "Albert Gu, Karan Goel, and Christopher R´ e. Efficiently modeling long sequences with\n",
      "structured state spaces. In Proceedings of International Conference on Learning Repre-\n",
      "sentations , 2021.\n",
      "Albert Gu, Karan Goel, Ankit Gupta, and Christopher R´ e. On the parameterization and\n",
      "initialization of diagonal state space models. Advances in Neural Information Processing\n",
      "Systems , 35:35971–35983, 2022a.\n",
      "Albert Gu, Karan Goel, Khaled Saab, and Chris R´ e. Structured state spaces: Com-\n",
      "bining continuous-time, recurrent, and convolutional models. https://hazyresearch.\n",
      "stanford.edu/blog/2022-01-14-s4-3 , 2022b. Retrieved 2022-01-14.\n",
      "Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei\n",
      "Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. Conformer:\n",
      "Convolution-augmented transformer for speech recognition. Proceedings of Interspeech\n",
      "2020, pages 5036–5040, 2020.\n",
      "Qipeng Guo, Xipeng Qiu, Pengfei Liu, Xiangyang Xue, and Zheng Zhang. Multi-scale\n",
      "self-attention for text classification. In Proceedings of the AAAI Conference on Artificial\n",
      "Intelligence , volume 34, pages 7847–7854, 2020.\n",
      "Madan Gupta, Liang Jin, and Noriyasu Homma. Static and dynamic neural networks: from\n",
      "fundamentals to advanced theory . John Wiley & Sons, 2004.\n",
      "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval\n",
      "augmented language model pre-training. In Proceedings of International conference on\n",
      "machine learning , pages 3929–3938. PMLR, 2020.\n",
      "104\n",
      "Page 105:\n",
      "Introduction to Transformers\n",
      "Eldad Haber and Lars Ruthotto. Stable architectures for deep neural networks. Inverse\n",
      "problems , 34(1):014004, 2017.\n",
      "Michael Hahn. Theoretical limitations of self-attention in neural sequence models. Trans-\n",
      "actions of the Association for Computational Linguistics , 8:156–171, 2020.\n",
      "Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui\n",
      "Tang, An Xiao, Chunjing Xu, Yixing Xu, et al. A survey on vision transformer. IEEE\n",
      "transactions on pattern analysis and machine intelligence , 45(1):87–110, 2022.\n",
      "Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol\n",
      "Gulati, Ruoming Pang, and Yonghui Wu. Contextnet: Improving convolutional neural\n",
      "networks for automatic speech recognition with global context. In Proceedings of Inter-\n",
      "speech 2020 , pages 3610–3614, 2020.\n",
      "Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamic\n",
      "neural networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intel-\n",
      "ligence , 44(11):7436–7456, 2021.\n",
      "Jie Hao, Xing Wang, Shuming Shi, Jinfeng Zhang, and Zhaopeng Tu. Multi-granularity\n",
      "self-attention for neural machine translation. In Proceedings of the 2019 Conference\n",
      "on Empirical Methods in Natural Language Processing and the 9th International Joint\n",
      "Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 887–897, 2019.\n",
      "Yiding Hao, Dana Angluin, and Robert Frank. Formal language recognition by hard atten-\n",
      "tion transformers: Perspectives from circuit complexity. Transactions of the Association\n",
      "for Computational Linguistics , 10:800–810, 2022.\n",
      "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for\n",
      "image recognition. In Proceedings of the IEEE conference on computer vision and pattern\n",
      "recognition , pages 770–778, 2016a.\n",
      "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep\n",
      "residual networks. In Proceedings of ECCV 2016 , pages 630–645, 2016b.\n",
      "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll´ ar, and Ross Girshick. Masked\n",
      "autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference\n",
      "on Computer Vision and Pattern Recognition , pages 16000–16009, 2022.\n",
      "Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-\n",
      "enhanced bert with disentangled attention. In Proceedings of International Conference\n",
      "on Learning Representations , 2021.\n",
      "Kenneth Heafield, Qianqian Zhu, and Roman Grundkiewicz. Findings of the WMT 2021\n",
      "shared task on efficient translation. In Proceedings of the Sixth Conference on Machine\n",
      "Translation , pages 639–651, 2021.\n",
      "Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan\n",
      "Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling\n",
      "is predictable, empirically. arXiv preprint arXiv:1712.00409 , 2017.\n",
      "105\n",
      "Page 106:\n",
      "Xiao and Zhu\n",
      "John Hewitt and Percy Liang. Designing and interpreting probes with control tasks. In\n",
      "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\n",
      "and the 9th International Joint Conference on Natural Language Processing (EMNLP-\n",
      "IJCNLP) , pages 2733–2743, 2019.\n",
      "Felix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations\n",
      "of sentences from unlabelled data. In Proceedings of the 2016 Conference of the North\n",
      "American Chapter of the Association for Computational Linguistics: Human Language\n",
      "Technologies , pages 1367–1377, 2016.\n",
      "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network.\n",
      "arXiv preprint arXiv:1503.02531 , 2015.\n",
      "Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu. Dynabert:\n",
      "Dynamic bert with adaptive width and depth. Advances in Neural Information Processing\n",
      "Systems , 33:9782–9793, 2020.\n",
      "Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan,\n",
      "Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mo-\n",
      "bilenetv3. In Proceedings of the IEEE/CVF international conference on computer vision ,\n",
      "pages 1314–1324, 2019.\n",
      "Chi Hu, Chenglong Wang, Xiangnan Ma, Xia Meng, Yinqiao Li, Tong Xiao, Jingbo Zhu,\n",
      "and Changliang Li. Ranknas: Efficient neural architecture search by pairwise ranking.\n",
      "InProceedings of the 2021 Conference on Empirical Methods in Natural Language Pro-\n",
      "cessing , pages 2469–2480, 2021.\n",
      "Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne,\n",
      "Noam Shazeer, Andrew M Dai, Matthew D Hoffman, Monica Dinculescu, and Douglas\n",
      "Eck. Music transformer: Generating music with long-term structure. In Proceedings of\n",
      "International Conference on Learning Representations , 2018.\n",
      "Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks\n",
      "with stochastic depth. In Proceedings of the 14th European Conference , pages 646–661.\n",
      "Springer, 2016.\n",
      "Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely\n",
      "connected convolutional networks. In Proceedings of the IEEE conference on computer\n",
      "vision and pattern recognition , pages 4700–4708, 2017.\n",
      "Zhiheng Huang, Davis Liang, Peng Xu, and Bing Xiang. Improve transformer models with\n",
      "better relative position embeddings. In Findings of the Association for Computational\n",
      "Linguistics: EMNLP 2020 , pages 3327–3335, 2020.\n",
      "Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data move-\n",
      "ment is all you need: A case study on optimizing transformers. In Proceedings of Machine\n",
      "Learning and Systems , volume 3, pages 711–732, 2021.\n",
      "106\n",
      "Page 107:\n",
      "Introduction to Transformers\n",
      "Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard,\n",
      "Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks\n",
      "for efficient integer-arithmetic-only inference. In Proceedings of the IEEE conference on\n",
      "computer vision and pattern recognition , pages 2704–2713, 2018.\n",
      "Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu,\n",
      "David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al.\n",
      "Perceiver io: A general architecture for structured inputs & outputs. In Proceedings of\n",
      "International Conference on Learning Representations , 2021.\n",
      "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon\n",
      "Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural\n",
      "language models. arXiv preprint arXiv:2001.08361 , 2020.\n",
      "Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran¸ cois Fleuret. Transform-\n",
      "ers are rnns: Fast autoregressive transformers with linear attention. In International\n",
      "conference on machine learning , pages 5156–5165. PMLR, 2020.\n",
      "Jonathan Kernes. Master positional encoding: Part i, 05 2021. URL https://\n",
      "towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3 .\n",
      "Patrick Kidger. On neural differential equations. arXiv preprint arXiv:2202.02435 , 2022.\n",
      "Gyuwan Kim and Kyunghyun Cho. Length-adaptive transformer: Train once with length\n",
      "drop, use anytime with search. In Proceedings of the 59th Annual Meeting of the As-\n",
      "sociation for Computational Linguistics and the 11th International Joint Conference on\n",
      "Natural Language Processing (Volume 1: Long Papers) , pages 6501–6511, 2021.\n",
      "Najoung Kim, Roma Patel, Adam Poliak, Patrick Xia, Alex Wang, Tom McCoy, Ian Tenney,\n",
      "Alexis Ross, Tal Linzen, Benjamin Van Durme, et al. Probing what different nlp tasks\n",
      "teach machines about function word comprehension. In Proceedings of the Eighth Joint\n",
      "Conference on Lexical and Computational Semantics (* SEM 2019) , pages 235–249, 2019.\n",
      "Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan\n",
      "Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W Mahoney, et al. Full stack\n",
      "optimization of transformer inference: a survey. arXiv preprint arXiv:2302.14017 , 2023.\n",
      "Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without\n",
      "convolution or region supervision. In Proceedings of International Conference on Machine\n",
      "Learning , pages 5583–5594. PMLR, 2021.\n",
      "Yoon Kim and Alexander M Rush. Sequence-level knowledge distillation. In Proceedings\n",
      "of the 2016 Conference on Empirical Methods in Natural Language Processing , pages\n",
      "1317–1327, 2016.\n",
      "Young Jin Kim and Hany Hassan Awadalla. Fastformers: Highly efficient transformer\n",
      "models for natural language understanding. In Proceedings of SustaiNLP: Workshop on\n",
      "Simple and Efficient Natural Language Processing , pages 149–158, 2020.\n",
      "107\n",
      "Page 108:\n",
      "Xiao and Zhu\n",
      "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv\n",
      "preprint arXiv:1412.6980 , 2014.\n",
      "Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer.\n",
      "InProceedings of International Conference on Learning Representations , 2020.\n",
      "Taku Kudo. Subword regularization: Improving neural network translation models with\n",
      "multiple subword candidates. In Proceedings of the 56th Annual Meeting of the Associa-\n",
      "tion for Computational Linguistics (Volume 1: Long Papers) , pages 66–75, 2018.\n",
      "Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,\n",
      "Joseph E Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large\n",
      "language model serving with pagedattention. arXiv preprint arXiv:2309.06180 , 2023.\n",
      "Fran¸ cois Lagunas, Ella Charlaix, Victor Sanh, and Alexander M Rush. Block pruning\n",
      "for faster transformers. In Proceedings of the 2021 Conference on Empirical Methods in\n",
      "Natural Language Processing , pages 10619–10629, 2021.\n",
      "Guillaume Lample, Alexandre Sablayrolles, Marc’Aurelio Ranzato, Ludovic Denoyer, and\n",
      "Herv´ e J´ egou. Large memory layers with product keys. Advances in Neural Information\n",
      "Processing Systems , 32, 2019.\n",
      "Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping\n",
      "Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models\n",
      "with conditional computation and automatic sharding. In Proceedings of International\n",
      "Conference on Learning Representations , 2021.\n",
      "Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via\n",
      "speculative decoding. In Proceedings of International Conference on Machine Learning ,\n",
      "pages 19274–19286. PMLR, 2023.\n",
      "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman\n",
      "Goyal, Heinrich K¨ uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨ aschel, et al. Retrieval-\n",
      "augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information\n",
      "Processing Systems , 33:9459–9474, 2020.\n",
      "Bei Li, Hui Liu, Ziyang Wang, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu, and\n",
      "Changliang Li. Does multi-encoder help? a case study on context-aware neural machine\n",
      "translation. In Proceedings of the 58th Annual Meeting of the Association for Computa-\n",
      "tional Linguistics , pages 3512–3518, 2020a.\n",
      "Bei Li, Ziyang Wang, Hui Liu, Yufan Jiang, Quan Du, Tong Xiao, Huizhen Wang, and\n",
      "Jingbo Zhu. Shallow-to-deep training for neural machine translation. In Proceedings of\n",
      "the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) ,\n",
      "pages 995–1005, 2020b.\n",
      "Bei Li, Ziyang Wang, Hui Liu, Quan Du, Tong Xiao, Chunliang Zhang, and Jingbo Zhu.\n",
      "Learning light-weight translation models from deep transformer. In Proceedings of the\n",
      "AAAI Conference on Artificial Intelligence , volume 35, pages 13217–13225, 2021.\n",
      "108\n",
      "Page 109:\n",
      "Introduction to Transformers\n",
      "Bei Li, Quan Du, Tao Zhou, Yi Jing, Shuhan Zhou, Xin Zeng, Tong Xiao, Jingbo Zhu, Xuebo\n",
      "Liu, and Min Zhang. Ode transformer: An ordinary differential equation-inspired model\n",
      "for sequence generation. In Proceedings of the 60th Annual Meeting of the Association\n",
      "for Computational Linguistics (Volume 1: Long Papers) , pages 8335–8351, 2022a.\n",
      "Bei Li, Tong Zheng, Yi Jing, Chengbo Jiao, Tong Xiao, and Jingbo Zhu. Learning multiscale\n",
      "transformer models for sequence generation. In International Conference on Machine\n",
      "Learning , pages 13225–13241. PMLR, 2022b.\n",
      "Hongkang Li, Meng Wang, Sijia Liu, and Pin-Yu Chen. A theoretical understanding of\n",
      "shallow vision transformers: Learning, generalization, and sample complexity. In The\n",
      "Eleventh International Conference on Learning Representations , 2022c.\n",
      "Junhui Li, Deyi Xiong, Zhaopeng Tu, Muhua Zhu, Min Zhang, and Guodong Zhou. Mod-\n",
      "eling source syntax for neural machine translation. In Proceedings of the 55th Annual\n",
      "Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages\n",
      "688–697, 2017.\n",
      "Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-\n",
      "image pre-training for unified vision-language understanding and generation. In Interna-\n",
      "tional Conference on Machine Learning , pages 12888–12900. PMLR, 2022d.\n",
      "Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Ma-\n",
      "lik, and Christoph Feichtenhofer. Mvitv2: Improved multiscale vision transformers for\n",
      "classification and detection. In Proceedings of the IEEE/CVF Conference on Computer\n",
      "Vision and Pattern Recognition , pages 4804–4814, 2022e.\n",
      "Kaiyuan Liao, Yi Zhang, Xuancheng Ren, Qi Su, Xu Sun, and Bin He. A global past-\n",
      "future early exit method for accelerating inference of pre-trained language models. In\n",
      "Proceedings of the 2021 Conference of the North American Chapter of the Association\n",
      "for Computational Linguistics: Human Language Technologies , pages 2013–2023, 2021.\n",
      "Tatiana Likhomanenko, Qiantong Xu, Gabriel Synnaeve, Ronan Collobert, and Alex Ro-\n",
      "gozhnikov. Cape: Encoding relative positions with continuous augmented positional\n",
      "embeddings. Advances in Neural Information Processing Systems , 34:16079–16092, 2021.\n",
      "Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. A survey of transformers. AI\n",
      "Open , 2022a.\n",
      "Ye Lin, Shuhan Zhou, Yanyang Li, Anxiang Ma, Tong Xiao, and Jingbo Zhu. Multi-path\n",
      "transformer is better: A case study on neural machine translation. In Findings of the\n",
      "Association for Computational Linguistics: EMNLP 2022 , pages 5646–5656, 2022b.\n",
      "Fenglin Liu, Xuancheng Ren, Zhiyuan Zhang, Xu Sun, and Yuexian Zou. Rethinking skip\n",
      "connection with layer normalization. In Proceedings of the 28th international conference\n",
      "on computational linguistics , pages 3586–3598, 2020a.\n",
      "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning.\n",
      "arXiv preprint arXiv:2304.08485 , 2023a.\n",
      "109\n",
      "Page 110:\n",
      "Xiao and Zhu\n",
      "Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the\n",
      "difficulty of training transformers. In Proceedings of the 2020 Conference on Empirical\n",
      "Methods in Natural Language Processing (EMNLP) , pages 5747–5763, 2020b.\n",
      "Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser,\n",
      "and Noam Shazeer. Generating wikipedia by summarizing long sequences. In Proceedings\n",
      "of International Conference on Learning Representations , 2018.\n",
      "Yang Liu, Yao Zhang, Yixin Wang, Feng Hou, Jin Yuan, Jiang Tian, Yang Zhang,\n",
      "Zhongchao Shi, Jianping Fan, and Zhiqiang He. A survey of visual transformers. IEEE\n",
      "Transactions on Neural Networks and Learning Systems , 2023b.\n",
      "Yuchen Liu, Junnan Zhu, Jiajun Zhang, and Chengqing Zong. Bridging the modality gap\n",
      "for speech-to-text translation. arXiv preprint arXiv:2010.14920 , 2020c.\n",
      "Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to\n",
      "attention-based neural machine translation. In Proceedings of the 2015 Conference on\n",
      "Empirical Methods in Natural Language Processing , pages 1412–1421, 2015.\n",
      "Christopher D Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal, and Omer Levy.\n",
      "Emergent linguistic structure in artificial neural networks trained by self-supervision.\n",
      "Proceedings of the National Academy of Sciences , 117(48):30046–30054, 2020.\n",
      "Pedro Henrique Martins, Zita Marinho, and Andr´ e FT Martins. ∞-former: Infinite memory\n",
      "transformer-former: Infinite memory transformer. In Proceedings of the 60th Annual\n",
      "Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages\n",
      "5468–5485, 2022.\n",
      "Saeed Masoudnia and Reza Ebrahimpour. Mixture of experts: a literature survey. The\n",
      "Artificial Intelligence Review , 42(2):275, 2014.\n",
      "JS McCarley, Rishav Chakravarti, and Avirup Sil. Structured pruning of a bert-based\n",
      "question answering model. arXiv preprint arXiv:1910.06360 , 2019.\n",
      "William Merrill, Ashish Sabharwal, and Noah A Smith. Saturated transformers are\n",
      "constant-depth threshold circuits. Transactions of the Association for Computational\n",
      "Linguistics , 10:843–856, 2022.\n",
      "Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one?\n",
      "Advances in neural information processing systems , 32, 2019.\n",
      "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed\n",
      "representations of words and phrases and their compositionality. In Proceedings of the\n",
      "26th International Conference on Neural Information Processing Systems - Volume 2 ,\n",
      "page 31113119, 2013.\n",
      "Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart Van Baalen,\n",
      "and Tijmen Blankevoort. A white paper on neural network quantization. arXiv preprint\n",
      "arXiv:2106.08295 , 2021.\n",
      "110\n",
      "Page 111:\n",
      "Introduction to Transformers\n",
      "Alan V Oppenheim and Ronald W Schafer. Digital signal processing(book). Prentice-Hall ,\n",
      "1975.\n",
      "Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan\n",
      "Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv\n",
      "preprint arXiv:2303.06349 , 2023.\n",
      "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\n",
      "Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language\n",
      "models to follow instructions with human feedback. Advances in Neural Information\n",
      "Processing Systems , 35:27730–27744, 2022.\n",
      "Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge distillation.\n",
      "InProceedings of the IEEE/CVF conference on computer vision and pattern recognition ,\n",
      "pages 3967–3976, 2019.\n",
      "Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexan-\n",
      "der Ku, and Dustin Tran. Image transformer. In International conference on machine\n",
      "learning , pages 4055–4064. PMLR, 2018.\n",
      "Baoyun Peng, Xiao Jin, Jiaheng Liu, Dongsheng Li, Yichao Wu, Yu Liu, Shunfeng Zhou,\n",
      "and Zhaoning Zhang. Correlation congruence for knowledge distillation. In Proceedings\n",
      "of the IEEE/CVF International Conference on Computer Vision , pages 5007–5016, 2019.\n",
      "Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao,\n",
      "Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing\n",
      "rnns for the transformer era. arXiv preprint arXiv:2305.13048 , 2023.\n",
      "H Peng, N Pappas, D Yogatama, R Schwartz, N Smith, and L Kong. Random feature\n",
      "attention. In Proceedings of International Conference on Learning Representations (ICLR\n",
      "2021) , 2021.\n",
      "Jorge P´ erez, Javier Marinkovi´ c, and Pablo Barcel´ o. On the turing completeness of modern\n",
      "neural network architectures. In Proceedings of International Conference on Learning\n",
      "Representations , 2018.\n",
      "Fabio Petroni, Tim Rockt¨ aschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang\n",
      "Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the\n",
      "2019 Conference on Empirical Methods in Natural Language Processing and the 9th In-\n",
      "ternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages\n",
      "2463–2473, 2019.\n",
      "Ngoc-Quan Pham, Thai-Son Nguyen, Jan Niehues, Markus M¨ uller, Sebastian St¨ uker, and\n",
      "Alexander Waibel. Very deep self-attention networks for end-to-end speech recognition.\n",
      "arXiv preprint arXiv:1904.13377 , 2019.\n",
      "Telmo Pessoa Pires, Ant´ onio V Lopes, Yannick Assogba, and Hendra Setiawan. One wide\n",
      "feedforward is all you need. arXiv preprint arXiv:2309.01826 , 2023.\n",
      "111\n",
      "Page 112:\n",
      "Xiao and Zhu\n",
      "Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury,\n",
      "Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling trans-\n",
      "former inference. In Proceedings of Machine Learning and Systems , 2023.\n",
      "Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear\n",
      "biases enables input length extrapolation. In Proceedings of International Conference on\n",
      "Learning Representations , 2021.\n",
      "Ivan Provilkov, Dmitrii Emelianenko, and Elena Voita. Bpe-dropout: Simple and effective\n",
      "subword regularization. In Proceedings of the 58th Annual Meeting of the Association for\n",
      "Computational Linguistics , pages 1882–1892, 2020.\n",
      "Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang. Block-\n",
      "wise self-attention for long document understanding. In Findings of the Association for\n",
      "Computational Linguistics: EMNLP 2020 , pages 2555–2565, 2020.\n",
      "Lawrence R Rabiner and Bernard Gold. Theory and application of digital signal processing.\n",
      "Prentice-Hall , 1975.\n",
      "Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P\n",
      "Lillicrap. Compressive transformers for long-range sequence modelling. In Proceedings of\n",
      "International Conference on Learning Representations , 2019.\n",
      "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\n",
      "Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning\n",
      "with a unified text-to-text transformer. Journal of Machine Learning Research , 21(140):\n",
      "1–67, 2020.\n",
      "Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese\n",
      "bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natu-\n",
      "ral Language Processing and the 9th International Joint Conference on Natural Language\n",
      "Processing (EMNLP-IJCNLP) , pages 3982–3992, 2019.\n",
      "Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta,\n",
      "and Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550 ,\n",
      "2014.\n",
      "Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-\n",
      "based sparse attention with routing transformers. Transactions of the Association for\n",
      "Computational Linguistics , 9:53–68, 2021.\n",
      "Michael Santacroce, Zixin Wen, Yelong Shen, and Yuanzhi Li. What matters in the struc-\n",
      "tured pruning of generative language models? arXiv preprint arXiv:2302.03773 , 2023.\n",
      "Imanol Schlag, Kazuki Irie, and J¨ urgen Schmidhuber. Linear transformers are secretly fast\n",
      "weight programmers. In Proceedings of International Conference on Machine Learning ,\n",
      "pages 9355–9366. PMLR, 2021.\n",
      "112\n",
      "Page 113:\n",
      "Introduction to Transformers\n",
      "Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Tran, Yi Tay,\n",
      "and Donald Metzler. Confident adaptive language modeling. Advances in Neural Infor-\n",
      "mation Processing Systems , 35:17456–17472, 2022.\n",
      "Roy Schwartz, Gabriel Stanovsky, Swabha Swayamdipta, Jesse Dodge, and Noah A Smith.\n",
      "The right tool for the job: Matching model and instance complexities. In Proceedings of\n",
      "the 58th Annual Meeting of the Association for Computational Linguistics , pages 6640–\n",
      "6651, 2020.\n",
      "Abigail See. Deep learning, structure and innate priors: A discussion between yann lecun\n",
      "and christopher manning, 02 2018. URL http://www.abigailsee.com/2018/02/21/\n",
      "deep-learning-structure-and-innate-priors.html .\n",
      "Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare\n",
      "words with subword units. In Proceedings of the 54th Annual Meeting of the Association\n",
      "for Computational Linguistics (Volume 1: Long Papers) , pages 1715–1725, 2016.\n",
      "Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position\n",
      "representations. In Proceedings of the 2018 Conference of the North American Chapter of\n",
      "the Association for Computational Linguistics: Human Language Technologies, Volume\n",
      "2 (Short Papers) , pages 464–468, 2018.\n",
      "Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint\n",
      "arXiv:1911.02150 , 2019.\n",
      "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hin-\n",
      "ton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-\n",
      "experts layer. In Proceedings of International Conference on Learning Representations ,\n",
      "2017.\n",
      "Dinghan Shen, Mingzhi Zheng, Yelong Shen, Yanru Qu, and Weizhu Chen. A simple\n",
      "but tough-to-beat data augmentation approach for natural language understanding and\n",
      "generation. arXiv preprint arXiv:2009.13818 , 2020.\n",
      "Xing Shi, Inkit Padhi, and Kevin Knight. Does string-based neural mt learn source syntax?\n",
      "InProceedings of the 2016 conference on empirical methods in natural language processing ,\n",
      "pages 1526–1534, 2016.\n",
      "Maciej Skorski, Alessandro Temperoni, and Martin Theobald. Revisiting weight initializa-\n",
      "tion of deep neural networks. In Asian Conference on Machine Learning , pages 1192–1207.\n",
      "PMLR, 2021.\n",
      "David So, Quoc Le, and Chen Liang. The evolved transformer. In Proceedings of Interna-\n",
      "tional conference on machine learning , pages 5877–5886. PMLR, 2019.\n",
      "Matthias Sperber, Jan Niehues, Graham Neubig, Sebastian St¨ uker, and Alex Waibel. Self-\n",
      "attentional acoustic models. In Proceedings of Interspeech 2018 , pages 3723–3727, 2018.\n",
      "Rupesh Kumar Srivastava, Klaus Greff, and J¨ urgen Schmidhuber. Highway networks. arXiv\n",
      "preprint arXiv:1505.00387 , 2015.\n",
      "113\n",
      "Page 114:\n",
      "Xiao and Zhu\n",
      "Pierre Stock, Angela Fan, Benjamin Graham, Edouard Grave, R´ emi Gribonval, Herve Je-\n",
      "gou, and Armand Joulin. Training with quantization noise for extreme model compres-\n",
      "sion. In Proceedings of International Conference on Learning Representations , 2021.\n",
      "Emma Strubell, Patrick Verga, Daniel Andor, David Weiss, and Andrew McCallum.\n",
      "Linguistically-informed self-attention for semantic role labeling. In Proceedings of the\n",
      "2018 Conference on Empirical Methods in Natural Language Processing , pages 5027–\n",
      "5038, 2018.\n",
      "Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced trans-\n",
      "former with rotary position embedding. arXiv preprint arXiv:2104.09864 , 2021.\n",
      "Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks.\n",
      "Advances in neural information processing systems , 28, 2015.\n",
      "Sainbayar Sukhbaatar, ´Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive\n",
      "attention span in transformers. In Proceedings of the 57th Annual Meeting of the Asso-\n",
      "ciation for Computational Linguistics , pages 331–335, 2019.\n",
      "Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong\n",
      "Wang, and Furu Wei. Retentive network: A successor to transformer for large language\n",
      "models. arXiv preprint arXiv:2307.08621 , 2023.\n",
      "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian\n",
      "Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In Proceedings of\n",
      "2nd International Conference on Learning Representations (ICLR 2014) , 2014.\n",
      "Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural\n",
      "networks. In International conference on machine learning , pages 6105–6114. PMLR,\n",
      "2019.\n",
      "Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn\n",
      "attention. In Proceedings of International Conference on Machine Learning , pages 9438–\n",
      "9447. PMLR, 2020a.\n",
      "Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A\n",
      "survey. CoRR , abs/2009.06732, 2020b.\n",
      "Ian Tenney, Dipanjan Das, and Ellie Pavlick. Bert rediscovers the classical nlp pipeline. In\n",
      "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics ,\n",
      "pages 4593–4601, 2019a.\n",
      "Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Na-\n",
      "joung Kim, Benjamin Van Durme, Samuel R Bowman, Dipanjan Das, et al. What do\n",
      "you learn from context? probing for sentence structure in contextualized word represen-\n",
      "tations. In Proceedings of International Conference on Learning Representations , 2019b.\n",
      "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\n",
      "Timoth´ ee Lacroix, Baptiste Rozi` ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\n",
      "114\n",
      "Page 115:\n",
      "Introduction to Transformers\n",
      "Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 ,\n",
      "2023a.\n",
      "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine\n",
      "Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\n",
      "Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 ,\n",
      "2023b.\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\n",
      "Gomez,  Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of\n",
      "Advances in Neural Information Processing Systems , volume 30, 2017.\n",
      "Oriol Vinyals,  Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton.\n",
      "Grammar as a foreign language. Advances in neural information processing systems , 28,\n",
      "2015.\n",
      "Elena Voita, Pavel Serdyukov, Rico Sennrich, and Ivan Titov. Context-aware neural machine\n",
      "translation learns anaphora resolution. In Proceedings of the 56th Annual Meeting of the\n",
      "Association for Computational Linguistics (Volume 1: Long Papers) , pages 1264–1274,\n",
      "2018.\n",
      "Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-\n",
      "head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In\n",
      "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics ,\n",
      "pages 5797–5808, 2019.\n",
      "Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. Do nlp models\n",
      "know numbers? probing numeracy in embeddings. In Proceedings of the 2019 Conference\n",
      "on Empirical Methods in Natural Language Processing and the 9th International Joint\n",
      "Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 5307–5315, 2019.\n",
      "Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, and Song\n",
      "Han. Hat: Hardware-aware transformers for efficient natural language processing. In\n",
      "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics ,\n",
      "pages 7675–7688, 2020a.\n",
      "Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei.\n",
      "Deepnet: Scaling transformers to 1,000 layers. arXiv preprint arXiv:2203.00555 , 2022a.\n",
      "Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, Zhiliang Peng,\n",
      "Yu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, et al. Foundation transformers.\n",
      "arXiv preprint arXiv:2210.06423 , 2022b.\n",
      "Jue Wang, Ke Chen, Gang Chen, Lidan Shou, and Julian McAuley. Skipbert: Efficient\n",
      "inference with shallow layer skipping. In Proceedings of the 60th Annual Meeting of the\n",
      "Association for Computational Linguistics (Volume 1: Long Papers) , pages 7287–7301,\n",
      "2022c.\n",
      "115\n",
      "Page 116:\n",
      "Xiao and Zhu\n",
      "Lin Wang and Kuk-Jin Yoon. Knowledge distillation and student-teacher learning for visual\n",
      "intelligence: A review and new outlooks. IEEE transactions on pattern analysis and\n",
      "machine intelligence , 44(6):3048–3068, 2021.\n",
      "Peihao Wang, Rameswar Panda, Lucas Torroba Hennigen, Philip Greengard, Leonid Kar-\n",
      "linsky, Rogerio Feris, David Daniel Cox, Zhangyang Wang, and Yoon Kim. Learning to\n",
      "grow pretrained models for efficient transformer training. In Proceedings of The Eleventh\n",
      "International Conference on Learning Representations , 2023.\n",
      "Qiang Wang, Fuxue Li, Tong Xiao, Yanyang Li, Yinqiao Li, and Jingbo Zhu. Multi-\n",
      "layer representation fusion for neural machine translation. In Proceedings of the 27th\n",
      "International Conference on Computational Linguistics , pages 3015–3026, 2018a.\n",
      "Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, and Lidia S\n",
      "Chao. Learning deep transformer models for machine translation. In Proceedings of the\n",
      "57th Annual Meeting of the Association for Computational Linguistics , pages 1810–1822,\n",
      "2019.\n",
      "Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-\n",
      "attention with linear complexity. arXiv preprint arXiv:2006.04768 , 2020b.\n",
      "Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez. Skipnet: Learning\n",
      "dynamic routing in convolutional networks. In Proceedings of the European Conference\n",
      "on Computer Vision (ECCV) , pages 409–424, 2018b.\n",
      "Ziheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruning of large language mod-\n",
      "els. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language\n",
      "Processing (EMNLP) , pages 6151–6162, 2020c.\n",
      "Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani\n",
      "Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of\n",
      "large language models. arXiv preprint arXiv:2206.07682 , 2022.\n",
      "Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers. In Proceedings of\n",
      "International Conference on Machine Learning , pages 11080–11090. PMLR, 2021.\n",
      "Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and Michael Auli. Pay less attention\n",
      "with lightweight and dynamic convolutions. In Proceedings of International Conference\n",
      "on Learning Representations , 2018a.\n",
      "Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and Michael Auli. Pay less attention\n",
      "with lightweight and dynamic convolutions. In Proceedings of International Conference\n",
      "on Learning Representations , 2019.\n",
      "Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing\n",
      "transformers. In Proceedings of International Conference on Learning Representations ,\n",
      "2021.\n",
      "116\n",
      "Page 117:\n",
      "Introduction to Transformers\n",
      "Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song Han. Lite transformer with long-\n",
      "short range attention. In Proceedings of International Conference on Learning Represen-\n",
      "tations (ICLR) , 2020.\n",
      "Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie, Larry S Davis, Kristen\n",
      "Grauman, and Rogerio Feris. Blockdrop: Dynamic inference paths in residual networks.\n",
      "InProceedings of the IEEE conference on computer vision and pattern recognition , pages\n",
      "8817–8826, 2018b.\n",
      "Tong Xiao, Yinqiao Li, Jingbo Zhu, Zhengtao Yu, and Tongran Liu. Sharing attention\n",
      "weights for fast transformer. In Proceedings of the Twenty-Eighth International Joint\n",
      "Conference on Artificial Intelligence (IJCAI-19) , pages 5292–5298, 2019.\n",
      "Saining Xie, Ross Girshick, Piotr Doll´ ar, Zhuowen Tu, and Kaiming He. Aggregated residual\n",
      "transformations for deep neural networks. In Proceedings of the IEEE conference on\n",
      "computer vision and pattern recognition , pages 1492–1500, 2017.\n",
      "Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. Deebert: Dynamic early\n",
      "exiting for accelerating bert inference. In Proceedings of the 58th Annual Meeting of the\n",
      "Association for Computational Linguistics , pages 2246–2251, 2020.\n",
      "Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai\n",
      "Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the trans-\n",
      "former architecture. In International Conference on Machine Learning , pages 10524–\n",
      "10533, 2020.\n",
      "Canwen Xu and Julian Mcauley. A survey on dynamic neural networks for natural language\n",
      "processing. In Findings of the Association for Computational Linguistics: EACL 2023 ,\n",
      "pages 2325–2336, 2023.\n",
      "Chen Xu, Bojie Hu, Yanyang Li, Yuhao Zhang, Shen Huang, Qi Ju, Tong Xiao, and Jingbo\n",
      "Zhu. Stacked acoustic-and-textual encoding: Integrating the pre-trained models into\n",
      "speech translation encoders. In Proceedings of the 59th Annual Meeting of the Association\n",
      "for Computational Linguistics and the 11th International Joint Conference on Natural\n",
      "Language Processing (Volume 1: Long Papers) , pages 2619–2630, 2021a.\n",
      "Chen Xu, Rong Ye, Qianqian Dong, Chengqi Zhao, Tom Ko, Mingxuan Wang, Tong Xiao,\n",
      "and Jingbo Zhu. Recent advances in direct speech-to-text translation. In Proceedings of\n",
      "the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI-23):\n",
      "Survey Track , pages 6796–6804, 2023a.\n",
      "Chen Xu, Yuhao Zhang, Chengbo Jiao, Xiaoqian Liu, Chi Hu, Xin Zeng, Tong Xiao, Anx-\n",
      "iang Ma, Huizhen Wang, and Jingbo Zhu. Bridging the granularity gap for acoustic\n",
      "modeling. In Findings of the Association for Computational Linguistics: ACL 2023 ,\n",
      "pages 10816–10833, 2023b.\n",
      "Hongfei Xu, Qiuhui Liu, Josef van Genabith, Deyi Xiong, and Jingyi Zhang. Lipschitz\n",
      "constrained parameter initialization for deep transformers. In Proceedings of the 58th\n",
      "117\n",
      "Page 118:\n",
      "Xiao and Zhu\n",
      "Annual Meeting of the Association for Computational Linguistics , pages 397–402, July\n",
      "2020.\n",
      "Peng Xu, Xiatian Zhu, and David A Clifton. Multimodal learning with transformers: A\n",
      "survey. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2023c.\n",
      "Zenan Xu, Daya Guo, Duyu Tang, Qinliang Su, Linjun Shou, Ming Gong, Wanjun Zhong,\n",
      "Xiaojun Quan, Daxin Jiang, and Nan Duan. Syntax-enhanced pre-trained model. In\n",
      "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\n",
      "and the 11th International Joint Conference on Natural Language Processing (Volume 1:\n",
      "Long Papers) , pages 5412–5422, 2021b.\n",
      "Baosong Yang, Zhaopeng Tu, Derek F Wong, Fandong Meng, Lidia S Chao, and Tong\n",
      "Zhang. Modeling localness for self-attention networks. In Proceedings of the 2018 Con-\n",
      "ference on Empirical Methods in Natural Language Processing , pages 4449–4458, 2018.\n",
      "Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and\n",
      "Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv\n",
      "preprint arXiv:2309.17421 , 2023a.\n",
      "Zi Yang, Samridhi Choudhary, Siegfried Kunzmann, and Zheng Zhang. Quantization-aware\n",
      "and tensor-compressed training of transformers for natural language understanding. arXiv\n",
      "preprint arXiv:2306.01076 , 2023b.\n",
      "Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. Hier-\n",
      "archical attention networks for document classification. In Proceedings of the 2016 con-\n",
      "ference of the North American chapter of the association for computational linguistics:\n",
      "human language technologies , pages 1480–1489, 2016.\n",
      "Rong Ye, Mingxuan Wang, and Lei Li. End-to-end speech translation via cross-modal\n",
      "progressive training. arXiv preprint arXiv:2104.10380 , 2021.\n",
      "Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A\n",
      "survey on multimodal large language models. arXiv preprint arXiv:2306.13549 , 2023.\n",
      "Yaodong Yu, Sam Buchanan, Druv Pai, Tianzhe Chu, Ziyang Wu, Shengbang Tong, Ben-\n",
      "jamin D Haeffele, and Yi Ma. White-box transformers via sparse rate reduction. arXiv\n",
      "preprint arXiv:2306.01129 , 2023.\n",
      "Seniha Esen Yuksel, Joseph N Wilson, and Paul D Gader. Twenty years of mixture of\n",
      "experts. IEEE transactions on neural networks and learning systems , 23(8):1177–1193,\n",
      "2012.\n",
      "Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Ku-\n",
      "mar. Are transformers universal approximators of sequence-to-sequence functions? In\n",
      "Proceedings of International Conference on Learning Representations , 2019.\n",
      "Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti,\n",
      "Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird:\n",
      "118\n",
      "Page 119:\n",
      "Introduction to Transformers\n",
      "Transformers for longer sequences. Advances in neural information processing systems ,\n",
      "33:17283–17297, 2020.\n",
      "Biao Zhang, Deyi Xiong, and Jinsong Su. Accelerating neural transformer via an average\n",
      "attention network. In Proceedings of the 56th Annual Meeting of the Association for\n",
      "Computational Linguistics (Volume 1: Long Papers) , pages 1789–1798, 2018.\n",
      "Biao Zhang, Ivan Titov, and Rico Sennrich. Improving deep transformer with depth-scaled\n",
      "initialization and merged attention. In Proceedings of the 2019 Conference on Empirical\n",
      "Methods in Natural Language Processing and the 9th International Joint Conference on\n",
      "Natural Language Processing (EMNLP-IJCNLP) , pages 898–909, 2019.\n",
      "Zhuosheng Zhang, Yuwei Wu, Junru Zhou, Sufeng Duan, Hai Zhao, and Rui Wang. Sg-net:\n",
      "Syntax-guided machine reading comprehension. In Proceedings of the AAAI Conference\n",
      "on Artificial Intelligence , pages 9636–9643, 2020.\n",
      "Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai\n",
      "Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting.\n",
      "InProceedings of the AAAI conference on artificial intelligence , volume 35, pages 11106–\n",
      "11115, 2021.\n",
      "Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian McAuley, Ke Xu, and Furu Wei. Bert\n",
      "loses patience: Fast and robust inference with early exit. Advances in Neural Information\n",
      "Processing Systems , 33:18330–18341, 2020.\n",
      "Zhi-Hua Zhou. Ensemble Methods: Foundations and Algorithms . Chapman and Hall/CRC,\n",
      "2012.\n",
      "Barret Zoph and Quoc Le. Neural architecture search with reinforcement learning. In\n",
      "Proceedings of International Conference on Learning Representations , 2016.\n",
      "119\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# Using a context manager to open and read a PDF\n",
    "with open('Introduction_to_transformers_NLP.pdf', 'rb') as file:\n",
    "    pdf_reader = PyPDF2.PdfReader(file)\n",
    "    # Reading all pages\n",
    "    x = \"\"\n",
    "    for page_number, page in enumerate(pdf_reader.pages):\n",
    "        print(f\"Page {page_number + 1}:\")\n",
    "        print(page.extract_text())\n",
    "        x=x+page.extract_text()+\" \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Character Text splitter and Recursive Character text splitter we took physical positioning of the words into account. We assume the paragraph contains similar information but that is not the case. Chunk size when doing RAG is an annoying hyperparam and it feels naive to tune it to a global constant value. We do embedding based chunking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301533"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction to Transformers\n",
      "Introduction to Transformers: an NLP Perspective\n",
      "Tong Xiao xiaotong@mail.neu.edu.cn\n",
      "NLP Lab., Northeastern University, Shenyang, China\n",
      "NiuTrans Research, Shenyang, China\n",
      "Jingbo Zhu zhujingbo@mail.neu.edu.cn\n",
      "NLP Lab., Northeastern University, Shenyang, China\n",
      "NiuTrans Research, Shenyang, China\n",
      "Abstract\n",
      "Transformers have dominated empirical machine learning models of natural language pro-\n",
      "cessing. In this paper, we introduce basic concepts of Transformers and present key tech-\n",
      "niques that form the recent advances of these models. This includes a description of the\n",
      "standard Transformer architecture, a series of model refinements, and common applica-\n",
      "tions. Given that Transformers and related deep learning techniques might be evolving in\n",
      "ways we have never seen, we cannot dive into all the model details or cover all the tech-\n",
      "nical areas. Instead, we focus on just those concepts that are helpful for gaining a good\n",
      "understanding of Transformers and their variants. We also summarize the key ideas that\n",
      "impact this field, thereby yielding some insights into the strengths and limitations of these\n",
      "models. 1arXiv:2311.17633v1  [cs.CL]  29 Nov 2023 Xiao and Zhu\n",
      "Contents\n",
      "1 Background 4\n",
      "2 The Basic Model 4\n",
      "2.1 The Transformer Architecture . .\n"
     ]
    }
   ],
   "source": [
    "docs = text_splitter.create_documents([x])\n",
    "print(docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting the data to vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "vector_store = Qdrant(\n",
    "    client=qdrant_client,\n",
    "    collection_name=QDRANT_COLLECTION_NAME,\n",
    "    embeddings=embeddings,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0242b708c3cb4856be71f50fa4c6f9f2',\n",
       " '90398af5f0a846c08b879afd164a88a7',\n",
       " 'cf39d4e3e75b4e488267d90774393aba',\n",
       " '6cdd16849db841098d73b4976399e00a',\n",
       " '668c9026e8da49b29b118063145e06d1',\n",
       " '91200b5c8966450ea9ee01586379a0b0',\n",
       " '9c4eab3bed7d4ce58ec00b4aece2ffd5',\n",
       " '256d3dd781fb464c8639615643c528ef',\n",
       " '47504352330b435b92862c52177c3454',\n",
       " '97c782bc22ff4e48ac773760ae89e574',\n",
       " '2268295eb12d4cfc8c4bc6f093d296b5',\n",
       " '7bdd08edf3a343fe8e733ae7f9f4af29',\n",
       " 'c5561038f1a242c58bf8c4c3765bc1db',\n",
       " 'ca958c625d46402f96092623c9a0b48a',\n",
       " '0c4c60407c5f4255be91435a3d4d968d',\n",
       " '451cafe41031457fa08a9eeea1e7a032',\n",
       " '0385b02d29064691ae0995b74671d57f',\n",
       " '45948be3ec004bd1b28df7f6b0ae14c6',\n",
       " '80eb72daa3ac41b9a8cdf453e2df5630',\n",
       " '08a382d40e6c4873b8daebc020384eab',\n",
       " 'd529456548b94b04b2b80aea888b1522',\n",
       " '183058fc07a949f8b49b583fa8eb96b5',\n",
       " '937f2c8774dc4f309b5baa7cd6a85a7d',\n",
       " 'd83d6272f9f74674981ad0d0e567f74f',\n",
       " 'c3400e60b25447a3afb5079292d838fe',\n",
       " '44a3899ab1504bb7b074383f5986c419',\n",
       " '36447ae1419c4b46b104a44f2e4fd440',\n",
       " 'e2134c8966ce4c23a773931392f56f09',\n",
       " '61e91a1a2837465a828ed87b239edbb4',\n",
       " '1ef4d70af0e543249d34637bf7cf7722',\n",
       " '08ca1587f3364fbc99d322f586395626',\n",
       " '0b62de07080e4d80a00196c81e204fe0',\n",
       " '88aa56561aa44ecab2155dd926e5a46c',\n",
       " '5242a8882e6d42329f51c93d791c2c82',\n",
       " 'b03f35c8d86d4294ab3bca56c8abefe9',\n",
       " '85f56fa03cc5470aab6a31314844847a',\n",
       " 'de72927cc2d144edafe9b7e6a65ca144',\n",
       " '958d51223f3d4f87b3ff598368082d7a',\n",
       " '88754413ac5b4115b1216c53619b1189',\n",
       " 'c5174a0f49ed47ff8910234f5eadfe5a',\n",
       " '1d2b14e92f964ffbb14fdbf934b8ebba',\n",
       " 'cdff25ab2e724878ba562bd2d1c30303',\n",
       " '47acf764bb1d4e9f8c13f404a57e98dd',\n",
       " '97ed00311ee7442c9722477866e7aa3f',\n",
       " 'ed944b838c5342b9b434c059a3f03c6f',\n",
       " '834a80339fe24b459e8cbc5abab91606',\n",
       " '4dc3c2f1661a414793ecd4eeceaa38fb',\n",
       " '6ff7750e099d4e9d95dc55f96abdae37',\n",
       " '15134a2c86914c13842a0aafa854eaee',\n",
       " 'c0dc16bd8dea419db4481683b349b96c',\n",
       " '7273d6f722b447489ae894220b8419f9',\n",
       " '72de87247ff04b789f267f099edbbc73',\n",
       " 'bf9f7d28daf7449182258f06c5f2cf30',\n",
       " '1fbdb8b9b52245d1b27aa2a183edee25',\n",
       " '6ab03481a13c4ad48af5a08feac6ab77',\n",
       " '15a52e7e53a1463bbf4768ac0c965bdd',\n",
       " '2a1aa29f5e0c423db89bbd9a9801d687',\n",
       " 'f59a7710025b4dbbb90a43d23e0591f3',\n",
       " 'bf4efb9edbe1401489ded8ad42f1db24',\n",
       " 'dfcc16c42fc1406dbae21eafc1491ce4',\n",
       " 'e4f44c2f3e7245f3a024e8faff6b2023',\n",
       " '0b32040faa5445a0b3553f83ecbf1204',\n",
       " 'f4774695f39b400ca803259bab83201d',\n",
       " '39f519ef3dfb4215b0c1f7d6ab276151',\n",
       " 'bde6c5ea3604408c84d2a2ec2f68b034',\n",
       " '45f072582ae04558a5e2482b7c3c1d4d',\n",
       " 'f82bbc8021a94e77ba7b285f33802189',\n",
       " '107ceeab946345eca7cc1737d1eaf119',\n",
       " 'bb5a8d29f4b34e7684a5cb76ae1ca2f2',\n",
       " 'ea826b6362c245c795ebf06ffe51db88',\n",
       " 'a810856291fc4a8c83ad973af0091a60',\n",
       " 'd24bb2cdc58e475588853b9ea5d63658',\n",
       " '788f06162b0f48979927ee0352bc6f4c',\n",
       " 'befba0a235b4477989673fa0177fc760',\n",
       " 'd314d0154b944ce88881bfa32ade54b6',\n",
       " '0ad4abc48790423ca3498f09f79d4374',\n",
       " '13e2df7821844f379ccd412611f02b11',\n",
       " '4eec876749e349279adccb0b0361389e',\n",
       " 'df1b21b6bcd64954a010944d30c3f28b',\n",
       " 'c50e706093604ef68d00319397c7c503',\n",
       " 'a70b5c867ad94b0f9520e6b9f83fbc2d',\n",
       " '3d32926436d9495b9ffef8c922d56a72',\n",
       " '88e9baa1f1264e6f9a224d0d3182bc23',\n",
       " '4ffaae65de6d47f3a3840fa9b8ec3ee4',\n",
       " '68396b4d440444d09395d344105e9e53',\n",
       " '6699f7d0d548435c99cfc8352729668a',\n",
       " 'af3852b33d424f5ea5b9a926a0f48cff',\n",
       " '0c392bd5460246fca5e5ab9511c71a16',\n",
       " '9c0f73fc29014b69a8dc63f489a40472',\n",
       " '68a0c56dabf645498ba32683f82bf2dd',\n",
       " '0baf3c0ca62443a19a392bd85b0ea213',\n",
       " 'cba876db0bc64049abedcb3f865a3a37',\n",
       " '02d0093a6b0f4e4399dac574edd266bb',\n",
       " 'b56bcc7ef72945768943df82ea3441e7',\n",
       " '31fa1ae586d149f5b8d82d93f34bfa25',\n",
       " '98d257bc4e8a468baaa6e329cb798220',\n",
       " '88d894700243417b9f29c897b6c93a86',\n",
       " 'ff1d8a370a914068b921b02c44be0f82',\n",
       " '8c07a765e7734d30a05ee784663d8c1e',\n",
       " '422d972bb9504d68a46bcce2f4956ed6',\n",
       " 'f18958b0c4154adc9158d80b158d48b0',\n",
       " '03fce4a1cfd643b28286555c25129fb8',\n",
       " 'fcf51de977294c8f9639c49fb67a6e53',\n",
       " '19092c5dd6b6443f9380218e1e5190c7',\n",
       " '99795dbf30cd4107bff02f567ea5b1e3',\n",
       " '1e4f340339a042ab8590e76abe6c666f',\n",
       " 'db0fb0a66ac748c68945821a3eea1252',\n",
       " '2bae7bb91a4849d680a0509b32ab3c4b',\n",
       " '077920680fd84ea383db3a64ab39d227',\n",
       " '239c34e9d00d45f9b5b33eff4b6240bd',\n",
       " '27ff07f5ba514aafb1d314014fa8bf7d',\n",
       " '35c5f01a9e2f4b7c934bbaea3694c13d',\n",
       " '1700bf15edad48dc822a3e1ba4e9c90a',\n",
       " 'a1e8bbe4ce3546f3985b8713ea3466a9',\n",
       " 'c4fef1b77f03459a8a807f70cba42b5f',\n",
       " '6dff5ac33aa0409b9773f870bb47c69c',\n",
       " 'c6ee496df093444385fbf3264af0382b',\n",
       " 'f742394b3c3e418eb553fa19e50dbb4e',\n",
       " '60cb9b286ab84af5bd9b20d0834b6c4a',\n",
       " '1fb5736186774e4398e99cda50ce26ba',\n",
       " '348b3095064b47279cc4613f5426d342',\n",
       " 'bdd87ea1e4254c2b9d9c3560f50b70fb',\n",
       " 'ea179e48c1ad4e198d6e5b624f68ceb6',\n",
       " 'acf91233756449729124265b4b1dc4b6',\n",
       " '1f4b49757e584fc28d9e2f69ba1cec0b',\n",
       " '6017c8f390a441ec98bcda74cc2e9700',\n",
       " '48901f9e93f740e1845860a46d3c7c7f',\n",
       " '7a4341da76ab40a0a84dfbe7177f65e3',\n",
       " '1a6850497362451e816577b96d9bd83f',\n",
       " '021664ec53834bd48997c0e5c746e784',\n",
       " '2d536d31822b48d38697e61e0c5fd42a',\n",
       " '0c7865a5159941f1aa566f6938be312e',\n",
       " '4892da8644bc4807b2d93850ad8a4a12',\n",
       " '4c11bc706db7422782d1680fad71d0e3',\n",
       " '956519fe153a4b72be357235654cba2c',\n",
       " '65a8dae0bc48401691cdbb413bcc79b9',\n",
       " '244592ca55654018acb9bcd4b39b74b4',\n",
       " 'e60b5cac2b0f4a7592c3e9f90eb053cd',\n",
       " '11b877971f3141c2b55e940126e7f560',\n",
       " 'd76bf7a9f4214cf584c3c82e4b198fab',\n",
       " '0458bd58fd0d4e69a3973d6a9c13b751',\n",
       " 'f281530aafb54d83bffb56e607e47b91',\n",
       " '88e37aeb2ca141f29ed491d4e49c6638',\n",
       " '91bd17e9a1ad411789359fd93f6eaa6d',\n",
       " 'adb10c030b0c40e4b7a7fd835bf4d1bf',\n",
       " '8967f613a7a64fb4a16ed0c406198c47',\n",
       " '27d1abc4975e4d69b3452b30fce42181',\n",
       " '3713734e595e4d6ba34d0e73a9655884',\n",
       " '49d47f0dbcac4bda90402d8a57ba5b83',\n",
       " 'e396d71e81ca4c1c87512aca1e6bb31b',\n",
       " '8fb6dcdd89614c61a87af97a88859fe2',\n",
       " 'fc2e36c3d0d1465eb58741c70fec6c74',\n",
       " '058d5caeb7394ba0ae4991f51f16c5c8',\n",
       " 'c63aa04a71c94e808e2c8aed254a5cdb',\n",
       " '3520397b342d4c929dac115dab97ca72',\n",
       " 'a4ff799028024b13a491bbacf9a81ea8',\n",
       " '20913b9f47f64a819cd57f697c97b07c',\n",
       " '681220b74c6d4c6097802b9cf1aa1b5e',\n",
       " '4c16475217e24e6aae3bb70e834fce64',\n",
       " '703973cf9ed84d70bef919debbd66d4f',\n",
       " '3469cb23edcf445e9c50b9410d52bd9e',\n",
       " '7eb6cb9c0d0c45d5bdee87366cc7ed6f',\n",
       " 'd4597ca5ea444cba806c9fae0fde446e',\n",
       " '03449503170143f09f66b421d80bba6f',\n",
       " '6df230e60f4d403fbb89ac28dea504db',\n",
       " 'b24ea9c1011a4c61b6a2b1e6da5450fb',\n",
       " '1e8b6ee902b44272a5fcb2544e0c3afd',\n",
       " '4f5d44288ab0482d9ba380f2385f85f4',\n",
       " 'eb18620560c541d696e76ccdf1c60874',\n",
       " '8686ddae13dc4a68bed2f6d00755a4fb',\n",
       " '8fe359754a7e46be983b53c63c4b9e52',\n",
       " '63963b0a9fb1403582791eb3d1acb808',\n",
       " '89583c07ccd7405b94c1435908e3348d',\n",
       " '63d359d948454c2e846d354a28841f3b',\n",
       " 'b7234169eff54af693890843b2679c78',\n",
       " '18bbccb80a7b446eb8643ab450c810d3',\n",
       " 'c4c922874d034bc496e01f826ea539f3',\n",
       " 'f75c2c8ace5d46838185b7bfa14aa4bf',\n",
       " '9569828ab827474098169f86c8233c44']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Add the documents to Quadrant\n",
    "vector_store.add_documents(docs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
